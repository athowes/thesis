---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: references.bib
---

```{r echo = FALSE}
options(scipen = 100)

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  dpi = 320,
  cache = TRUE,
  out.width = "95%",
  fig.align = 'center'
)
```

# Bayesian spatio-temporal statistics {#bayes-st}
\adjustmtc
\markboth{Bayesian}{}
<!-- For PDF output, include these two LaTeX commands after unnumbered chapter headings, otherwise the mini table of contents and the running header will show the previous chapter -->

## Bayesian statistics

Bayesian statistics is a mathematical paradigm for learning from data, that allows flexible and principled incorporation of scientific knowledge.
As such, it is particularly well suited for attending to the challenges I present in Section \ref{sec:surveillance} wherein the data demands careful analysis.
In this section I provide brief overview.
For a more complete introduction, I recommend @mcelreath2020statistical or @gelman2013bayesian.

### Bayesian modelling

At its best, the Bayesian paradigm allows the analyst focus on how best to model the data.
This is achieved by the construction of a generative model $p(\y, \bvartheta)$ for the observed data $\y = (y_1, \ldots, y_n)$ together with parameters $\bvartheta = (\vartheta_1, \ldots, \vartheta_d)$, where $n$ is the dimension of the data and $d$ is the number of parameters.
The model is generative in the sense that one can simulate from it to obtain draws
\begin{equation}
(\y, \bvartheta) \sim p(\y, \bvartheta)
\end{equation}
If these draws differ too greatly from what the analyst would expect, then the generative model can be refined.
In this way, models can be built iteratively, with complexity added gradually. 

The model is usually constructed from two parts, known as the likelihood $p(\y \, | \, \bvartheta)$ and the prior $p(\bvartheta)$ whereby $p(\y, \bvartheta) = p(\y \, | \, \bvartheta) p(\bvartheta)$.
The likelihood, as a function of $\bvartheta$ with $\y$ fixed, reflects the probability of observing the data when the true value of the parameters is $\bvartheta$.
The prior encapsulates beliefs about the parameters $\bvartheta$ before the data is observed.
There has been substantial discussion about how the prior should be specified.
The main axis of disagreement is the degree of subjectivity which should be allowed.
However, the distinction between likelihood and prior can sometimes be blurred (Section \@ref(hierarchical-lgm-elgm)).
As such, it may be argued that these difficulties are not unique to the prior and ultimately the challenge of specifying the data generating process is better thought of more holistically [@gelman2017prior].

### Bayesian computation

The posterior distribution $p(\bvartheta \, | \, \y)$ encapsulates probabilistic beliefs about the parameters given the observed data.
Using the eponymous Bayes' theorem, it is given by
\begin{equation}
p(\bvartheta \, | \, \y) = \frac{p(\y \, | \, \bvartheta) p(\bvartheta)}{p(\y)}. \label{eq:posterior}
\end{equation}
Unfortunately, Equation \ref{eq:posterior} is usually intractable to calculate directly because of the integral $p(\y) = \int p(\y, \bvartheta) \text{d}\bvartheta$ in the denominator.
As such, the typical situation is that although the numerator is proportional to the posterior $p(\bvartheta \, | \, \y) \propto p(\y \, | \, \bvartheta) p(\bvartheta)$ and easy to evaluate, it is difficult to evaluate the posterior itself.

A great variety of computational methods have been developed to tackle this problem.
Briefly, the main categories are:

* Markov chain Monte Carlo (MCMC) is the most popular approach, and proceeds by simulating samples from a Markov chain with the posterior as its stationary distribution.
* Variational Bayes approaches assume the posterior distribution belongs to a certain class of functions and use optimisation to choose the best member of that class.

### Interplay between modelling and computation

Bayesian computation aspires to abstract away calculation of the posterior distribution from the analyst.
Modern computational techniques and software have made this aspiration a reality for many models.
However, computation of the posterior remains intractable for a substantial majority of models.
As such, the analyst need not only to be concerned with choosing a model suitable for the data, but also choosing a model for which the posterior is tractable in reasonable time.
It is in this sense, that there is an interplay between modelling and computation.
As computation improves, the space of models available to the analyst expands.

## Spatio-temporal statistics

In spatio-temporal statistics [@cressie2015statistics] we observe data indexed by spatial or temporal location.
In this thesis we assume that the spatial study region $\mathcal{S} \subseteq \mathbb{R}^2$ has two dimensions, corresponding to latitude and longitude.
Data may be associated to a point $s \in \mathcal{S}$ or area $A \subseteq \mathcal{S}$ in the study region.
The temporal study period $\mathcal{T} \subseteq \mathbb{R}$ can more generally be assumed to be one dimensional.

Correlation structure is an important feature of spatio-temporal data.

## Small-area estimation

## Model classes {#hierarchical-lgm-elgm}

### Hierarchical models {#hierarchical}

Real world data usually has hierarchical structure.
For example, there might be multiple measurements of the same unit (Figure \ref{fig:hierarchical-structure}).

Bayesian hierarchical models are comprised of multiple stages.
For example in a three-stage model, we have $\bvartheta = (\x, \btheta) = (x_1, \ldots, x_N, \theta_1, \ldots, \theta_m)$ such that
\begin{align}
\y &\sim p(\y \, | \, \x, \btheta), \\
\x &\sim p(\x \, | \, \btheta), \\
\btheta &\sim p(\btheta),
\end{align}
with posterior proportional to $p(\x, \btheta \, | \, \y) \propto p(\y \, | \, \x, \btheta) p(\x \, | \, \btheta) p(\btheta)$.

```{r hierarchical-structure, fig.cap="Data often has a hierarchical structure."}
knitr::include_graphics("figures/hierarchical-structure.pdf")
```

### Mixed effects

Mixed effect models contain both fixed and random effects.
Fixed and random effects have notoriously many different definitions [@gelman2005analysis], but nonetheless.
Partial pooling.
Structured random effects.

### Latent Gaussian models

Latent Gaussian models [LGMs; @rue2009approximate] are a class of three-stage Bayesian hierarchical models in which, loosely speaking, the middle layer is Gaussian.
More specifically, in an LGM, the likelihood is given by
\begin{align*}
y_i &\sim p(y_i \, | \, \eta_i, \theta_1), \quad i = 1, \ldots, n, \\
\mu_i &= \mathbb{E}(y_i \, | \, \eta_i) = g(\eta_i), \\
\eta_i &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}).
\end{align*}
The likelihood is $p(\y \, | \, \bmeta, \btheta_1) = \prod_{i = 1}^n p(y_i \, | \, \eta_i, \btheta_1)$, where $\bmeta = (\eta_1, \ldots, \eta_n)$.
Each response has conditional mean $\mu_i$ with inverse link function $g: \mathbb{R} \to \mathbb{R}$ such that $\mu_i = g(\eta_i)$.
The vector $\btheta_1 \in \mathbb{R}^{s_1}$, with $s_1$ assumed small, are additional parameters of the likelihood.
The structured additive predictor $\eta_i$ may include an intercept $\beta_0$, linear effects $\beta_j$ of the covariates $z_{ji}$, and unknown functions $f_k(\cdot)$ of the covariates $u_{ki}$.
The parameters $\beta_0$, $\{\beta_j\}$, $\{f_k(\cdot)\}$ are each assigned Gaussian priors, and can be collected into a vector $\x \in \mathbb{R}^N$ such that $\x \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}(\btheta_2)^{-1})$ where $\btheta_2 \in \mathbb{R}^{s_2}$ are further parameters, again with $s_2$ assumed small.
Let $\btheta = (\btheta_1, \btheta_2) \in \mathbb{R}^m$ with $m = s_1 + s_2$ be all hyperparameters, with prior $p(\btheta)$.
In total, the parameters of the LGM $\bvartheta = (\x, \btheta)$ comprise both the latent field and hyperparameters.

Spatio-temporal data are well suited to being modelled with LGMs.

### Extended latent Gaussian models

<!-- Strictly speaking, many-to-one is not an issue for `R-INLA`, the latent field is implemented as a concatenation of many vectors already e.g. for $\eta_i = \beta_0 + \phi_i$ with $i = 1, \ldots, n$ the latent field is $(\eta_1, \ldots, \eta_n, \beta_0, \phi_1, \ldots, \phi_n)^\top$ of dimension $2n + 1$ -->
<!-- Finn Lindgren is working on a method for non-linear predictors, called the [iterative INLA method](https://github.com/inlabru-org/inlabru/blob/55896f10d563c14e34cab577b29b733aac051f86/vignettes/method.Rmd). More [slides](https://informatique-mia.inrae.fr/reseau-resste/sites/default/files/2020-09/slides-Lindgren_Avignon2018.pdf) here -->
<!-- Thesis work of [Follestad](https://cds.cern.ch/record/639625/files/sis-2003-305.pdf) that stayed as a preprint -->

Some leading-edge spatio-temporal models used in small-area estimation fall outside the LGM class.
<!-- There are other complex models from [ecology](https://www.bioss.ac.uk/rsse/2013/September2013slides-Illian.pdf) that can't currently be fit using `R-INLA` -->
However, many of these models do fit into the class of extended latent Gaussian models (ELGMs) as proposed by @stringer2021fast.
By allowing many-to-one link functions, ELGMs facilitate modelling of non-linearities.
In particular, the structured additive predictor is redefined as $\bmeta = (\eta_1, \ldots \eta_{N_n})$, where $N_n \in \mathbb{N}$ is a function of $n$, and it is possible that $N_n \neq n$.
Each mean response $\mu_i$ now depends on some subset $\mathcal{J}_i \subseteq [N_n]$ of indices of $\bmeta$, with $\cup_{i = 1}^n \mathcal{J}_i = [N_n]$ and $1 \leq |\mathcal{J}_i| \leq N_n$, where $[N_n] = \{1, \ldots, N_n\}$.
The inverse link function $g(\cdot)$ is redefined for each observation to be a possibly many-to-one mapping $g_i: \mathbb{R}^{|\mathcal{J}_i|} \to \mathbb{R}$, such that $\mu_i = g_i(\bmeta_{\mathcal{J}_i})$.
Put together, ELGMs are then of the form
\begin{align*}
y_i &\sim p(y_i \, | \, \bmeta_{\mathcal{J}_i}, \btheta_1), \quad i = 1, \ldots, n, \\
\mu_i &= \mathbb{E}(y_i \, | \, \bmeta_{\mathcal{J}_i}) = g_i(\bmeta_{\mathcal{J}_i}), \\
\eta_j &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}), \quad j \in [N_n],
\end{align*}
with latent field and hyperparameter priors as in the LGM case.

In my experience, the ELGMs class is well suited to the demands of small-area estimation.
Indeed, I will use this class of models throughout the thesis.
In Chapter \@ref(beyond-borders) I consider...
While I am able to transform it to an LGM using the Poisson-multinomial transformation, the multinomial logistic regression model I use in Chapter \@ref(multi-agyw) is naturally written as an ELGM where each observation depends on the set of structured additive predictors corresponding to...
Finally, Chapter \@ref(naomi-aghq) I show that the Naomi small-area estimation model used to produce estimates of HIV indicators across over 35 countries is an ELGM.
