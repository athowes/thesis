---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: references.bib
---

```{r echo = FALSE}
options(scipen = 100)

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  dpi = 320,
  cache = TRUE,
  out.width = "95%",
  fig.align = 'center'
)
```

# Bayesian spatio-temporal statistics {#bayes-st}
\adjustmtc
\markboth{Bayesian spatio-temporal statistics}{}
<!-- For PDF output, include these two LaTeX commands after unnumbered chapter headings, otherwise the mini table of contents and the running header will show the previous chapter -->

## Bayesian statistics

Bayesian statistics is a mathematical paradigm for learning from data.
It is well suited for to the challenges posed by Section \@ref(surveillance) because it allows principled and flexible integration of data and scientific knowledge.
In this section I provide brief overview.
For a more complete introduction, I recommend @mcelreath2020statistical or @gelman2013bayesian.

### Bayesian modelling

At its best, the Bayesian paradigm allows the analyst focus on how best to model the data.
This is achieved by the construction of a generative model $p(\mathbf{y}, \boldsymbol{\mathbf{\phi}})$ for the observed data $\mathbf{y} = (y_1, \ldots, y_n)$ together with parameters $\boldsymbol{\mathbf{\phi}} = (\phi_1, \ldots, \phi_d)$.
Here $n$ is the dimension of the data and $d$ is the number of parameters.
The model is generative in the sense that one can simulate from it to generate draws
\begin{equation}
(\mathbf{y}, \boldsymbol{\mathbf{\phi}}) \sim p(\mathbf{y}, \boldsymbol{\mathbf{\phi}})
\end{equation}
If these draws differ too greatly from what the analyst would expect, then the generative model does not capture their scientific understanding, and can be refined.
In this way, models can be built iteratively, with complexity added gradually. 

The model is usually constructed from two parts, known respectively as the likelihood $p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\phi}})$ and the prior distribution $p(\boldsymbol{\mathbf{\phi}})$.
The joint distribution is obtained by the product $p(\mathbf{y}, \boldsymbol{\mathbf{\phi}}) = p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}})$.
The likelihood, as a function of $\boldsymbol{\mathbf{\phi}}$ with $\mathbf{y}$ fixed, reflects the probability of observing the data when the true value of the parameters is $\boldsymbol{\mathbf{\phi}}$.
The prior distribution encapsulates beliefs about the parameters $\boldsymbol{\mathbf{\phi}}$ before the data is observed.

Recommendations for specifying the prior distribution vary.
A central issue is the extent to which subjective information should be incorporated into the prior distribution, and thereby influence the posterior distribution.
Proponents of the objective Bayesian paradigm suggest that the prior distribution should be non-informative, so as not to introduce subjectivity into the analysis.
That said, we shall see that the distinction between likelihood and prior distribution can be blurred (Section \@ref(hierarchical-lgm-elgm)).
As such, it may be argued that issues of subjectivity are not unique to the prior distribution, and ultimately the challenge of specifying the data generating process is better thought of more holistically [@gelman2017prior].

### Bayesian computation

(ref:conjugate) An example of Bayesian modelling and computation for a simple one parameter model. The likelihood is $y_i \sim \text{Poisson}(\phi)$ for $i = 1, 2, 3$ and prior distribution is $\phi \sim \text{Gamma}(3, 1)$. Given observed data $\mathbf{y} = (1, 2, 3)$ the posterior distribution is available in closed form as $\text{Gamma}(9, 4)$. This is because the model is conjugate and the posterior distribution is in the same family as the prior distribution. Conjugate models are frequently used because of their convenience, in preference to other perhaps more appropriate models which might be more computationally demanding.

```{r conjugate, fig.cap="(ref:conjugate)"}
knitr::include_graphics("figures/bayesian/conjugate.png")
```

The posterior distribution $p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y})$ encapsulates probabilistic beliefs about the parameters given the observed data.
Using the eponymous Bayes' theorem, it is given by
\begin{equation}
p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) = \frac{p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}})}{p(\mathbf{y})}. (\#eq:posterior)
\end{equation}
Unfortunately, it is usually intractable to calculate Equation \@ref(eq:posterior) directly because of the potentially high-dimensional integral $p(\mathbf{y}) = \int p(\mathbf{y}, \boldsymbol{\mathbf{\phi}}) \text{d}\boldsymbol{\mathbf{\phi}}$ in the denominator, sometimes known as the marginal likelihood or evidence.
As such, although it is easy to evaluate $p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) \propto p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}})$, it is typically difficult to evaluate the posterior distribution itself.

A variety of computational methods have been developed to tackle this problem.
The main categories are:

1. **Sampling algorithms** These approaches look to generate samples from the posterior distribution, and are known as Monte Carlo methods, named after the casino [@robert2005monte].
The most popular is Markov chain Monte Carlo (MCMC), which proceeds by simulating from a Markov chain with the posterior distribution as its stationary distribution.
In this thesis, I make use of the No-U-Turn sampler [NUTS; @hoffman2014no], a Hamiltonian Monte Carlo [HMC; @duane1987hybrid; @neal2011mcmc] algorithm, implemented in the Stan [@carpenter2017stan] probabilistic programming language (PPL).
2. **Variational inference** In variational inference (VI), the posterior distribution is assumed to belong to a particular class of functions.
Optimisation algorithms are then used to choose the best member of that class.
Though VI is fast, it may not be accurate [@yao2018yes].
3. **Expectation maximisation**
4. **Deterministic approximations** These approximations are the subject of Chapter \@ref(naomi-aghq).

### Interplay between modelling and computation

Bayesian computation aspires to abstract away calculation of the posterior distribution from the analyst.
Modern computational techniques and software have made this aspiration a reality for many models.
However, computation of the posterior distribution remains intractable for a majority of models.
As such, the analyst need not only to be concerned with choosing a model suitable for the data, but also choosing a model for which the posterior distribution is tractable in reasonable time.
As such, there is an important interplay between modelling and computation, wherein models are bound by the limits of computation.
As computation improves, the space of models available to the analyst expands.

## Spatio-temporal statistics

In spatio-temporal statistics [@cressie2015statistics], we observe data indexed by spatial or temporal location.
In this thesis we assume that the spatial study region $\mathcal{S} \subseteq \mathbb{R}^2$ has two dimensions, corresponding to latitude and longitude.
Data may be associated to a point $s \in \mathcal{S}$ or area $A \subseteq \mathcal{S}$ in the study region.
The temporal study period $\mathcal{T} \subseteq \mathbb{R}$ can more generally be assumed to be one dimensional.
Similarly, data may be associated to a point $t \in \mathcal{T}$ or period of time $T \in \mathcal{T}$.

Spatio-temporal data has some important properties.
Foremost among them is correlation structure.
Tobler's first law of geography, also expressed by @fisher1936design, is that "everything is related to everything else, but near things are more related than distant things" [@tobler1970computer].
This law extends not only to space but also time.

<!-- Add figure here to demonstrate spatial data -->

## Model classes {#hierarchical-lgm-elgm}

### Hierarchical models {#hierarchical}

(ref:hierarchical-structure) A simple example of group structure within data. Each individual $i = 1, \ldots, n$ is associated to $m_i$ observations $y_{i1}, \ldots, y_{im_i}$.

```{r hierarchical-structure, fig.cap="(ref:hierarchical-structure)"}
knitr::include_graphics("figures/bayesian/hierarchical-structure.png")
```

Often it is natural to group some observations together.
Bayesian hierarchical or multilevel models, comprised of multiple stages, allow for natural handling data of this sort, even with complex nested or crossed grouping structures.
In a three-stage hierarchical model, we partition the parameters so that $\boldsymbol{\mathbf{\phi}} = (\mathbf{x}, \boldsymbol{\mathbf{\theta}})$.
I refer to $\mathbf{x} = (x_1, \ldots, x_n)$ as the latent field, and $\boldsymbol{\mathbf{\theta}} = (\theta_1, \ldots, \theta_m)$ as the hyperparameters.
The generative model for data $\mathbf{y}$ is then
\begin{align}
\mathbf{y} &\sim p(\mathbf{y} \, | \, \mathbf{x}, \boldsymbol{\mathbf{\theta}}), \\
\mathbf{x} &\sim p(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}), \\
\boldsymbol{\mathbf{\theta}} &\sim p(\boldsymbol{\mathbf{\theta}}),
\end{align}
with posterior distribution proportional to $p(\mathbf{x}, \boldsymbol{\mathbf{\theta}} \, | \, \mathbf{y}) \propto p(\mathbf{y} \, | \, \mathbf{x}, \boldsymbol{\mathbf{\theta}}) p(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}) p(\boldsymbol{\mathbf{\theta}})$.

For example, Figure \@ref(fig:hierarchical-structure) illustrates a case where each individual $i = 1, \ldots, n$ in a study is observed $m_i$ times.
Observations $y_{i1}, \ldots y_{im_i}$ of the same individual are grouped together, and are more likely to have more similar properties than observations of different individuals.
Hierarchical models often control over if and how information is shared between groups.

1. **Complete pooling** In this model, the group structure is ignored and all $\sum_{i = 1}^n m_i$ observations are treated as independent and identically distributed
\begin{align}
y_{ij} \sim \mathcal{N}(\mu, \sigma), \\
(\mu, \sigma) \sim p(\mu, \sigma).
\end{align}
2. **No pooling** Alternatively, the groups can be modelled entirely separately with group specific mean $\mu_i$ and standard deviation $\sigma_i$ parameters
\begin{align}
y_{ij} \sim \mathcal{N}(\mu_i, \sigma_i), \\
(\mu_i, \sigma_i) \sim p(\mu_i, \sigma_i).
\end{align}
3. **Partial pooling** In this model, some amount of information is shared between the groups
\begin{align}
y_{ij} &\sim \mathcal{N}(\mu_i, \sigma), \\
\mu_i &= \beta + u_i, \\
\beta &\sim p(\beta), \\
\mathbf{u} &\sim p(\mathbf{u}), \\
\sigma &\sim p(\sigma),
\end{align}
where the vector $\mathbf{u} = (u_1, \ldots, u_n)$.

### Mixed effects models

Fixed effects refer to those elements of the latent field which are constant across groups.
Random effects refer to those elements of the latent field which vary across groups.
These terms have notoriously many different, and incompatible, definitions which can cause confusion [@gelman2005analysis].
I nonetheless find them useful to introduce here.

For concreteness, in the partial pooling model above, the latent field is $\mathbf{x} = (\beta, u_1, \ldots, u_n)$.
The scalar $\beta$ is a fixed effect which applies to all $n$ groups.
The vector $\mathbf{u}$ are random effects which alter the mean differently for each group.
The only hyperparameter is the standard deviation $\theta = \sigma$.

Random effects can also be structured to share information between some groups more than others.
In spatio-temporal statistics, structured spatial and temporal random effects are often used to impose smoothness.
Spatial random effects are the subject of Chapter \@ref(beyond-borders).

### Latent Gaussian models

Latent Gaussian models [LGMs; @rue2009approximate] are a class of three-stage Bayesian hierarchical models in which the middle layer is Gaussian.
To be more precise, in an LGM, the likelihood is given by
\begin{align*}
y_i &\sim p(y_i \, | \, \eta_i, \boldsymbol{\mathbf{\theta}}_1), \quad i = 1, \ldots, n, \\
\mu_i &= \mathbb{E}(y_i \, | \, \eta_i) = g(\eta_i), \\
\eta_i &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}).
\end{align*}
The likelihood is given by a product $p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\eta}}, \boldsymbol{\mathbf{\theta}}_1) = \prod_{i = 1}^n p(y_i \, | \, \eta_i, \boldsymbol{\mathbf{\theta}}_1)$, where $\boldsymbol{\mathbf{\eta}} = (\eta_1, \ldots, \eta_n)$.
Each response has conditional mean $\mu_i$ with inverse link function $g: \mathbb{R} \to \mathbb{R}$ such that $\mu_i = g(\eta_i)$.
The vector $\boldsymbol{\mathbf{\theta}}_1 \in \mathbb{R}^{s_1}$, with $s_1$ assumed small, are additional parameters of the likelihood.
The structured additive predictor $\eta_i$ may include an intercept $\beta_0$, fixed effects $\beta_j$ of the covariates $z_{ji}$, and random effects $f_k(\cdot)$ of the covariates $u_{ki}$.
The parameters $\beta_0$, $\{\beta_j\}$, $\{f_k(\cdot)\}$ are each assigned Gaussian prior distributions, and can be collected into a vector $\mathbf{x} \in \mathbb{R}^N$ such that $\mathbf{x} \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}(\boldsymbol{\mathbf{\theta}}_2)^{-1})$ where $\boldsymbol{\mathbf{\theta}}_2 \in \mathbb{R}^{s_2}$ are further hyperparameters, again with $s_2$ assumed small.
Let $\boldsymbol{\mathbf{\theta}} = (\boldsymbol{\mathbf{\theta}}_1, \boldsymbol{\mathbf{\theta}}_2) \in \mathbb{R}^m$ with $m = s_1 + s_2$ be all hyperparameters, with prior distribution $p(\boldsymbol{\mathbf{\theta}})$.

### Extended latent Gaussian models

<!-- Strictly speaking, many-to-one is not an issue for `R-INLA`, the latent field is implemented as a concatenation of many vectors already e.g. for $\eta_i = \beta_0 + \phi_i$ with $i = 1, \ldots, n$ the latent field is $(\eta_1, \ldots, \eta_n, \beta_0, \phi_1, \ldots, \phi_n)^\top$ of dimension $2n + 1$ -->
<!-- Finn Lindgren is working on a method for non-linear predictors, called the [iterative INLA method](https://github.com/inlabru-org/inlabru/blob/55896f10d563c14e34cab577b29b733aac051f86/vignettes/method.Rmd). More [slides](https://informatique-mia.inrae.fr/reseau-resste/sites/default/files/2020-09/slides-Lindgren_Avignon2018.pdf) here -->
<!-- Thesis work of [Follestad](https://cds.cern.ch/record/639625/files/sis-2003-305.pdf) that stayed as a preprint -->

Extended latent Gaussian models [ELGMs; @stringer2021fast] facilitate modelling of LGMs with greater non-linearities.
In particular, the structured additive predictor is redefined as $\boldsymbol{\mathbf{\eta}} = (\eta_1, \ldots \eta_{N_n})$, where $N_n \in \mathbb{N}$ is a function of $n$, and it is possible that $N_n \neq n$.
Each mean response $\mu_i$ now depends on some subset $\mathcal{J}_i \subseteq [N_n]$ of indices of $\boldsymbol{\mathbf{\eta}}$, with $\cup_{i = 1}^n \mathcal{J}_i = [N_n]$ and $1 \leq |\mathcal{J}_i| \leq N_n$, where $[N_n] = \{1, \ldots, N_n\}$.
The inverse link function $g(\cdot)$ is redefined for each observation to be a possibly many-to-one mapping $g_i: \mathbb{R}^{|\mathcal{J}_i|} \to \mathbb{R}$, such that $\mu_i = g_i(\boldsymbol{\mathbf{\eta}}_{\mathcal{J}_i})$.
Put together, ELGMs are of the form
\begin{align*}
y_i &\sim p(y_i \, | \, \boldsymbol{\mathbf{\eta}}_{\mathcal{J}_i}, \boldsymbol{\mathbf{\theta}}_1), \quad i = 1, \ldots, n, \\
\mu_i &= \mathbb{E}(y_i \, | \, \boldsymbol{\mathbf{\eta}}_{\mathcal{J}_i}) = g_i(\boldsymbol{\mathbf{\eta}}_{\mathcal{J}_i}), \\
\eta_j &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}), \quad j = 1, \ldots, N_n,
\end{align*}
with latent field and hyperparameter prior distributions as in the LGM case.

The ELGM class is well suited to small-area estimation of HIV indicators.
Indeed, this class of models is used throughout the thesis.
While it can be transformed to an LGM using the Poisson-multinomial transformation [@baker1994multinomial] the multinomial logistic regression model used in Chapter \@ref(multi-agyw) is naturally written as an ELGM where each observation depends on the set of structured additive predictors corresponding to the set of multinomial observations.
In Chapter \@ref(naomi-aghq), the Naomi small-area estimation model used to produce estimates of HIV indicators is shown to have the features of an ELGM.

## Survey methods {#survey}

Large national household surveys provide the highest quality population-level information about HIV indicators in SSA.
Demographic and Health Surveys (DHS) are funded by the United States Agency for International Development (USAID) and run every three to five years in most countries.
Population-based HIV Impact Assessment (PHIA) surveys are funded by PEPFAR and run every two to three years in high HIV burden countries.

### Survey notation and key terms

Consider a population of individuals $i = 1, \ldots, N$ with outcomes of interest $y_i$.
A census is a type of survey where all individuals are sampled.
Supposing responses from all individuals were recorded, then the population means can be calculated direct.
For example, if $G_i = G(y_i)$ then the population mean of $G$ is
\begin{equation}
\bar G = \frac{1}{N} \sum_{i = 1}^N G(y_i).
\end{equation}

In practice, it is usually too expensive to run a census.
Instead, only a subset of the individuals are sampled.
Furthermore, only a subset of those sampled have their outcome recorded due to nonresponse or otherwise.
Let $S_i$ be an indicator for whether or not individual $i$ is sampled, and $R_i$ be an indicator for whether or not $y_i$ is recorded.
If $S_i = 0$ then $R_i = 0$.
If $S_i = 1$ then individual $i$ may not respond such that $R_i = 0$.
The population mean may be estimated directly based on the recorded subset of the population by
\begin{equation}
\bar G_R = \frac{\sum_{i = 1}^N R_i G(y_i)}{\sum_{i = 1}^N R_i}, \label{eq:direct}
\end{equation}
where $m_R = \sum_{i = 1}^N R_i$ is the recorded sample size.

A probability sample refers to the case when individuals are selected to be included in the survey at random.
In a non-probability sample, inclusion or exclusion from the survey is deterministic.
A simple random sample (SRS) is a probability sample where the sampling probability for each individual is equal, so that $P(S_i = 1) = 1 / N$.
The survey design is called complex when the sampling probabilities for each individual vary, such that $P(S_i = 1) = \pi_i$ with $\sum_{i = 1}^N \pi_i = 1$ and $\pi_i > 0$.

Complex survey designs can offer both greater practicality and statistical efficiency than a SRS.
However, particular care is required in analysing data collected using complex survey designs.
Under a complex design, failing to take into account the unequal sampling probabilities will result in bias.
That said, even for a SRS, bias can be caused by nonresponse.

### Survey design

<!-- Add figure here to demonstrate sampling of DHS data -->

The DHS [@measure2012sampling] employs a two-stage sampling procedure.
In the first stage, ennumeration areas (EAs) from a recently conducted census are typically used as the primary sampling unit (PSU).
The EAs are then stratified by region, as well as urban-rural.
After appropriate sample sizes are determined, EAs sampled with probability proportional to size (PPS) measured 
In the second stage, the secondary sampling units (SSUs) are households.
All households in the selected EAs are listed, before being sampled systematically.
Finally, each selected household is visited, and all adults are interviewed.

The probability an individual is sampled is equal to the probability their household is sampled.
The first-stage sampling probability of the $j$th cluster in stratum $h$ given by
$$
\pi_{1hj} = n_h \times \frac{N_{hj}}{\sum_j N_{hj}},
$$
where $N_{hj}$ is the number of households and $n_h$ be the number of clusters selected in stratum $h$.
The second-stage sampling probability each household within the $i$th cluster in stratum $h$ is
$$
\pi_{1hj} = \frac{n_{hj}}{N_{hj}},
$$
where $n_{hj}$ is the numer of households selected in cluster $j$ and stratum $h$.
That is, each household in the cluster has equal selection probability.
The overall selection probability of each household in cluster $j$ of stratum $h$ is $\pi_{hi} = \pi_{1hj} \times \pi_{2hj}$.

### Survey analysis

Suppose a complex survey is run with sampling probabilities $\pi_i$.
The most popular method for taking into account that some individuals are more likely to be included in the survey than others is to overweight the responses of those unlikely to be included, and underweight the responses of those likely to be included.
This can be achieved using design weights $\delta_i = 1 / \pi_i$, which can be thought of as the number of individuals in the population represented by the $i$th sampled individual.
Let $P(R_i = 1 \, | \, S_i = 1) = \upsilon_i$ be the probability of response for sampled individual $i$.
The problem of nonresponse can be treated in the same way using nonresponse weights $\gamma_i = 1 / \upsilon_i$, which analogously can be thought of as the number of sampled individuals represented by the $i$th recorded individual.
Multiplying the design and nonresponse weights gives survey weights $\omega_i = \delta_i \times \gamma_i$.

A weighted estimate [@hajek1971discussion] of the population mean using the survey weights $\omega_i$ is given by
\begin{equation}
\bar G_\omega = \frac{\sum_{i = 1}^N \omega_j R_i G(y_i)}{\sum_{i = 1}^N \omega_i R_i}. \label{eq:hajek}
\end{equation}
Decomposing the additive error of this estimate provides useful intuition as to the potential benefits of survey weighting.
Following @meng2018statistical then under SRS
\begin{align}
\bar G_\omega - \bar G &= \frac{\mathbb{E}(\omega_i R_i G_i)}{\mathbb{E}(\omega_i R_i)} - \mathbb{E}(G_i) = \frac{\mathbb{C}(\omega_i R_i G_i)}{\mathbb{E}(\omega_i R_i)} \\ 
&= \rho_{R_\omega, G} \times \sqrt{\frac{N - m_{R_\omega}}{m_{R_\omega}}} \times \sigma_G,
\end{align}
where $R_\omega = \omega R$.
The data defect correlation (DDC) $\rho_{R_\omega, G}$ measures the correlation between the weighted recording mechanism and given function of the outcome of interest.
To minimise the DDC then $G \perp \!\!\! \perp R_\omega$.
The data scarcity $\sigma_{R_\omega} = \sqrt{(N - m_{R_\omega})/m_{R_\omega}}$ measures the effective proportion of the population who have been recorded.
The problem difficultly $\sigma_G$ measures the intrinsic difficulty of the estimation problem, and is independent of the sampling or analysis method.

For simplicity, let $G(y_i) = y_i$ and each $y_i \in \{0, 1\}$.
We weight then model following @chen2014use.
While this approach acknowledges the survey design, it has some important limitations.
We ignore clustering structure.
All of this isn't great and that someone should figure this out [@gelman2007struggles].
