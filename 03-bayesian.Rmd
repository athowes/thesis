---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: references.bib
---

```{r echo = FALSE}
options(scipen = 100)

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  dpi = 320,
  cache = TRUE,
  out.width = "95%",
  fig.align = 'center'
)
```

# Bayesian spatio-temporal statistics {#bayes-st}
\adjustmtc
\markboth{Bayesian spatio-temporal statistics}{}
<!-- For PDF output, include these two LaTeX commands after unnumbered chapter headings, otherwise the mini table of contents and the running header will show the previous chapter -->

## Bayesian statistics

Bayesian statistics is a mathematical paradigm for learning from data.
It is well suited for to the challenges posed by Section \ref{sec:surveillance} because it allows principled and flexible integration of data and scientific knowledge.
In this section I provide brief overview.
For a more complete introduction, I recommend @mcelreath2020statistical or @gelman2013bayesian.

### Bayesian modelling

At its best, the Bayesian paradigm allows the analyst focus on how best to model the data.
This is achieved by the construction of a generative model $p(\y, \bphi)$ for the observed data $\y = (y_1, \ldots, y_n)$ together with parameters $\bphi = (\phi_1, \ldots, \phi_d)$, where $n$ is the dimension of the data and $d$ is the number of parameters.
The model is generative in the sense that one can simulate from it to generate draws
\begin{equation}
(\y, \bphi) \sim p(\y, \bphi)
\end{equation}
If these draws differ too greatly from what the analyst would expect, then the generative model does not capture their scientific understanding, and can be refined.
In this way, models can be built iteratively, with complexity added gradually. 

The model is usually constructed from two parts, known respectively as the likelihood $p(\y \, | \, \bphi)$ and the prior distribution $p(\bphi)$.
The joint distribution is obtained by the product $p(\y, \bphi) = p(\y \, | \, \bphi) p(\bphi)$.
The likelihood, as a function of $\bphi$ with $\y$ fixed, reflects the probability of observing the data when the true value of the parameters is $\bphi$.
The prior distribution encapsulates beliefs about the parameters $\bphi$ before the data is observed.

Recommendations for specifying the prior distribution vary.
A central issue is the extent to which subjective information should be incorporated into the prior distribution, and thereby influence the posterior distribution.
Proponents of the objective Bayesian paradigm suggest that the prior distribution should be non-informative, so as not to introduce subjectivity into the analysis.
That said, we shall see that the distinction between likelihood and prior distribution can be blurred (Section \@ref(hierarchical-lgm-elgm)).
As such, it may be argued that issues of subjectivity are not unique to the prior distribution, and ultimately the challenge of specifying the data generating process is better thought of more holistically [@gelman2017prior].

### Bayesian computation

(ref:conjugate) A simple example of Bayesian modelling and computation for the observation model $y_i \sim \text{Poisson}(\phi)$ for $i = 1, 2, 3$ with prior $\phi \sim \text{Gamma}(3, 1)$. Given observed data $\y = (1, 2, 3)$ the posterior is available in closed form as $\text{Gamma}(9, 4)$ because the model is so-called conjugate. Though it is unlikely that the best model for a particular problem will have the conjugacy property, conjugate models are still frequently used because of their convenience.

```{r conjugate, fig.cap="(ref:conjugate)"}
knitr::include_graphics("figures/bayesian/conjugate.png")
```

The posterior distribution $p(\bphi \, | \, \y)$ encapsulates probabilistic beliefs about the parameters given the observed data.
Using the eponymous Bayes' theorem, it is given by
\begin{equation}
p(\bphi \, | \, \y) = \frac{p(\y \, | \, \bphi) p(\bphi)}{p(\y)}. \label{eq:posterior}
\end{equation}
Unfortunately, it is usually intractable to calculate Equation \@ref(eq:posterior) directly because of the integral $p(\y) = \int p(\y, \bphi) \text{d}\bphi$ in the denominator, sometimes known as the marginal likelihood or evidence.
As such, although it is easy to evaluate $p(\bphi \, | \, \y) \propto p(\y \, | \, \bphi) p(\bphi)$, it is typically difficult to evaluate the posterior distribution itself.

A variety of computational methods have been developed to tackle this problem.
Briefly, the main categories are:

1. **Sampling algorithms** Known as Monte Carlo [@robert2005monte] algorithms after the casino, these approaches look to generate samples from the posterior distribution.
The most popular is Markov chain Monte Carlo (MCMC), which proceeds by simulating from a Markov chain with the posterior distribution as its stationary distribution.
In this thesis, I make use of the No-U-Turn sampler (NUTS), a Hamiltonian Monte Carlo (HMC) algorithm, implemented in the Stan probabilistic programming language (PPL).
2. **Variational Bayes** In variational Bayes, the posterior distribution is assumed to belong to a particular class of functions and use optimisation to choose the best member of that class.

### Interplay between modelling and computation

Bayesian computation aspires to abstract away calculation of the posterior distribution from the analyst.
Modern computational techniques and software have made this aspiration a reality for many models.
However, computation of the posterior distribution remains intractable for a majority of models.
As such, the analyst need not only to be concerned with choosing a model suitable for the data, but also choosing a model for which the posterior distribution is tractable in reasonable time.
As such, there is an important interplay between modelling and computation, wherein models are bound by the limits of computation.
As computation improves, the space of models available to the analyst expands.

## Spatio-temporal statistics

In spatio-temporal statistics [@cressie2015statistics], we observe data indexed by spatial or temporal location.
In this thesis we assume that the spatial study region $\mathcal{S} \subseteq \mathbb{R}^2$ has two dimensions, corresponding to latitude and longitude.
Data may be associated to a point $s \in \mathcal{S}$ or area $A \subseteq \mathcal{S}$ in the study region.
The temporal study period $\mathcal{T} \subseteq \mathbb{R}$ can more generally be assumed to be one dimensional.
Similarly, data may be associated to a point $t \in \mathcal{T}$ or period of time $T \in \mathcal{T}$.

Spatio-temporal data has some important properties.
Foremost among them is correlation structure.
Tobler's first law of geography, earlier expressed by @fisher1936design, is that "everything is related to everything else, but near things are more related than distant things" [@tobler1970computer].
This law extends not only to space but also time.

## Model classes {#hierarchical-lgm-elgm}

### Hierarchical models {#hierarchical}

```{r hierarchical-structure, fig.cap="Data often has a hierarchical structure."}
knitr::include_graphics("figures/bayesian/hierarchical-structure.pdf")
```

Real world data usually has hierarchical structure.
For example, we might have a study (Figure \ref{fig:hierarchical-structure}) where each individual $j = 1, \ldots, J$ has a group of $n_j$ observations.
Then, it would be inappropriate to treat all $\sum_j n_j$ observations as being independent and identically distributed (IID).
Observations belonging to the same group, that is of the same individual, are likely to be more similar than observations from different groups, which refer to different individuals.

Bayesian hierarchical or multilevel models, comprised of multiple stages, allow handling data of this sort.
For example, in a three-stage hierarchical model, we partition the parameters so that $\bphi = (\x, \btheta)$.
I refer to $\x = (x_1, \ldots, x_N)$ as the latent field, and $\btheta = (\theta_1, \ldots, \theta_m)$ as the hyperparameters.
The generative model for data $\y$ is then
\begin{align}
\y &\sim p(\y \, | \, \x, \btheta), \\
\x &\sim p(\x \, | \, \btheta), \\
\btheta &\sim p(\btheta),
\end{align}
with posterior distribution proportional to $p(\x, \btheta \, | \, \y) \propto p(\y \, | \, \x, \btheta) p(\x \, | \, \btheta) p(\btheta)$.
Referring back to the previous example, we may informally think about the latent field as modelling properties of individuals, and the hyperparameters as modelling shared properties of individuals or the observation process.

### Mixed effects models

Fixed effects refer to parts of the latent field which are constant across groups.
Random effects refer to parts of the latent field which vary across groups.
Though these terms have notoriously many different definitions [@gelman2005analysis], I nonetheless find them useful to introduce here.
No pooling, partial pooling, complete pooling.
Structured random effects.

### Latent Gaussian models

Latent Gaussian models [LGMs; @rue2009approximate] are a class of three-stage Bayesian hierarchical models in which, loosely speaking, the middle layer is Gaussian.
More specifically, in an LGM, the likelihood is given by
\begin{align*}
y_i &\sim p(y_i \, | \, \eta_i, \btheta_1), \quad i = 1, \ldots, n, \\
\mu_i &= \mathbb{E}(y_i \, | \, \eta_i) = g(\eta_i), \\
\eta_i &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}).
\end{align*}
The likelihood is given by a product $p(\y \, | \, \bmeta, \btheta_1) = \prod_{i = 1}^n p(y_i \, | \, \eta_i, \btheta_1)$, where $\bmeta = (\eta_1, \ldots, \eta_n)$.
Each response has conditional mean $\mu_i$ with inverse link function $g: \mathbb{R} \to \mathbb{R}$ such that $\mu_i = g(\eta_i)$.
The vector $\btheta_1 \in \mathbb{R}^{s_1}$, with $s_1$ assumed small, are additional parameters of the likelihood.
The structured additive predictor $\eta_i$ may include an intercept $\beta_0$, fixed effects $\beta_j$ of the covariates $z_{ji}$, and random effects $f_k(\cdot)$ of the covariates $u_{ki}$.
The parameters $\beta_0$, $\{\beta_j\}$, $\{f_k(\cdot)\}$ are each assigned Gaussian prior distributions, and can be collected into a vector $\x \in \mathbb{R}^N$ such that $\x \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}(\btheta_2)^{-1})$ where $\btheta_2 \in \mathbb{R}^{s_2}$ are further hyperparameters, again with $s_2$ assumed small.
Let $\btheta = (\btheta_1, \btheta_2) \in \mathbb{R}^m$ with $m = s_1 + s_2$ be all hyperparameters, with prior distribution $p(\btheta)$.

Spatio-temporal data are well suited to being modelled with LGMs.

### Extended latent Gaussian models

<!-- Strictly speaking, many-to-one is not an issue for `R-INLA`, the latent field is implemented as a concatenation of many vectors already e.g. for $\eta_i = \beta_0 + \phi_i$ with $i = 1, \ldots, n$ the latent field is $(\eta_1, \ldots, \eta_n, \beta_0, \phi_1, \ldots, \phi_n)^\top$ of dimension $2n + 1$ -->
<!-- Finn Lindgren is working on a method for non-linear predictors, called the [iterative INLA method](https://github.com/inlabru-org/inlabru/blob/55896f10d563c14e34cab577b29b733aac051f86/vignettes/method.Rmd). More [slides](https://informatique-mia.inrae.fr/reseau-resste/sites/default/files/2020-09/slides-Lindgren_Avignon2018.pdf) here -->
<!-- Thesis work of [Follestad](https://cds.cern.ch/record/639625/files/sis-2003-305.pdf) that stayed as a preprint -->

Some models used in small-area estimation fall outside the LGM class.
<!-- There are other complex models from [ecology](https://www.bioss.ac.uk/rsse/2013/September2013slides-Illian.pdf) that can't currently be fit using `R-INLA` -->
Many of these models do fit into the class of extended latent Gaussian models (ELGMs) as proposed by @stringer2021fast.
By allowing many-to-one link functions, ELGMs facilitate modelling of non-linearities.
In particular, the structured additive predictor is redefined as $\bmeta = (\eta_1, \ldots \eta_{N_n})$, where $N_n \in \mathbb{N}$ is a function of $n$, and it is possible that $N_n \neq n$.
Each mean response $\mu_i$ now depends on some subset $\mathcal{J}_i \subseteq [N_n]$ of indices of $\bmeta$, with $\cup_{i = 1}^n \mathcal{J}_i = [N_n]$ and $1 \leq |\mathcal{J}_i| \leq N_n$, where $[N_n] = \{1, \ldots, N_n\}$.
The inverse link function $g(\cdot)$ is redefined for each observation to be a possibly many-to-one mapping $g_i: \mathbb{R}^{|\mathcal{J}_i|} \to \mathbb{R}$, such that $\mu_i = g_i(\bmeta_{\mathcal{J}_i})$.
Put together, ELGMs are then of the form
\begin{align*}
y_i &\sim p(y_i \, | \, \bmeta_{\mathcal{J}_i}, \btheta_1), \quad i = 1, \ldots, n, \\
\mu_i &= \mathbb{E}(y_i \, | \, \bmeta_{\mathcal{J}_i}) = g_i(\bmeta_{\mathcal{J}_i}), \\
\eta_j &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}), \quad j = 1, \ldots, N_n,
\end{align*}
with latent field and hyperparameter prior distributions as in the LGM case.

The ELGM class is well suited to small-area estimation of HIV indicators.
Indeed, this class of models is used throughout the thesis.
While it can be transformed to an LGM using the Poisson-multinomial transformation [@baker1994multinomial], the multinomial logistic regression model used in Chapter \@ref(multi-agyw) is naturally written as an ELGM in which each observation depends on the set of structured additive predictors corresponding to each multinomial observation.
In Chapter \@ref(naomi-aghq), the Naomi small-area estimation model used to produce estimates of HIV indicators is shown to have the features of an ELGM.

## Survey methods

Large national household surveys provide the highest quality population-level information about HIV indicators in SSA.
Demographic and Health Surveys (DHS) are funded by USAID and run every three to five years in most countries.
Population-based HIV Impact Assessment (PHIA) surveys are funded by PEPFAR and run in high HIV burden countries.

### Survey notation and key terms

Consider a population of individuals $i = 1, \ldots, N$ and outcomes of interest $y_i$.
If a census were run in, and all individuals responded, then the population mean $\bar y$ could be calculated exactly by
\begin{equation}
\bar y = \frac{1}{N} \sum_{i = 1}^N y_i.
\end{equation}
In practice, running a census is impractical and rarely possible.
Usually, only a subset of the individuals are sampled, and only a subset of those have their outcome recorded due to non-response.
Let $S_i$ be an indicator for whether or not individual $i$ is sampled, and $R_i$ be an indicator for whether or not $y_i$ is recorded.
If $S_i = 0$ then $R_i = 0$, but if $S_i = 1$ then individual $i$ may not respond such that $R_i = 0$.
The population mean may be estimated directly based on the recorded subset of the population by
\begin{equation}
\bar y_R = \frac{\sum_{i = 1}^N R_i y_i}{\sum_{i = 1}^N R_i} = \frac{\sum_{i = 1}^N R_i y_i}{m_R}, \label{eq:direct}
\end{equation}
where $m_R$ is the recorded sample size.

A probability sample refers to the case when individuals are selected to be included in the survey at random.
In a non-probability sample, inclusion or exclusion from the survey is deterministic.
A simple random sample (SRS) is a probability sample where the sampling probability for each individual is equal, so that $\mathbb{P}(S_i = 1) = 1 / N$ for all $i$.
The survey design is called complex when the sampling probabilities for each individual vary, such that $\mathbb{P}(S_i = 1) = \pi_i$ with $\sum_{i = 1}^N \pi_i = 1$ and $\pi_i > 0$ for all $i$.
Complex survey designs can offer both greater practicality and statistical efficiency than SRS.
However, particular care is required in analysing data collected using complex survey designs.
In particular, the direct estimator given in Equation \@ref(eq:direct), which does not take into account the sampling probabilities, is biased even in the case of no non-response.
That said, even for SRS, non-response is a ubiquitous problem which also must be treated with care.
Let $\mathbb{P}(R_i = 1 \, | \, S_i = 1) = \upsilon_i$ be the probability of response of individual $i$ given inclusion in the survey.

### Survey design

1. Stratification into administrative units
2. Primary sampling units (PSUs) and their populations
3. Sample PSUs proportional to their population
4. Select households using systematic sampling
5. Visit each household and interview all adults

### Survey analysis

Suppose a complex survey is run with sampling probabilities $\pi_i$.
Design weights $\delta_i = 1 / \pi_i$ express the number of population units represented by the $i$th sampled unit.
Non-response weights $\gamma_i = 1 / \upsilon_i$ express the number of sampled units represented by the $i$th recorded unit. 
Survey weights $\omega_i = \delta_i \times \gamma_i$ express the number of population units represented by the $i$th recorded unit.
A weighted estimate [@hajek1971discussion] of the population mean is then
\begin{equation}
\bar y_\omega = \frac{\sum_{j = 1}^N \omega_j R_j y_j}{\sum_{j = 1}^N \omega_j R_j}. \label{eq:hajek}
\end{equation}

Decomposing the additive error of Equation \@ref(eq:hajek) provides useful intuition as to the utility of survey weighting [@meng2018statistical]
\begin{align}
\bar y_\omega - \bar y &= \frac{\mathbb{E}(R_i Y_i)}{\mathbb{E}(R_i)} - \mathbb{E}(Y_i) \\ 
&= \frac{\mathbb{C}(R_i Y_i)}{\mathbb{E}(R_i)} \\
&= \rho_{Y, R} \times \sqrt{\frac{N - m_R}{m_R}} \times \sigma_y.
\end{align}
The data quality $\rho_{Y, R}$ measures the correlation between the outcome of interest and the recording mechanism.
The data quantity $\sqrt{(N - m_R)/m_R}$
The problem difficultly $\sigma_y$ measures the intrinsic difficulty of the estimation problem, and is entirely independent of the sampling or analysis methods used.
Under SRS then.

The Kish effective sample size (ESS) [@kish1965survey] is
\begin{equation}
m^\star = \frac{\left(\sum_{i = 1}^N \omega_i \right)^2}{\sum_{i = 1}^N \omega_i^2}.
\end{equation}
We weight then model following @chen2014use.
We ignore clustering structure.
Discuss how all of this isn't great and that someone should figure this out.

### An adaptive design proposal

@bradley2023alsd
