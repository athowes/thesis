---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: references.bib
---

```{r echo = FALSE}
options(scipen = 100)

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  dpi = 320,
  cache = TRUE,
  out.width = "95%",
  fig.align = 'center'
)
```

# Bayesian spatio-temporal statistics {#bayes-st}
\adjustmtc
\markboth{Bayesian spatio-temporal statistics}{}
<!-- For PDF output, include these two LaTeX commands after unnumbered chapter headings, otherwise the mini table of contents and the running header will show the previous chapter -->

## Bayesian statistics

Bayesian statistics is a mathematical paradigm for learning from data.
It is especially well suited to facing the challenges posed by Section \@ref(surveillance).
First, because it allows for principled and flexible integration of data with domain knowledge.
Second, because uncertainty is handled as an integral part of the Bayesian paradigm.
In this section I provide a brief overview.
For a more complete introduction, I recommend @gelman2013bayesian or @mcelreath2020statistical.

### Bayesian modelling

At its best, the Bayesian paradigm allows the analyst focus on how best to model the data.
This is achieved mathematically by the construction of a generative model $p(\mathbf{y}, \boldsymbol{\mathbf{\phi}})$ for the observed data $\mathbf{y}$ together with parameters $\boldsymbol{\mathbf{\phi}}$, where the notation $p(\cdot)$ denotes a probability distribution.
I assume the data to be an $n$-vector $(y_1, \ldots, y_n)$, and the parameters to be a $n$-vector $(\phi_1, \ldots, \phi_d)$.
Choice of the parameters used to model the data will depend upon the requirements of the analysis.

The model is generative in the sense that it can be simulated from to obtain samples $(\mathbf{y}, \boldsymbol{\mathbf{\phi}}) \sim p(\mathbf{y}, \boldsymbol{\mathbf{\phi}})$.
If these samples differ too greatly from what the analyst would expect, then the generative model does not capture their prior scientific understanding of the data, and can be refined.
In this way, prior predictive checks [@gelman2013bayesian; Chapter 6] can be used to build generative models iteratively, with complexity added gradually.

The model is usually constructed from two parts, known respectively as the likelihood $p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\phi}})$ and the prior distribution $p(\boldsymbol{\mathbf{\phi}})$.
The joint distribution is obtained by the product
\begin{equation}
p(\mathbf{y}, \boldsymbol{\mathbf{\phi}}) = p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}}).
\end{equation}
The likelihood, as a function of $\boldsymbol{\mathbf{\phi}}$ with $\mathbf{y}$ fixed, reflects the probability of observing the data when the true value of the parameters is $\boldsymbol{\mathbf{\phi}}$.
The prior distribution encapsulates beliefs about the parameters $\boldsymbol{\mathbf{\phi}}$ before the data is observed.

Recommendations for specifying the prior distribution vary.
The extent to which subjective information should be incorporated into the prior distribution, and in doing so influence the posterior distribution, is a central topic of discussion.
Proponents of the objective Bayesian paradigm [@berger2006case] put forward that the prior distribution should be non-informative, so as not to introduce subjectivity into the analysis.
That said, we shall see in Section \@ref(hierarchical-lgm-elgm) that the distinction between likelihood and prior distribution can be unclear.
As such, it may be argued that issues of subjectivity are not unique to the prior distribution, and ultimately the challenge of specifying the data generating process is better thought of more holistically [@gelman2017prior].

### Bayesian computation

(ref:conjugate) An example of Bayesian modelling and computation for a simple one parameter model. Here the likelihood is $y_i \sim \text{Poisson}(\phi)$ for $i = 1, 2, 3$ and prior distribution on the rate parameter is $\phi \sim \text{Gamma}(3, 1)$. I simulated observed data $\mathbf{y} = (1, 2, 3)$ from the distribution $\text{Poisson}(2.5)$. As such, the true data generating process is within the space of generative models being considered (This situation is sometimes known [@bernardo2001bayesian] as the $\mathcal{M}$-closed world, in contrast to the $\mathcal{M}$-open world where the model is said to be misspecified .) Furthermore, the posterior distribution is available in closed form as $\text{Gamma}(9, 4)$. This is because the posterior distribution is in the same family of probability distributions as the prior distribution, and the model is described as being conjugate. Conjugate models are often used because of their convenience. Though other models may be more suitable, they will typically be more computationally demanding. In this situation, which is typical, the posterior distribution is more tightly peaked than the prior distribution.

```{r conjugate, fig.cap="(ref:conjugate)"}
knitr::include_graphics("figures/bayesian/conjugate.png")
```

The posterior distribution $p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y})$ encapsulates probabilistic beliefs about the parameters given the observed data.
Using the eponymous Bayes' theorem
\begin{equation}
p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) = \frac{p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}})}{p(\mathbf{y})}. (\#eq:posterior)
\end{equation}

The primary goal in a Bayesian analysis is to obtain the posterior distribution.
Unfortunately, most of the time it is intractable to calculate directly.
This is because of the potentially high-dimensional integral
\begin{equation}
p(\mathbf{y}) = \int p(\mathbf{y}, \boldsymbol{\mathbf{\phi}}) \text{d}\boldsymbol{\mathbf{\phi}}
\end{equation}
in the denominator of Equation \@ref(eq:posterior).
As such, although it is easy to evaluate $p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) \propto p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}})$, it is typically difficult to evaluate the posterior distribution itself.
The difficulty in performing Bayesian inference in general may be thought of as analogous to the difficulty in calculating integrals.
As with integration, in some cases closed form solutions are available.
Figure \@ref(fig:conjugate) illustrates one such case, where the prior distribution and posterior distribution are in the same family of probability distributions.

A great variety of computational methods have been developed to tackle this problem [@martin2023computing].
They may broadly be divided into Monte Carlo algorithms, and deterministic approximations.

#### Monte Carlo algorithms

Monte Carlo algorithms [@robert2005monte] look to generate samples from the posterior distribution 
\begin{equation}
\boldsymbol{\mathbf{\phi}}_i \sim p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}), \quad i \in 1, \ldots M.
\end{equation}
These samples may be used in any future computations involving the posterior distribution.
For example, if $G = G(\boldsymbol{\mathbf{\phi}})$ then the expectation of $G$ with respect to the posterior distribution can be approximated by
\begin{equation}
\mathbb{E}(G \, | \, \mathbf{y}) = \int G(\boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) \text{d} \boldsymbol{\mathbf{\phi}} \approx \sum_{i = 1}^M G(\boldsymbol{\mathbf{\phi}}_i).
\end{equation}
Most quantities of interest can be cast as posterior expectations.

Markov chain Monte Carlo (MCMC) methods [@roberts2004general] are the most popular class of sampling algorithms.
Using MCMC samples are generated from by simulating from an ergodic Markov chain with the posterior distribution as its stationary distribution.
The Metropolis-Hastings [@metropolis1953equation; @hastings1970monte] algorithm uses a proposal distribution $q(\boldsymbol{\mathbf{\phi}}_{i + 1} \, | \, \boldsymbol{\mathbf{\phi}}_i)$ to generate candidate parameters for the next step in the Markov chain.

In this thesis, I use the the No-U-Turn sampler [NUTS; @hoffman2014no], a Hamiltonian Monte Carlo [HMC; @duane1987hybrid; @neal2011mcmc] algorithm, implemented in the Stan [@carpenter2017stan] probabilistic programming language (PPL).
HMC uses derivatives of the posterior distribution to generate efficient Metropolis-Hastings proposal distributions.
NUTS automatically adapts the tuning parameters of HMC based local properties of the posterior distribution.
Though not a one-size-fits-all solution, NUTS has proven empirically to be a good choice for sampling from a range of posterior distributions.

(ref:stan) NUTS can be used to sample from the posterior distribution in the example of Figure \@ref(fig:conjugate). Panel A shows a histogram of the NUTS samples as compared to the true posterior. Panel B shows a traceplot. Panel C shows the convergence of the empirical posterior mean to the true value.

```{r stan, fig.cap="(ref:stan)"}
knitr::include_graphics("figures/bayesian/stan.png")
```

#### Deterministic approximations

In variational inference [VI; @blei2017variational] the posterior distribution is assumed to belong to a particular family of functions.
Optimisation algorithms are then used to choose the best member of that family, typically by minimising the Kullback-Leibler divergence to the posterior distribution.
VI is typically faster than Monte Carlo methods, especially for large datasets or models.
However, it lacks theoretical guarantees and is often known to inaccurately estimate posterior variances [@giordano2018covariances].
Developing diagnostics to evaluate the accuracy of VI is an important area of ongoing research [@yao2018yes].
The expectation maximisation [EM; @dempster1977maximum] and expectation propagation [EP; @minka2001expectation] algorithms are closely related to VI.

Need to talk about:
Laplace approximation.
Quadrature.
Integrated nested Laplace approximation.
These approximations are the subject of Chapter \@ref(naomi-aghq).
Unsure how much detail to go into here.

### Interplay between modelling and computation

Bayesian computation aspires to abstract away calculation of the posterior distribution from the analyst.
Modern computational techniques and software have made this aspiration a reality for many models.
However, computation of the posterior distribution remains intractable for a majority of models.
As such, the analyst need not only to be concerned with choosing a model suitable for the data, but also choosing a model for which the posterior distribution is tractable in reasonable time.
As such, there is an important interplay between modelling and computation, wherein models are bound by the limits of computation.
As computation improves, the space of models available to the analyst expands.

## Spatio-temporal statistics

(ref:st) The position of the capital of South Africa, Cape Town, can be considered a point location. The ZF Mgcawu District Municipality is an example of an area. World AIDS Day, designated on the 1st of December every year, can be considered a point in time. The second fiscal quarter, running through April, May and June, and denoted by Q2 represents a period of time. Both Cape Town and World AIDS Day in reality are areas of lesser extent, rather than true point locations. There are few instances of true infinitesimal point locations in everyday life.

```{r st, fig.cap="(ref:st)"}
knitr::include_graphics("figures/bayesian/st.png")
```

In spatio-temporal statistics [@cressie2015statistics], observations are indexed by spatial or temporal location (Figure \@ref(fig:st)).
In this thesis I assume that the spatial study region $\mathcal{S} \subseteq \mathbb{R}^2$ has two dimensions, corresponding to latitude and longitude.
Observations may be associated to a point $s \in \mathcal{S}$ or area $A \subseteq \mathcal{S}$ in the study region.
The temporal study period $\mathcal{T} \subseteq \mathbb{R}$ can more generally be assumed to be one dimensional.
Similarly, observations may be associated to a point $t \in \mathcal{T}$ or period of time $T \subseteq \mathcal{T}$.

### Properties of spatio-temporal data

Spatio-temporal data has some important properties:

1. **Correlation structure**:
Tobler's first law of geography is that "everything is related to everything else, but near things are more related than distant things" [@tobler1970computer].
In "The Design of Experiments" @fisher1936design similarly observed that neighbouring crops were more likely to have similar yields.
This law may be formalised using spatial covariance functions.
When these functions vary over space they are called non-stationary.

    As well as space, Tobler's first law extends to time.
    Observations made close together in time tend to be similar.
    The field of time series analysis [@hyndman2018forecasting].

    Because of its correlation structure, spatio-temporal data are not independent and identically distributed (IID).
    Only one observation of a spatio-temporal process is typically realised.

2. **Scales**:
Spatio-temporal observations might be at various possible scales.
May want to model data at a scale it was not observated at.
May want to model data from different scales simultaneously.
Downscaling.
Upscaling.
Change-of-support.
Misalignment.
3. **Size**:
Data with both spatial and temporal dimensions are often large.
Models typically require many parameters.
Storage, manipulation, and inference for spatio-temporal data can be challenging.

### Small-area estimation

Because of the large number of possible locations in space and time, and cost of collecting data, often no or limited direct observations are available for any given location.
In this situtation, direct estimates of indicators of interest are either impossible or inaccurate.
Small-area estimation [SAE; @pfeffermann2013new] methods aim to overcome the limitations of small data by sharing information across space and time (Figure \@ref(fig:zmb-maps), Figure \@ref(fig:zmb-scatter)).

(ref:zmb-maps) I simulated simple random samples with varying sample size (5, 25, 125) in each of the 156 constituencies of Zambia. I then calculated direct and modelled estimates for each survey. The model was a logistic regression with linear predictor given by an intercept and a Besag spatial random effect. HIV estimates for Zambia have previously been generated at the district-level comprising 116 spatial units. Moving forward, there is interest in generating estimates at the constituency level, as program planning is more locally devolved. This figure is adapted from a presentation I gave for the Zambia HIV Estimates Technical Working Group, available from [`athowes/zambia-unaids`](https://github.com/athowes/zambia-unaids).

```{r zmb-maps, fig.cap="(ref:zmb-maps)"}
knitr::include_graphics("figures/bayesian/zmb-maps.png")
```

(ref:zmb-scatter) The estimates from surveys with higher sample size have higher Pearson correlation coefficient $R$ with the underlying truth. For a fixed sample size, correlation can be improved by using modelled estimates to borrow information across spatial units, rather than using the higher variance direct estimates. Points along the dashed diagonal line correspond to agreement between the estimate obtained from the survey and the underlying truth used to generate the data. The setting matches that of Figure \@ref(fig:zmb-maps).

```{r zmb-scatter, fig.cap="(ref:zmb-scatter)"}
knitr::include_graphics("figures/bayesian/zmb-scatter.png")
```

## Model classes {#hierarchical-lgm-elgm}

### Hierarchical models {#hierarchical}

(ref:hierarchical-structure) A simple example of group structure within data in which each individual $i = 1, \ldots, n$ is associated to $m_i$ observations $y_{i1}, \ldots, y_{im_i}$.

```{r hierarchical-structure, fig.cap="(ref:hierarchical-structure)"}
knitr::include_graphics("figures/bayesian/hierarchical-structure.png")
```

Some observations in data are typically natural to group together.
For example, each individual $i = 1, \ldots, n$ in a study might be observed $m_i$ times (Figure \@ref(fig:hierarchical-structure)).
Observations $y_{i1}, \ldots y_{im_i}$ of the same individual are grouped together.
These observations are more likely to be similar than observations of different individuals.
We might also group observations by spatio-temporal location.

Bayesian hierarchical or multilevel models allow for natural handling of group structure in data, even for complex nested or crossed structures.
In a three-stage hierarchical model, we partition the parameters so that $\boldsymbol{\mathbf{\phi}} = (\mathbf{x}, \boldsymbol{\mathbf{\theta}})$.
I refer to $\mathbf{x} = (x_1, \ldots, x_n)$ as the latent field, and $\boldsymbol{\mathbf{\theta}} = (\theta_1, \ldots, \theta_m)$ as the hyperparameters.
The generative model for data $\mathbf{y}$ is then
\begin{align}
\mathbf{y} &\sim p(\mathbf{y} \, | \, \mathbf{x}, \boldsymbol{\mathbf{\theta}}), \\
\mathbf{x} &\sim p(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}), \\
\boldsymbol{\mathbf{\theta}} &\sim p(\boldsymbol{\mathbf{\theta}}),
\end{align}
with posterior distribution proportional to $p(\mathbf{x}, \boldsymbol{\mathbf{\theta}} \, | \, \mathbf{y}) \propto p(\mathbf{y} \, | \, \mathbf{x}, \boldsymbol{\mathbf{\theta}}) p(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}) p(\boldsymbol{\mathbf{\theta}})$.

Bayesian hierarchical models allow control over whether and how information is shared across groups.

1. **Complete pooling**:
In the complete pooling model, group structure is ignored and all $\sum_{i = 1}^n m_i$ observations are treated as IID
\begin{align}
y_{ij} \sim \mathcal{N}(\mu, \sigma), \\
(\mu, \sigma) \sim p(\mu, \sigma).
\end{align}
2. **No pooling**:
Alternatively, the groups can be modelled entirely separately with group specific mean $\mu_i$ and standard deviation $\sigma_i$ parameters
\begin{align}
y_{ij} \sim \mathcal{N}(\mu_i, \sigma_i), \\
(\mu_i, \sigma_i) \sim p(\mu_i, \sigma_i).
\end{align}
3. **Partial pooling**:
In this model, some amount of information is shared between the groups
\begin{align}
y_{ij} &\sim \mathcal{N}(\mu_i, \sigma), \\
\mu_i &= \beta + u_i, \\
\beta &\sim p(\beta), \\
\mathbf{u} &\sim p(\mathbf{u}), \\
\sigma &\sim p(\sigma),
\end{align}
where the vector $\mathbf{u} = (u_1, \ldots, u_n)$.

### Mixed effects models

Fixed effects refer to those elements of the latent field which are constant across groups.
Random effects refer to those elements of the latent field which vary across groups.
These terms have notoriously many different, and incompatible, definitions which can cause confusion [@gelman2005analysis].
I nonetheless find them useful to introduce here.

For concreteness, in the partial pooling model above, the latent field is $\mathbf{x} = (\beta, u_1, \ldots, u_n)$.
The scalar $\beta$ is a fixed effect which applies to all $n$ groups.
The vector $\mathbf{u}$ are random effects which alter the mean differently for each group.
The only hyperparameter is the standard deviation $\theta = \sigma$.

Random effects can also be structured to share information between some groups more than others.
In spatio-temporal statistics, structured spatial and temporal random effects are often used to impose smoothness.
Spatial random effects are the subject of Chapter \@ref(beyond-borders).

### Latent Gaussian models

Latent Gaussian models [LGMs; @rue2009approximate] are a class of three-stage Bayesian hierarchical models in which the middle layer is Gaussian.
To be more precise, in an LGM, the likelihood is given by
\begin{align*}
y_i &\sim p(y_i \, | \, \eta_i, \boldsymbol{\mathbf{\theta}}_1), \quad i = 1, \ldots, n, \\
\mu_i &= \mathbb{E}(y_i \, | \, \eta_i) = g(\eta_i), \\
\eta_i &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}).
\end{align*}
The likelihood is given by a product $p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\eta}}, \boldsymbol{\mathbf{\theta}}_1) = \prod_{i = 1}^n p(y_i \, | \, \eta_i, \boldsymbol{\mathbf{\theta}}_1)$, where $\boldsymbol{\mathbf{\eta}} = (\eta_1, \ldots, \eta_n)$.
Each response has conditional mean $\mu_i$ with inverse link function $g: \mathbb{R} \to \mathbb{R}$ such that $\mu_i = g(\eta_i)$.
The vector $\boldsymbol{\mathbf{\theta}}_1 \in \mathbb{R}^{s_1}$, with $s_1$ assumed small, are additional parameters of the likelihood.
The structured additive predictor $\eta_i$ may include an intercept $\beta_0$, fixed effects $\beta_j$ of the covariates $z_{ji}$, and random effects $f_k(\cdot)$ of the covariates $u_{ki}$.
The parameters $\beta_0$, $\{\beta_j\}$, $\{f_k(\cdot)\}$ are each assigned Gaussian prior distributions, and can be collected into a vector $\mathbf{x} \in \mathbb{R}^N$ such that $\mathbf{x} \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}(\boldsymbol{\mathbf{\theta}}_2)^{-1})$ where $\boldsymbol{\mathbf{\theta}}_2 \in \mathbb{R}^{s_2}$ are further hyperparameters, again with $s_2$ assumed small.
Let $\boldsymbol{\mathbf{\theta}} = (\boldsymbol{\mathbf{\theta}}_1, \boldsymbol{\mathbf{\theta}}_2) \in \mathbb{R}^m$ with $m = s_1 + s_2$ be all hyperparameters, with prior distribution $p(\boldsymbol{\mathbf{\theta}})$.

### Extended latent Gaussian models

<!-- Strictly speaking, many-to-one is not an issue for `R-INLA`, the latent field is implemented as a concatenation of many vectors already e.g. for $\eta_i = \beta_0 + \phi_i$ with $i = 1, \ldots, n$ the latent field is $(\eta_1, \ldots, \eta_n, \beta_0, \phi_1, \ldots, \phi_n)^\top$ of dimension $2n + 1$ -->
<!-- Finn Lindgren is working on a method for non-linear predictors, called the [iterative INLA method](https://github.com/inlabru-org/inlabru/blob/55896f10d563c14e34cab577b29b733aac051f86/vignettes/method.Rmd). More [slides](https://informatique-mia.inrae.fr/reseau-resste/sites/default/files/2020-09/slides-Lindgren_Avignon2018.pdf) here -->
<!-- Thesis work of [Follestad](https://cds.cern.ch/record/639625/files/sis-2003-305.pdf) that stayed as a preprint -->

Extended latent Gaussian models [ELGMs; @stringer2021fast] facilitate modelling of LGMs with greater non-linearities.
In particular, the structured additive predictor is redefined as $\boldsymbol{\mathbf{\eta}} = (\eta_1, \ldots \eta_{N_n})$, where $N_n \in \mathbb{N}$ is a function of $n$, and it is possible that $N_n \neq n$.
Each mean response $\mu_i$ now depends on some subset $\mathcal{J}_i \subseteq [N_n]$ of indices of $\boldsymbol{\mathbf{\eta}}$, with $\cup_{i = 1}^n \mathcal{J}_i = [N_n]$ and $1 \leq |\mathcal{J}_i| \leq N_n$, where $[N_n] = \{1, \ldots, N_n\}$.
The inverse link function $g(\cdot)$ is redefined for each observation to be a possibly many-to-one mapping $g_i: \mathbb{R}^{|\mathcal{J}_i|} \to \mathbb{R}$, such that $\mu_i = g_i(\boldsymbol{\mathbf{\eta}}_{\mathcal{J}_i})$.
Put together, ELGMs are of the form
\begin{align*}
y_i &\sim p(y_i \, | \, \boldsymbol{\mathbf{\eta}}_{\mathcal{J}_i}, \boldsymbol{\mathbf{\theta}}_1), \quad i = 1, \ldots, n, \\
\mu_i &= \mathbb{E}(y_i \, | \, \boldsymbol{\mathbf{\eta}}_{\mathcal{J}_i}) = g_i(\boldsymbol{\mathbf{\eta}}_{\mathcal{J}_i}), \\
\eta_j &= \beta_0 + \sum_{l = 1}^{p} \beta_j z_{ji} + \sum_{k = 1}^{r} f_k(u_{ki}), \quad j = 1, \ldots, N_n,
\end{align*}
with latent field and hyperparameter prior distributions as in the LGM case.

The ELGM class is well suited to small-area estimation of HIV indicators.
Indeed, this class of models is used throughout the thesis.
While it can be transformed to an LGM using the Poisson-multinomial transformation [@baker1994multinomial] the multinomial logistic regression model used in Chapter \@ref(multi-agyw) is naturally written as an ELGM where each observation depends on the set of structured additive predictors corresponding to the set of multinomial observations.
In Chapter \@ref(naomi-aghq), the Naomi small-area estimation model used to produce estimates of HIV indicators is shown to have the features of an ELGM.

## Survey methods {#survey}

Large national household surveys provide the highest quality population-level information about HIV indicators in SSA.
Demographic and Health Surveys (DHS) are funded by the United States Agency for International Development (USAID) and run every three to five years in most countries.
Population-based HIV Impact Assessment (PHIA) surveys are funded by PEPFAR and run every two to three years in high HIV burden countries.

### Survey notation and key terms

Consider a population of individuals $i = 1, \ldots, N$ with outcomes of interest $y_i$.
A census is a type of survey where all individuals are sampled.
Supposing responses from all individuals were recorded, then any population means can be calculated directly.
For example, if $G_i = G(y_i)$ then the population mean of $G$ is
\begin{equation}
\bar G = \frac{1}{N} \sum_{i = 1}^N G(y_i).
\end{equation}

In practice, it is usually too expensive to run a census.
Instead, only a subset of the individuals are sampled.
Furthermore, only a subset of those sampled have their outcome recorded, due to nonresponse or otherwise.
Let $S_i$ be an indicator for whether or not individual $i$ is sampled, and $R_i$ be an indicator for whether or not $y_i$ is recorded.
If $S_i = 0$ then $R_i = 0$.
If $S_i = 1$ then individual $i$ may not respond such that $R_i = 0$.
The population mean may be estimated directly based on the recorded subset of the population by
\begin{equation}
\bar G_R = \frac{\sum_{i = 1}^N R_i G(y_i)}{\sum_{i = 1}^N R_i}, \label{eq:direct}
\end{equation}
where $m_R = \sum_{i = 1}^N R_i$ is the recorded sample size.

A probability sample refers to the case when individuals are selected to be included in the survey at random.
In a non-probability sample, inclusion or exclusion from the survey is deterministic.
A simple random sample (SRS) is a probability sample where the sampling probability for each individual is equal, so that $P(S_i = 1) = 1 / N$.
The survey design is called complex when the sampling probabilities for each individual vary, such that $P(S_i = 1) = \pi_i$ with $\sum_{i = 1}^N \pi_i = 1$ and $\pi_i > 0$.

Complex survey designs can offer both greater practicality and statistical efficiency than a SRS.
However, particular care is required in analysing data collected using complex survey designs.
Under a complex design, failing to take into account the unequal sampling probabilities will result in bias.
That said, even for a SRS, nonresponse can cause analogous bias.

### Survey design

<!-- Add figure here to demonstrate sampling of DHS data -->

The DHS [@measure2012sampling] employs a two-stage sampling procedure.
In the first stage, ennumeration areas (EAs) from a recently conducted census are typically used as the primary sampling unit (PSU).
The EAs are then stratified by region, as well as urban-rural.
After appropriate sample sizes are determined, EAs sampled with probability proportional to size (PPS) measured 
In the second stage, the secondary sampling units (SSUs) are households.
All households in the selected EAs are listed, before being sampled systematically.
Finally, each selected household is visited, and all adults are interviewed.

The probability an individual is sampled is equal to the probability their household is sampled.
The first-stage sampling probability of the $j$th cluster in stratum $h$ given by
\begin{equation}
\pi_{1hj} = n_h \times \frac{N_{hj}}{\sum_j N_{hj}},
\end{equation}
where $N_{hj}$ is the number of households and $n_h$ be the number of clusters selected in stratum $h$.
The second-stage sampling probability each household within the $i$th cluster in stratum $h$ is
\begin{equation}
\pi_{1hj} = \frac{n_{hj}}{N_{hj}},
\end{equation}
where $n_{hj}$ is the numer of households selected in cluster $j$ and stratum $h$.
That is, each household in the cluster has equal selection probability.
The overall selection probability of each household in cluster $j$ of stratum $h$ is $\pi_{hi} = \pi_{1hj} \times \pi_{2hj}$.

### Survey analysis

Suppose a complex survey is run with sampling probabilities $\pi_i$.
The most popular method for taking into account that some individuals are more likely to be included in the survey than others is to overweight the responses of those unlikely to be included, and underweight the responses of those likely to be included.
This can be achieved using design weights $\delta_i = 1 / \pi_i$, which can be thought of as the number of individuals in the population represented by the $i$th sampled individual.
Let $P(R_i = 1 \, | \, S_i = 1) = \upsilon_i$ be the probability of response for sampled individual $i$.
The problem of nonresponse can be treated in the same way using nonresponse weights $\gamma_i = 1 / \upsilon_i$, which analogously can be thought of as the number of sampled individuals represented by the $i$th recorded individual.
Multiplying the design and nonresponse weights gives survey weights $\omega_i = \delta_i \times \gamma_i$.

A weighted estimate [@hajek1971discussion] of the population mean using the survey weights $\omega_i$ is given by
\begin{equation}
\bar G_\omega = \frac{\sum_{i = 1}^N \omega_j R_i G(y_i)}{\sum_{i = 1}^N \omega_i R_i}. \label{eq:hajek}
\end{equation}
Decomposing the additive error of this estimate provides useful intuition as to the potential benefits of survey weighting.
Following @meng2018statistical then under SRS
\begin{align}
\bar G_\omega - \bar G &= \frac{\mathbb{E}(\omega_i R_i G_i)}{\mathbb{E}(\omega_i R_i)} - \mathbb{E}(G_i) = \frac{\mathbb{C}(\omega_i R_i G_i)}{\mathbb{E}(\omega_i R_i)} \\ 
&= \rho_{R_\omega, G} \times \sqrt{\frac{N - m_{R_\omega}}{m_{R_\omega}}} \times \sigma_G,
\end{align}
where $R_\omega = \omega R$.
The data defect correlation (DDC) $\rho_{R_\omega, G}$ measures the correlation between the weighted recording mechanism and given function of the outcome of interest.
To minimise the DDC then $G \perp \!\!\! \perp R_\omega$.
The data scarcity $\sigma_{R_\omega} = \sqrt{(N - m_{R_\omega})/m_{R_\omega}}$ measures the effective proportion of the population who have been recorded.
The problem difficultly $\sigma_G$ measures the intrinsic difficulty of the estimation problem, and is independent of the sampling or analysis method.

For simplicity, let $G(y_i) = y_i$ and each $y_i \in \{0, 1\}$.
We weight then model following @chen2014use.
While this approach acknowledges the survey design, it has some important limitations.
We ignore clustering structure.
All of this isn't great and that someone should figure this out [@gelman2007struggles].
