<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Bayesian spatio-temporal statistics | Bayesian spatio-temporal methods for small-area estimation of HIV indicators</title>
  <meta name="description" content="3 Bayesian spatio-temporal statistics | Bayesian spatio-temporal methods for small-area estimation of HIV indicators" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Bayesian spatio-temporal statistics | Bayesian spatio-temporal methods for small-area estimation of HIV indicators" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Bayesian spatio-temporal statistics | Bayesian spatio-temporal methods for small-area estimation of HIV indicators" />
  
  
  

<meta name="author" content="Adam Howes" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="hiv-aids.html"/>
<link rel="next" href="beyond-borders.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="templates/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#chapter-overview"><i class="fa fa-check"></i><b>1.1</b> Chapter overview</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hiv-aids.html"><a href="hiv-aids.html"><i class="fa fa-check"></i><b>2</b> The HIV/AIDS epidemic</a>
<ul>
<li class="chapter" data-level="2.1" data-path="hiv-aids.html"><a href="hiv-aids.html#background"><i class="fa fa-check"></i><b>2.1</b> Background</a></li>
<li class="chapter" data-level="2.2" data-path="hiv-aids.html"><a href="hiv-aids.html#surveillance"><i class="fa fa-check"></i><b>2.2</b> HIV surveillance</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="hiv-aids.html"><a href="hiv-aids.html#hiv-data"><i class="fa fa-check"></i><b>2.2.1</b> Data</a></li>
<li class="chapter" data-level="2.2.2" data-path="hiv-aids.html"><a href="hiv-aids.html#challenges"><i class="fa fa-check"></i><b>2.2.2</b> Challenges</a></li>
<li class="chapter" data-level="2.2.3" data-path="hiv-aids.html"><a href="hiv-aids.html#statistical-approaches"><i class="fa fa-check"></i><b>2.2.3</b> Statistical approaches</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayes-st.html"><a href="bayes-st.html"><i class="fa fa-check"></i><b>3</b> Bayesian spatio-temporal statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayes-st.html"><a href="bayes-st.html#bayesian-statistics"><i class="fa fa-check"></i><b>3.1</b> Bayesian statistics</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="bayes-st.html"><a href="bayes-st.html#bayesian-modelling"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian modelling</a></li>
<li class="chapter" data-level="3.1.2" data-path="bayes-st.html"><a href="bayes-st.html#bayesian-computation"><i class="fa fa-check"></i><b>3.1.2</b> Bayesian computation</a></li>
<li class="chapter" data-level="3.1.3" data-path="bayes-st.html"><a href="bayes-st.html#interplay-between-modelling-and-computation"><i class="fa fa-check"></i><b>3.1.3</b> Interplay between modelling and computation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="bayes-st.html"><a href="bayes-st.html#st-statistics"><i class="fa fa-check"></i><b>3.2</b> Spatio-temporal statistics</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="bayes-st.html"><a href="bayes-st.html#properties"><i class="fa fa-check"></i><b>3.2.1</b> Properties of spatio-temporal data</a></li>
<li class="chapter" data-level="3.2.2" data-path="bayes-st.html"><a href="bayes-st.html#small-area-estimation"><i class="fa fa-check"></i><b>3.2.2</b> Small-area estimation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="bayes-st.html"><a href="bayes-st.html#model-structure"><i class="fa fa-check"></i><b>3.3</b> Model structure</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="bayes-st.html"><a href="bayes-st.html#linear-model"><i class="fa fa-check"></i><b>3.3.1</b> Linear model</a></li>
<li class="chapter" data-level="3.3.2" data-path="bayes-st.html"><a href="bayes-st.html#generalised-linear-model"><i class="fa fa-check"></i><b>3.3.2</b> Generalised linear model</a></li>
<li class="chapter" data-level="3.3.3" data-path="bayes-st.html"><a href="bayes-st.html#generalised-linear-mixed-effects-model"><i class="fa fa-check"></i><b>3.3.3</b> Generalised linear mixed effects model</a></li>
<li class="chapter" data-level="3.3.4" data-path="bayes-st.html"><a href="bayes-st.html#lgm"><i class="fa fa-check"></i><b>3.3.4</b> Latent Gaussian model</a></li>
<li class="chapter" data-level="3.3.5" data-path="bayes-st.html"><a href="bayes-st.html#elgm"><i class="fa fa-check"></i><b>3.3.5</b> Extended latent Gaussian model</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="bayes-st.html"><a href="bayes-st.html#model-comparison"><i class="fa fa-check"></i><b>3.4</b> Model comparison</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="bayes-st.html"><a href="bayes-st.html#bayes-factors"><i class="fa fa-check"></i><b>3.4.1</b> Bayes factors</a></li>
<li class="chapter" data-level="3.4.2" data-path="bayes-st.html"><a href="bayes-st.html#information-criteria"><i class="fa fa-check"></i><b>3.4.2</b> Information criteria</a></li>
<li class="chapter" data-level="3.4.3" data-path="bayes-st.html"><a href="bayes-st.html#cross-validation"><i class="fa fa-check"></i><b>3.4.3</b> Cross-validation</a></li>
<li class="chapter" data-level="3.4.4" data-path="bayes-st.html"><a href="bayes-st.html#scoring-rules"><i class="fa fa-check"></i><b>3.4.4</b> Scoring rules</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="bayes-st.html"><a href="bayes-st.html#survey"><i class="fa fa-check"></i><b>3.5</b> Survey methods</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="bayes-st.html"><a href="bayes-st.html#survey-notation-and-key-terms"><i class="fa fa-check"></i><b>3.5.1</b> Survey notation and key terms</a></li>
<li class="chapter" data-level="3.5.2" data-path="bayes-st.html"><a href="bayes-st.html#survey-design"><i class="fa fa-check"></i><b>3.5.2</b> Survey design</a></li>
<li class="chapter" data-level="3.5.3" data-path="bayes-st.html"><a href="bayes-st.html#survey-analysis"><i class="fa fa-check"></i><b>3.5.3</b> Survey analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="beyond-borders.html"><a href="beyond-borders.html"><i class="fa fa-check"></i><b>4</b> Models for areal spatial structure</a>
<ul>
<li class="chapter" data-level="4.1" data-path="beyond-borders.html"><a href="beyond-borders.html#adjacency-models"><i class="fa fa-check"></i><b>4.1</b> Models based on adjacency</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="beyond-borders.html"><a href="beyond-borders.html#besag"><i class="fa fa-check"></i><b>4.1.1</b> The Besag model</a></li>
<li class="chapter" data-level="4.1.2" data-path="beyond-borders.html"><a href="beyond-borders.html#best-practises-for-the-besag-model"><i class="fa fa-check"></i><b>4.1.2</b> Best practises for the Besag model</a></li>
<li class="chapter" data-level="4.1.3" data-path="beyond-borders.html"><a href="beyond-borders.html#concerns"><i class="fa fa-check"></i><b>4.1.3</b> Concerns about the Besag model</a></li>
<li class="chapter" data-level="4.1.4" data-path="beyond-borders.html"><a href="beyond-borders.html#weighted-icar-models"><i class="fa fa-check"></i><b>4.1.4</b> Weighted ICAR models</a></li>
<li class="chapter" data-level="4.1.5" data-path="beyond-borders.html"><a href="beyond-borders.html#bym2"><i class="fa fa-check"></i><b>4.1.5</b> The reparameterised Besag-York-Mollié model</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="beyond-borders.html"><a href="beyond-borders.html#kernel-models"><i class="fa fa-check"></i><b>4.2</b> Models using kernels</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="beyond-borders.html"><a href="beyond-borders.html#centroid-kernel"><i class="fa fa-check"></i><b>4.2.1</b> Centroid kernel</a></li>
<li class="chapter" data-level="4.2.2" data-path="beyond-borders.html"><a href="beyond-borders.html#integrated-kernel"><i class="fa fa-check"></i><b>4.2.2</b> Integrated kernel</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="beyond-borders.html"><a href="beyond-borders.html#simulation-study"><i class="fa fa-check"></i><b>4.3</b> Simulation study</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="beyond-borders.html"><a href="beyond-borders.html#synthetic-data"><i class="fa fa-check"></i><b>4.3.1</b> Synthetic data</a></li>
<li class="chapter" data-level="4.3.2" data-path="beyond-borders.html"><a href="beyond-borders.html#inferential-models"><i class="fa fa-check"></i><b>4.3.2</b> Inferential models</a></li>
<li class="chapter" data-level="4.3.3" data-path="beyond-borders.html"><a href="beyond-borders.html#model-assessment"><i class="fa fa-check"></i><b>4.3.3</b> Model assessment</a></li>
<li class="chapter" data-level="4.3.4" data-path="beyond-borders.html"><a href="beyond-borders.html#results"><i class="fa fa-check"></i><b>4.3.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="beyond-borders.html"><a href="beyond-borders.html#hiv-study"><i class="fa fa-check"></i><b>4.4</b> HIV prevalence study</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="beyond-borders.html"><a href="beyond-borders.html#household-survey-data"><i class="fa fa-check"></i><b>4.4.1</b> Household survey data</a></li>
<li class="chapter" data-level="4.4.2" data-path="beyond-borders.html"><a href="beyond-borders.html#inferential-models-1"><i class="fa fa-check"></i><b>4.4.2</b> Inferential models</a></li>
<li class="chapter" data-level="4.4.3" data-path="beyond-borders.html"><a href="beyond-borders.html#model-comparison-1"><i class="fa fa-check"></i><b>4.4.3</b> Model comparison</a></li>
<li class="chapter" data-level="4.4.4" data-path="beyond-borders.html"><a href="beyond-borders.html#results-1"><i class="fa fa-check"></i><b>4.4.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="beyond-borders.html"><a href="beyond-borders.html#discussion"><i class="fa fa-check"></i><b>4.5</b> Discussion</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="beyond-borders.html"><a href="beyond-borders.html#modelling"><i class="fa fa-check"></i><b>4.5.1</b> Modelling</a></li>
<li class="chapter" data-level="4.5.2" data-path="beyond-borders.html"><a href="beyond-borders.html#model-selection"><i class="fa fa-check"></i><b>4.5.2</b> Model selection</a></li>
<li class="chapter" data-level="4.5.3" data-path="beyond-borders.html"><a href="beyond-borders.html#inference-1"><i class="fa fa-check"></i><b>4.5.3</b> Inference</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multi-agyw.html"><a href="multi-agyw.html"><i class="fa fa-check"></i><b>5</b> A model for risk group proportions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="multi-agyw.html"><a href="multi-agyw.html#background-1"><i class="fa fa-check"></i><b>5.1</b> Background</a></li>
<li class="chapter" data-level="5.2" data-path="multi-agyw.html"><a href="multi-agyw.html#data"><i class="fa fa-check"></i><b>5.2</b> Data</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="multi-agyw.html"><a href="multi-agyw.html#behavioural-data-from-household-surveys"><i class="fa fa-check"></i><b>5.2.1</b> Behavioural data from household surveys</a></li>
<li class="chapter" data-level="5.2.2" data-path="multi-agyw.html"><a href="multi-agyw.html#other-data"><i class="fa fa-check"></i><b>5.2.2</b> Other data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="multi-agyw.html"><a href="multi-agyw.html#model-for-risk-group-proportions"><i class="fa fa-check"></i><b>5.3</b> Model for risk group proportions</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="multi-agyw.html"><a href="multi-agyw.html#st-multinomial"><i class="fa fa-check"></i><b>5.3.1</b> Spatio-temporal multinomial logistic regression</a></li>
<li class="chapter" data-level="5.3.2" data-path="multi-agyw.html"><a href="multi-agyw.html#s-logistic"><i class="fa fa-check"></i><b>5.3.2</b> Spatial logistic regression</a></li>
<li class="chapter" data-level="5.3.3" data-path="multi-agyw.html"><a href="multi-agyw.html#female-sex-worker-population-size-adjustment"><i class="fa fa-check"></i><b>5.3.3</b> Female sex worker population size adjustment</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="multi-agyw.html"><a href="multi-agyw.html#prevalence-and-incidence-by-risk-group"><i class="fa fa-check"></i><b>5.4</b> Prevalence and incidence by risk group</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="multi-agyw.html"><a href="multi-agyw.html#disaggregation-of-naomi-prevalence-estimates"><i class="fa fa-check"></i><b>5.4.1</b> Disaggregation of Naomi prevalence estimates</a></li>
<li class="chapter" data-level="5.4.2" data-path="multi-agyw.html"><a href="multi-agyw.html#disaggregation-of-naomi-incidence-estimates"><i class="fa fa-check"></i><b>5.4.2</b> Disaggregation of Naomi incidence estimates</a></li>
<li class="chapter" data-level="5.4.3" data-path="multi-agyw.html"><a href="multi-agyw.html#expected-new-infections-reached"><i class="fa fa-check"></i><b>5.4.3</b> Expected new infections reached</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="multi-agyw.html"><a href="multi-agyw.html#results-2"><i class="fa fa-check"></i><b>5.5</b> Results</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="multi-agyw.html"><a href="multi-agyw.html#model-for-risk-group-proportions-1"><i class="fa fa-check"></i><b>5.5.1</b> Model for risk group proportions</a></li>
<li class="chapter" data-level="5.5.2" data-path="multi-agyw.html"><a href="multi-agyw.html#prevalence-and-incidence-by-risk-group-1"><i class="fa fa-check"></i><b>5.5.2</b> Prevalence and incidence by risk group</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="multi-agyw.html"><a href="multi-agyw.html#discussion-1"><i class="fa fa-check"></i><b>5.6</b> Discussion</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="multi-agyw.html"><a href="multi-agyw.html#limitations"><i class="fa fa-check"></i><b>5.6.1</b> Limitations</a></li>
<li class="chapter" data-level="5.6.2" data-path="multi-agyw.html"><a href="multi-agyw.html#conclusion"><i class="fa fa-check"></i><b>5.6.2</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="naomi-aghq.html"><a href="naomi-aghq.html"><i class="fa fa-check"></i><b>6</b> Fast approximate Bayesian inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#naomi-inference"><i class="fa fa-check"></i><b>6.1</b> Inference methods and software</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#la"><i class="fa fa-check"></i><b>6.1.1</b> The Laplace approximation</a></li>
<li class="chapter" data-level="6.1.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#quadrature"><i class="fa fa-check"></i><b>6.1.2</b> Quadrature</a></li>
<li class="chapter" data-level="6.1.3" data-path="naomi-aghq.html"><a href="naomi-aghq.html#inla"><i class="fa fa-check"></i><b>6.1.3</b> Integrated nested Laplace approximation</a></li>
<li class="chapter" data-level="6.1.4" data-path="naomi-aghq.html"><a href="naomi-aghq.html#software"><i class="fa fa-check"></i><b>6.1.4</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#universal"><i class="fa fa-check"></i><b>6.2</b> A universal INLA implementation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#epil"><i class="fa fa-check"></i><b>6.2.1</b> Epilepsy GLMM</a></li>
<li class="chapter" data-level="6.2.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#loaloa"><i class="fa fa-check"></i><b>6.2.2</b> Loa loa ELGM</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="naomi-aghq.html"><a href="naomi-aghq.html#naomi"><i class="fa fa-check"></i><b>6.3</b> The Naomi model</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#naomi-model"><i class="fa fa-check"></i><b>6.3.1</b> Model structure</a></li>
<li class="chapter" data-level="6.3.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#naomi-elgm"><i class="fa fa-check"></i><b>6.3.2</b> Naomi as an ELGM</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="naomi-aghq.html"><a href="naomi-aghq.html#pca-aghq"><i class="fa fa-check"></i><b>6.4</b> AGHQ in moderate dimensions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#aghq-with-variable-levels"><i class="fa fa-check"></i><b>6.4.1</b> AGHQ with variable levels</a></li>
<li class="chapter" data-level="6.4.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#principal-components-analysis"><i class="fa fa-check"></i><b>6.4.2</b> Principal components analysis</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="naomi-aghq.html"><a href="naomi-aghq.html#malawi"><i class="fa fa-check"></i><b>6.5</b> Malawi case-study</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#naomi-nuts"><i class="fa fa-check"></i><b>6.5.1</b> NUTS convergence and suitability</a></li>
<li class="chapter" data-level="6.5.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#use-of-pca-aghq"><i class="fa fa-check"></i><b>6.5.2</b> Use of PCA-AGHQ</a></li>
<li class="chapter" data-level="6.5.3" data-path="naomi-aghq.html"><a href="naomi-aghq.html#inference-comparison"><i class="fa fa-check"></i><b>6.5.3</b> Inference comparison</a></li>
<li class="chapter" data-level="6.5.4" data-path="naomi-aghq.html"><a href="naomi-aghq.html#exceedance-probabilities"><i class="fa fa-check"></i><b>6.5.4</b> Exceedance probabilities</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="naomi-aghq.html"><a href="naomi-aghq.html#discussion-2"><i class="fa fa-check"></i><b>6.6</b> Discussion</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#a-universal-inla-implementation"><i class="fa fa-check"></i><b>6.6.1</b> A universal INLA implementation</a></li>
<li class="chapter" data-level="6.6.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#pca-aghq-with-application-to-inla-for-naomi"><i class="fa fa-check"></i><b>6.6.2</b> PCA-AGHQ with application to INLA for Naomi</a></li>
<li class="chapter" data-level="6.6.3" data-path="naomi-aghq.html"><a href="naomi-aghq.html#suggestions-for-future-work"><i class="fa fa-check"></i><b>6.6.3</b> Suggestions for future work</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>7</b> Conclusions</a>
<ul>
<li class="chapter" data-level="7.1" data-path="conclusions.html"><a href="conclusions.html#strengths"><i class="fa fa-check"></i><b>7.1</b> Strengths</a></li>
<li class="chapter" data-level="7.2" data-path="conclusions.html"><a href="conclusions.html#weaknesses"><i class="fa fa-check"></i><b>7.2</b> Weaknesses</a></li>
<li class="chapter" data-level="7.3" data-path="conclusions.html"><a href="conclusions.html#future-work"><i class="fa fa-check"></i><b>7.3</b> Future work</a></li>
<li class="chapter" data-level="7.4" data-path="conclusions.html"><a href="conclusions.html#conclusions-1"><i class="fa fa-check"></i><b>7.4</b> Conclusions</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="models-for-areal-spatial-structure.html"><a href="models-for-areal-spatial-structure.html"><i class="fa fa-check"></i><b>A</b> Models for areal spatial structure</a>
<ul>
<li class="chapter" data-level="A.1" data-path="models-for-areal-spatial-structure.html"><a href="models-for-areal-spatial-structure.html#aghq-nuts"><i class="fa fa-check"></i><b>A.1</b> Comparison of AGHQ to NUTS</a></li>
<li class="chapter" data-level="A.2" data-path="models-for-areal-spatial-structure.html"><a href="models-for-areal-spatial-structure.html#lengthscale-prior"><i class="fa fa-check"></i><b>A.2</b> Lengthscale prior sensitivity analysis</a></li>
<li class="chapter" data-level="A.3" data-path="models-for-areal-spatial-structure.html"><a href="models-for-areal-spatial-structure.html#simulation-appendix"><i class="fa fa-check"></i><b>A.3</b> Simulation study</a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="models-for-areal-spatial-structure.html"><a href="models-for-areal-spatial-structure.html#lengthscale"><i class="fa fa-check"></i><b>A.3.1</b> Lengthscale hyperparameter</a></li>
<li class="chapter" data-level="A.3.2" data-path="models-for-areal-spatial-structure.html"><a href="models-for-areal-spatial-structure.html#bym2-proportion"><i class="fa fa-check"></i><b>A.3.2</b> BYM2 proportion hyperparameter</a></li>
<li class="chapter" data-level="A.3.3" data-path="models-for-areal-spatial-structure.html"><a href="models-for-areal-spatial-structure.html#mean-squared-error"><i class="fa fa-check"></i><b>A.3.3</b> Mean squared error</a></li>
<li class="chapter" data-level="A.3.4" data-path="models-for-areal-spatial-structure.html"><a href="models-for-areal-spatial-structure.html#continuous-ranked-probability-score"><i class="fa fa-check"></i><b>A.3.4</b> Continuous ranked probability score</a></li>
<li class="chapter" data-level="A.3.5" data-path="models-for-areal-spatial-structure.html"><a href="models-for-areal-spatial-structure.html#calibration"><i class="fa fa-check"></i><b>A.3.5</b> Calibration</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="models-for-areal-spatial-structure.html"><a href="models-for-areal-spatial-structure.html#hiv-appendix"><i class="fa fa-check"></i><b>A.4</b> HIV study</a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="models-for-areal-spatial-structure.html"><a href="models-for-areal-spatial-structure.html#estimates-1"><i class="fa fa-check"></i><b>A.4.1</b> Estimates</a></li>
<li class="chapter" data-level="A.4.2" data-path="models-for-areal-spatial-structure.html"><a href="models-for-areal-spatial-structure.html#cross-validation-1"><i class="fa fa-check"></i><b>A.4.2</b> Cross-validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="a-model-for-risk-group-proportions.html"><a href="a-model-for-risk-group-proportions.html"><i class="fa fa-check"></i><b>B</b> A model for risk group proportions</a>
<ul>
<li class="chapter" data-level="B.1" data-path="a-model-for-risk-group-proportions.html"><a href="a-model-for-risk-group-proportions.html#the-global-aids-strategy"><i class="fa fa-check"></i><b>B.1</b> The Global AIDS Strategy</a></li>
<li class="chapter" data-level="B.2" data-path="a-model-for-risk-group-proportions.html"><a href="a-model-for-risk-group-proportions.html#household-survey-data-1"><i class="fa fa-check"></i><b>B.2</b> Household survey data</a></li>
<li class="chapter" data-level="B.3" data-path="a-model-for-risk-group-proportions.html"><a href="a-model-for-risk-group-proportions.html#spatial-analysis-levels"><i class="fa fa-check"></i><b>B.3</b> Spatial analysis levels</a></li>
<li class="chapter" data-level="B.4" data-path="a-model-for-risk-group-proportions.html"><a href="a-model-for-risk-group-proportions.html#survey-questions"><i class="fa fa-check"></i><b>B.4</b> Survey questions and risk group allocation</a></li>
<li class="chapter" data-level="B.5" data-path="a-model-for-risk-group-proportions.html"><a href="a-model-for-risk-group-proportions.html#additional-figures"><i class="fa fa-check"></i><b>B.5</b> Additional figures</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html"><i class="fa fa-check"></i><b>C</b> Fast approximate Bayesian inference</a>
<ul>
<li class="chapter" data-level="C.1" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#epilepsy-example"><i class="fa fa-check"></i><b>C.1</b> Epilepsy example</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#tmb-epil"><i class="fa fa-check"></i><b>C.1.1</b> <code>TMB</code> C++ template</a></li>
<li class="chapter" data-level="C.1.2" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#tmb-modified-epil"><i class="fa fa-check"></i><b>C.1.2</b> Modified <code>TMB</code> C++ template</a></li>
<li class="chapter" data-level="C.1.3" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#stan-epil"><i class="fa fa-check"></i><b>C.1.3</b> <code>Stan</code> C++ template</a></li>
<li class="chapter" data-level="C.1.4" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#nuts-convergence-and-suitability"><i class="fa fa-check"></i><b>C.1.4</b> NUTS convergence and suitability</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#loa-loa-example"><i class="fa fa-check"></i><b>C.2</b> Loa loa example</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#nuts-convergence-and-suitability-1"><i class="fa fa-check"></i><b>C.2.1</b> NUTS convergence and suitability</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#naomi-math"><i class="fa fa-check"></i><b>C.3</b> Simplified Naomi model description</a>
<ul>
<li class="chapter" data-level="C.3.1" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#naomi-process"><i class="fa fa-check"></i><b>C.3.1</b> Process specification</a></li>
<li class="chapter" data-level="C.3.2" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#naomi-likelihood"><i class="fa fa-check"></i><b>C.3.2</b> Additional likelihood specification</a></li>
<li class="chapter" data-level="C.3.3" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#naomi-identifiability"><i class="fa fa-check"></i><b>C.3.3</b> Identifiability constraints</a></li>
<li class="chapter" data-level="C.3.4" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#naomi-implementation"><i class="fa fa-check"></i><b>C.3.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#nuts-convergence-and-suitability-2"><i class="fa fa-check"></i><b>C.4</b> NUTS convergence and suitability</a></li>
<li class="chapter" data-level="C.5" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#use-of-pca-aghq-1"><i class="fa fa-check"></i><b>C.5</b> Use of PCA-AGHQ</a></li>
<li class="chapter" data-level="C.6" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#normalising-constant-estimation"><i class="fa fa-check"></i><b>C.6</b> Normalising constant estimation</a></li>
<li class="chapter" data-level="C.7" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#inference-comparison-1"><i class="fa fa-check"></i><b>C.7</b> Inference comparison</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><p>Bayesian spatio-temporal methods for small-area estimation of HIV indicators</p></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayes-st" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Bayesian spatio-temporal statistics<a href="bayes-st.html#bayes-st" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!-- For PDF output, include these two LaTeX commands after unnumbered chapter headings, otherwise the mini table of contents and the running header will show the previous chapter -->
<div id="bayesian-statistics" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Bayesian statistics<a href="bayes-st.html#bayesian-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayesian statistics is a mathematical paradigm for learning from data.
It is especially well suited to facing the challenges presented in Section <a href="hiv-aids.html#surveillance">2.2</a> for the following reasons.
First, because it allows for principled and flexible integration of prior domain knowledge.
Second, because uncertainty over all unknown quantities is handled as an integral part of the Bayesian paradigm.
This section provides a brief and at times opinionated overview of Bayesian statistics.
For a more complete introduction, I recommend <span class="citation">Gelman et al. (<a href="#ref-gelman2013bayesian">2013</a>)</span>, <span class="citation">McElreath (<a href="#ref-mcelreath2020statistical">2020</a>)</span> or <span class="citation">Gelman et al. (<a href="#ref-gelman2020bayesian">2020</a>)</span>.</p>
<div id="bayesian-modelling" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Bayesian modelling<a href="bayes-st.html#bayesian-modelling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Bayesian approach to data analysis is based on construction of a probability model for the observed data <span class="math inline">\(\mathbf{y} = (y_1, \ldots, y_n)\)</span>.
Parameters <span class="math inline">\(\boldsymbol{\mathbf{\phi}} = (\phi_1, \ldots, \phi_d)\)</span> are used to describe features of the data.
Both the data and parameters are assumed to be random variables, with joint probability distribution written as <span class="math inline">\(p(\mathbf{y}, \boldsymbol{\mathbf{\phi}})\)</span>.
Subsequent calculations, and the conclusions to follow, are made by manipulating the model using probability theory.</p>
<p>Models are most naturally constructed from two parts known as the likelihood <span class="math inline">\(p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\phi}})\)</span> and the prior distribution <span class="math inline">\(p(\boldsymbol{\mathbf{\phi}})\)</span>.
The joint distribution is obtained by the product of these two parts
<span class="math display" id="eq:joint">\[\begin{equation}
p(\mathbf{y}, \boldsymbol{\mathbf{\phi}}) = p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}}). \tag{3.1}
\end{equation}\]</span>
The likelihood, as a function of <span class="math inline">\(\boldsymbol{\mathbf{\phi}}\)</span> with <span class="math inline">\(\mathbf{y}\)</span> fixed, reflects the probability of observing the data when the value of the parameters is <span class="math inline">\(\boldsymbol{\mathbf{\phi}}\)</span>.
The prior distribution encapsulates beliefs about the parameters <span class="math inline">\(\boldsymbol{\mathbf{\phi}}\)</span> before the data are observed.</p>
<p>Recommendations for specifying prior distributions vary.
A central issue is the extent to which subjective information should be incorporated into the prior distribution.
Proponents of the objective Bayesian paradigm <span class="citation">(<a href="#ref-berger2006case">Berger 2006</a>)</span> put forward that the prior distribution should be non-informative, so as not to introduce subjectivity into the analysis.
Others see subjectivity as fundamental to scientific inquiry, with no viable alternative <span class="citation">(<a href="#ref-goldstein2006subjective">Goldstein 2006</a>)</span>.
Though subjectivity typically discussed with regard to the prior distribution, we shall see in Section <a href="bayes-st.html#model-structure">3.3</a> that the distinction between prior distribution and likelihood is not always clear.
As such, it may be argued that issues of subjectivity are not unique to prior distribution specification, and ultimately that the challenge of specifying the data generating process is better thought of more holistically <span class="citation">(<a href="#ref-gelman2017prior">Gelman, Simpson, and Betancourt 2017</a>)</span>.</p>
<p>The probability model can be simulated from to obtain samples <span class="math inline">\((\mathbf{y}, \boldsymbol{\mathbf{\phi}}) \sim p(\mathbf{y}, \boldsymbol{\mathbf{\phi}})\)</span>.
If the samples of the data <span class="math inline">\(\mathbf{y}\)</span> differ too greatly from what the analyst would expect to see in reality, then the model does not capture their prior scientific understanding.
Models which do not produce plausible data samples can be refined.
Checks of this kind [<span class="citation">Gelman et al. (<a href="#ref-gelman2013bayesian">2013</a>)</span>; Chapter 6] can be used to help iteratively build models, gradually adding complexity as required.</p>
</div>
<div id="bayesian-computation" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Bayesian computation<a href="bayes-st.html#bayesian-computation" class="anchor-section" aria-label="Anchor link to header"></a></h3>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:conjugate"></span>
<img src="figures/bayesian/conjugate.png" alt="An example of Bayesian modelling and computation for a simple one parameter model. Here the likelihood is \(y_i \sim \text{Poisson}(\phi)\) for \(i = 1, 2, 3\) and prior distribution on the rate parameter \(\phi &gt; 0\) is \(\phi \sim \text{Gamma}(3, 1)\). Observed data \(\mathbf{y} = (1, 2, 3)\) was simulated from the distribution \(\text{Poisson}(2.5)\). The true data generating process is within the space of models being considered. (This situation is sometimes known (Bernardo and Smith 2001) as the \(\mathcal{M}\)-closed world, in contrast to the \(\mathcal{M}\)-open world where the model is said to be misspecified.) Furthermore, the posterior distribution is available in closed form as \(\text{Gamma}(9, 4)\). This is because the posterior distribution is in the same family of probability distributions as the prior distribution. Models of this kind are described as being conjugate. Conjugate models are often used because of their convenience. Though other models may be more suitable, they will typically be more computationally demanding. The posterior distribution here is more tightly peaked than the prior distribution. This contraction is typically, but not always, the case." width="95%" />
<p class="caption">
Figure 3.1: An example of Bayesian modelling and computation for a simple one parameter model. Here the likelihood is <span class="math inline">\(y_i \sim \text{Poisson}(\phi)\)</span> for <span class="math inline">\(i = 1, 2, 3\)</span> and prior distribution on the rate parameter <span class="math inline">\(\phi &gt; 0\)</span> is <span class="math inline">\(\phi \sim \text{Gamma}(3, 1)\)</span>. Observed data <span class="math inline">\(\mathbf{y} = (1, 2, 3)\)</span> was simulated from the distribution <span class="math inline">\(\text{Poisson}(2.5)\)</span>. The true data generating process is within the space of models being considered. (This situation is sometimes known <span class="citation">(<a href="#ref-bernardo2001bayesian">Bernardo and Smith 2001</a>)</span> as the <span class="math inline">\(\mathcal{M}\)</span>-closed world, in contrast to the <span class="math inline">\(\mathcal{M}\)</span>-open world where the model is said to be misspecified.) Furthermore, the posterior distribution is available in closed form as <span class="math inline">\(\text{Gamma}(9, 4)\)</span>. This is because the posterior distribution is in the same family of probability distributions as the prior distribution. Models of this kind are described as being conjugate. Conjugate models are often used because of their convenience. Though other models may be more suitable, they will typically be more computationally demanding. The posterior distribution here is more tightly peaked than the prior distribution. This contraction is typically, but not always, the case.
</p>
</div>
<p>Having constructed a model (Equation <a href="bayes-st.html#eq:joint">(3.1)</a>), the primary goal in a Bayesian analysis is to obtain the posterior distribution <span class="math inline">\(p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y})\)</span>.
This distribution encapsulates probabilistic beliefs about the parameters given the observed data.
As such, the posterior distribution has a central role in use of the statistical analysis for decision making.</p>
<p>Using the eponymous Bayes’ theorem, the posterior distribution is obtained by
<span class="math display" id="eq:posterior">\[\begin{equation}
p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) = \frac{p(\mathbf{y}, \boldsymbol{\mathbf{\phi}})}{p(\mathbf{y})} = \frac{p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}})}{p(\mathbf{y})}. \tag{3.2}
\end{equation}\]</span>
Unfortunately, most of the time it is intractable to calculate the posterior distribution analytically.
This is because of the potentially high-dimensional integral
<span class="math display" id="eq:evidence">\[\begin{equation}
p(\mathbf{y}) = \int p(\mathbf{y}, \boldsymbol{\mathbf{\phi}}) \text{d}\boldsymbol{\mathbf{\phi}} \tag{3.3}
\end{equation}\]</span>
in the denominator of Equation <a href="bayes-st.html#eq:posterior">(3.2)</a>.
The result of this integral is known as the evidence <span class="math inline">\(p(\mathbf{y})\)</span>, and quantifies the probability of obtaining the data under the model.
Hence, although it is easy to evaluate a quantity proportional to the posterior distribution
<span class="math display">\[\begin{equation}
p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) \propto p(\mathbf{y} \, | \, \boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}}),
\end{equation}\]</span>
it is typically difficult to evaluate the posterior distribution itself.</p>
<p>The difficulty in performing Bayesian inference may be thought of as analogous to the difficulty in calculating integrals.
As with integration, in specific cases closed form analytic solutions are available.
Figure <a href="bayes-st.html#fig:conjugate">3.1</a> illustrates one such case, where the prior distribution and posterior distribution are in the same family of probability distributions.
In the more general case no analytic solution is available, and computational methods must be relied on.
Computational strategies for approximating the posterior distribution <span class="citation">(<a href="#ref-martin2023computing">Martin, Frazier, and Robert 2023</a>)</span> may broadly be divided into Monte Carlo algorithms and deterministic approximations.</p>
<div id="monte-carlo" class="section level4 hasAnchor" number="3.1.2.1">
<h4><span class="header-section-number">3.1.2.1</span> Monte Carlo algorithms<a href="bayes-st.html#monte-carlo" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Monte Carlo algorithms <span class="citation">(<a href="#ref-robert2005monte">Robert and Casella 2005</a>)</span> aim to generate samples from the posterior distribution
<span class="math display">\[\begin{equation}
\boldsymbol{\mathbf{\phi}}_s \sim p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}), \quad s \in 1, \ldots S.
\end{equation}\]</span>
These samples may be used in any future computations involving functions of the posterior distribution.
For example, if <span class="math inline">\(G = G(\boldsymbol{\mathbf{\phi}})\)</span> then the expectation of <span class="math inline">\(G\)</span> with respect to the posterior distribution can be approximated by
<span class="math display">\[\begin{equation}
\mathbb{E}(G \, | \, \mathbf{y}) = \int G(\boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) \text{d} \boldsymbol{\mathbf{\phi}} \approx \frac{1}{S} \sum_{s = 1}^S G(\boldsymbol{\mathbf{\phi}}_s).
\end{equation}\]</span>
Most quantities of interest can be cast as posterior expectations which can then be approximated empirically using samples in this way.</p>
<p>Markov chain Monte Carlo (MCMC) methods <span class="citation">(<a href="#ref-roberts2004general">Roberts and Rosenthal 2004</a>)</span> are the most popular class of sampling algorithms.
Using MCMC, samples are generated from by simulating from an ergodic Markov chain with the posterior distribution as its stationary distribution.
The Metropolis-Hastings [MH; <span class="citation">Metropolis et al. (<a href="#ref-metropolis1953equation">1953</a>)</span>; <span class="citation">Hastings (<a href="#ref-hastings1970monte">1970</a>)</span>] algorithm uses a proposal distribution <span class="math inline">\(q(\boldsymbol{\mathbf{\phi}}_{s + 1} \, | \, \boldsymbol{\mathbf{\phi}}_s)\)</span> to generate candidate parameters for the next step in the Markov chain.
Many MCMC algorithms, including the Gibbs sampler <span class="citation">(<a href="#ref-geman1984stochastic">Geman and Geman 1984</a>)</span>, are special cases of MH.</p>
<p>Other notable classes of sampling algorithms include importance sampling [IS; <span class="citation">Tokdar and Kass (<a href="#ref-tokdar2010importance">2010</a>)</span>] methods in which the samples are weighted, sequential Monte Carlo [SMC; <span class="citation">Chopin, Papaspiliopoulos, et al. (<a href="#ref-chopin2020introduction">2020</a>)</span>] methods based on sampling from a sequence of distributions, and approximate Bayesian computation [ABC; <span class="citation">Sisson, Fan, and Beaumont (<a href="#ref-sisson2018handbook">2018</a>)</span>]
Though these methods have found applications in specific domains, MCMC is currently more widely used because of its generality, theoretical reliability, as well as benefiting from more accessible software implementations.</p>
<p>This thesis uses the No-U-Turn sampler [NUTS; <span class="citation">Hoffman, Gelman, et al. (<a href="#ref-hoffman2014no">2014</a>)</span>], a Hamiltonian Monte Carlo [HMC; <span class="citation">Duane et al. (<a href="#ref-duane1987hybrid">1987</a>)</span>; <span class="citation">Neal et al. (<a href="#ref-neal2011mcmc">2011</a>)</span>] algorithm, as implemented in the <code>Stan</code> <span class="citation">(<a href="#ref-carpenter2017stan">Carpenter et al. 2017</a>)</span> probabilistic programming language (PPL).
HMC uses derivatives of the posterior distribution to generate efficient MH proposal distributions based on Hamiltonian dynamics.
NUTS automatically adapts the tuning parameters of HMC based local properties of the posterior distribution.
Though not a one-size-fits-all solution, NUTS has been shown empirically to be a good choice for sampling from a range of posterior distributions.
Figure <a href="bayes-st.html#fig:stan">3.2</a> shows an example of using the NUTS MCMC algorithm to sample from a posterior distribution.</p>
<p>After running an MCMC sampler, it is important that diagnostic checks are used to evaluate convergence and assess whether the results of the Markov chain can be used to compute posterior quantities.
It is never possible to be sure that results computed from MCMC will be accurate, though it is possible to know when they will be inaccurate.
Panel <a href="bayes-st.html#fig:stan">3.2</a>B shows the traceplot for a Markov chain which has converged, and moves freely through the range of plausible parameter values.
A range of convergence diagnostics have been developed for MCMC <span class="citation">(<a href="#ref-roy2020convergence">Roy 2020</a>; <a href="#ref-margossian2023many">C. C. Margossian and Gelman 2023</a>)</span>.
Two widely used examples are the potential scale reduction factor <span class="math inline">\(\hat R\)</span> <span class="citation">(<a href="#ref-gelman1992inference">Gelman and Rubin 1992</a>)</span>, which compares the variance between and within parallel Markov chains, and the effective sample size (ESS), which measures the efficiency of samples drawn from MCMC.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:stan"></span>
<img src="figures/bayesian/stan.png" alt="NUTS can be used to sample from the posterior distribution described in Figure 3.1. Panel A shows a histogram of the NUTS samples as compared to the true posterior. The visual appearance of a histogram depends highly on the number of bins chosen. Other visualisations, such as empirical cumulative difference function plots, though less initially intuitive, are preferred for accurate distributinal sample comparisons. Panel B is a traceplot showing the path of the Markov chain \(\{\phi_s\}_{s = 1}^{1000}\) as it explores the posterior distribution. In this case, the Markov chain moves freely throughout the posterior distribution, without getting stuck in any one location for long, indicating good performance of the sampler. Panel C shows the convergence of the empirical posterior mean \(\frac{1}{s} \sum_{l \leq s} \phi_l\) to the true value of \(\mathbb{E}(\phi)\) as more iterations of the Markov chain are included in the sum." width="95%" />
<p class="caption">
Figure 3.2: NUTS can be used to sample from the posterior distribution described in Figure <a href="bayes-st.html#fig:conjugate">3.1</a>. Panel A shows a histogram of the NUTS samples as compared to the true posterior. The visual appearance of a histogram depends highly on the number of bins chosen. Other visualisations, such as empirical cumulative difference function plots, though less initially intuitive, are preferred for accurate distributinal sample comparisons. Panel B is a traceplot showing the path of the Markov chain <span class="math inline">\(\{\phi_s\}_{s = 1}^{1000}\)</span> as it explores the posterior distribution. In this case, the Markov chain moves freely throughout the posterior distribution, without getting stuck in any one location for long, indicating good performance of the sampler. Panel C shows the convergence of the empirical posterior mean <span class="math inline">\(\frac{1}{s} \sum_{l \leq s} \phi_l\)</span> to the true value of <span class="math inline">\(\mathbb{E}(\phi)\)</span> as more iterations of the Markov chain are included in the sum.
</p>
</div>
</div>
<div id="deterministic-approximations" class="section level4 hasAnchor" number="3.1.2.2">
<h4><span class="header-section-number">3.1.2.2</span> Deterministic approximations<a href="bayes-st.html#deterministic-approximations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Monte Carlo methods discussed in Section <a href="bayes-st.html#monte-carlo">3.1.2.1</a> make use of stochasticity to generate samples from the posterior distribution.
Deterministic approximations offer an alternative approach, which aims to more directly approximate the posterior distribution or posterior normalising constant.
These approaches are often faster than Monte Carlo methods, especially for large datasets or models.</p>
<p>One such deterministic approximation is the Laplace approximation.
It involves approximating the posterior normalising constant using Laplace’s method of integration, and is equivalent to approximating the posterior distribution by a Gaussian distribution.
The integrated nested Laplace approximation [INLA; <span class="citation">Håvard Rue, Martino, and Chopin (<a href="#ref-rue2009approximate">2009</a>)</span>] combines quadrature with the Laplace approximation.
These methods are used often in this thesis.</p>
<p>Variational inference [VI; <span class="citation">Blei, Kucukelbir, and McAuliffe (<a href="#ref-blei2017variational">2017</a>)</span>] is another noteworthy deterministic approximation.
The well-known expectation maximisation [EM; <span class="citation">Dempster, Laird, and Rubin (<a href="#ref-dempster1977maximum">1977</a>)</span>] and expectation propagation [EP; <span class="citation">Minka (<a href="#ref-minka2001expectation">2001</a>)</span>] algorithms are closely related to VI.
In VI, the approximate posterior distribution is assumed to belong to a particular family of functions.
Optimisation algorithms are then used to choose the best member of that family, typically by minimising the Kullback-Leibler divergence to the posterior distribution.
Though popular in some settings, VI lacks theoretical guarantees and is known to often inaccurately estimate posterior variances <span class="citation">(<a href="#ref-giordano2018covariances">Giordano, Broderick, and Jordan 2018</a>)</span>.
Developing diagnostics to evaluate the accuracy of VI is an important area of ongoing research <span class="citation">(<a href="#ref-yao2018yes">Yao et al. 2018</a>)</span>.</p>
</div>
</div>
<div id="interplay-between-modelling-and-computation" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Interplay between modelling and computation<a href="bayes-st.html#interplay-between-modelling-and-computation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Modern computational techniques and software like PPLs have succeeded in abstracting away calculation of the posterior distribution from the analyst for many models.
However, computation remains intractable in what can be argued to be the majority of cases.
The analyst needs therefore not only to be concerned with choosing a model suitable for the data, but also with choosing a model for which the posterior distribution may tractably be calculated in reasonable time.
As such, there is an important interplay between modelling and computation, wherein models are bound by the limits of computation.
As computational techniques and tools improve, the space of models available to the analyst expands.</p>
</div>
</div>
<div id="st-statistics" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Spatio-temporal statistics<a href="bayes-st.html#st-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Space and time are important features of infectious disease data, including those related to HIV.
The field of spatio-temporal statistics <span class="citation">(<a href="#ref-cressie2015statistics">Cressie and Wikle 2015</a>)</span> is concerned with such observations indexed by spatial or temporal location.
It unifies the fields of spatial statistics <span class="citation">(<a href="#ref-bivand2008applied">Bivand et al. 2008</a>)</span>, concerned with observations indexed by space, and time series analysis <span class="citation">(<a href="#ref-shumway2017time">Shumway and Stoffer 2017</a>)</span>, concerned with observations index by time.</p>
<p>Section <a href="bayes-st.html#properties">3.2.1</a> characterises the shared properties of spatio-temporal data.
Section <a href="bayes-st.html#small-area-estimation">3.2.2</a> describes how these properties facilitate the class of methods called small-area estimation that the work in this thesis falls under.</p>
<div id="properties" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Properties of spatio-temporal data<a href="bayes-st.html#properties" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="scales" class="section level4 hasAnchor" number="3.2.1.1">
<h4><span class="header-section-number">3.2.1.1</span> Scales<a href="bayes-st.html#scales" class="anchor-section" aria-label="Anchor link to header"></a></h4>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:st"></span>
<img src="figures/bayesian/st.png" alt="In Panel A, the spatial location of Cape Town in South Africa can be considered a point, and the ZF Mgcawu District Municipality (DM) can be considered as an an area. In Panel B, World AIDS Day, designated on the 1st of December every year, can be considered a point in time, whereas the second fiscal quarter, running through April, May and June, and denoted by Q2 represents a period of time. (In reality, both Cape Town and World AIDS Day are areas, rather than true point locations. Instances of infinitesimal point locations in everyday life are rare.)" width="95%" />
<p class="caption">
Figure 3.3: In Panel A, the spatial location of Cape Town in South Africa can be considered a point, and the ZF Mgcawu District Municipality (DM) can be considered as an an area. In Panel B, World AIDS Day, designated on the 1st of December every year, can be considered a point in time, whereas the second fiscal quarter, running through April, May and June, and denoted by Q2 represents a period of time. (In reality, both Cape Town and World AIDS Day are areas, rather than true point locations. Instances of infinitesimal point locations in everyday life are rare.)
</p>
</div>
<p>The scale of spatio-temporal data refers to its extent and resolution.
Its extent is the length of time and the size of the spatial study region over which data was collected.
Its resolution is how fine-grained those observations were.</p>
<p>The spatial study region <span class="math inline">\(\mathcal{S} \subseteq \mathbb{R}^2\)</span> used in this thesis is typically a country or collection of countries, and assumed to have two dimensions, corresponding to latitude and longitude.
Observations may be associated to a point <span class="math inline">\(s \in \mathcal{S}\)</span> or area <span class="math inline">\(A \subseteq \mathcal{S}\)</span> in the spatial study region.
Panel A<a href="bayes-st.html#fig:st">3.3</a>) illustrates a spatial point and area.</p>
<p>The temporal study period <span class="math inline">\(\mathcal{T} \subseteq \mathbb{R}\)</span> can more generally be assumed to be one-dimensional.
This feature, and the fact that time moves only in the forward direction, is what distinguishes time from space.
As with space, observations may be associated to a point <span class="math inline">\(t \in \mathcal{T}\)</span> or period of time <span class="math inline">\(T \subseteq \mathcal{T}\)</span>.
Panel B<a href="bayes-st.html#fig:st">3.3</a>) illustrates a temporal point and period.</p>
<p>The change-of-support problem <span class="citation">(<a href="#ref-gelfand2001change">Gelfand, Zhu, and Carlin 2001</a>)</span> occurs when data is modelled at a scale different to the one it was observed at.
Special cases of the change-of-support problem include downscaling, upscaling, and dealing with so-called misaligned data.
It is also possible that spatio-temporal observations of the same process are made at multiple scales.
Jointly modelling data at different scales simultaneously is another closely related challenge to the change-of-support problem.</p>
</div>
<div id="correlation-structure" class="section level4 hasAnchor" number="3.2.1.2">
<h4><span class="header-section-number">3.2.1.2</span> Covariance structure<a href="bayes-st.html#correlation-structure" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In “The Design of Experiments” <span class="citation">Fisher (<a href="#ref-fisher1936design">1936</a>)</span> observed that neighbouring crops were more likely to have similar yields than those far apart.
This observation was later termed Tobler’s first law of geography, that “everything is related to everything else, but near things are more related than distant things” <span class="citation">(<a href="#ref-tobler1970computer">Tobler 1970</a>)</span>.
As well as space, Tobler’s first law applies to time.
Observations made close together in time tend to be similar.</p>
<p>This law can be formalised using space-time covariance functions.
A space-time covariance structure <span class="citation">(<a href="#ref-porcu202130">Porcu, Furrer, and Nychka 2021</a>)</span> is said to be separable when it can be factorised as a product of individual spatial and temporal covariances, and nonseparable when it can’t.
A separable space-time covariance could have spatial and temporal components which are either independent and identically distributed (IID) or structured <span class="citation">(<a href="#ref-knorr2000bayesian">Knorr-Held 2000</a>)</span>.
Spatial covariance functions are called isotropic when they apply equally in all directions, and stationary when they are invariant over space.
Temporal covariance structures are often periodic.
Because of their covariance structure, spatio-temporal data are not IID.
Only one observation of a spatio-temporal process is realised.</p>
</div>
<div id="size" class="section level4 hasAnchor" number="3.2.1.3">
<h4><span class="header-section-number">3.2.1.3</span> Size<a href="bayes-st.html#size" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Data with both spatial and temporal dimensions are often large.
This makes storage and mathematical operations on spatio-temporal data potentially challenging.
Furthermore, models for spatio-temporal data typically require many parameters.
Whereas large IID data can be modelled using a small number of parameters, each observation in a spatio-temporal dataset may need to be characterised by its own parameters.
Large data combined with large models make Bayesian inference challenging.</p>
</div>
</div>
<div id="small-area-estimation" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Small-area estimation<a href="bayes-st.html#small-area-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Data always has some cost to collect.
This cost can be significant and prohibitive, especially for data relating to people where collection is difficult to automate.
As a result, given the large number of possible locations in space and time, often no or limited direct observations may be available for any given space-time location.
Direct estimates of indicators of interest are either impossible or inaccurate in this setting.</p>
<p>Small-area estimation [SAE; <span class="citation">Pfeffermann et al. (<a href="#ref-pfeffermann2013new">2013</a>)</span>] methods aim to overcome the limitations of small data by sharing information.
In the spatio-temporal setting sharing of information occurs across space and time.
Prior knowledge that observations in one spatio-temporal location are correlated with those at another can be used to improve estimates.
Figures <a href="bayes-st.html#fig:zmb-maps">3.4</a> and <a href="bayes-st.html#fig:zmb-scatter">3.5</a> illustrate the unreliability of direct estimates from small sample sizes, as well as the way in which a spatial model may be used to overcome this limitation in part.</p>
<p>SAE methods are more generally useful when data are limited for subpopulations of interest.
These subpopulations could be generated by spatio-temporal variables, as well as by other variables such as demographics.
Just as we expect there to be spatio-temporal correlation structure, we also can expect there to be demographic correlation structure.
For example, those of the same sex are more likely to be similar, as are those of similar ages or socio-economic strata.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:zmb-maps"></span>
<img src="figures/bayesian/zmb-maps.png" alt="Simulation of a simple random sample \(y_i \sim \text{Bin}(m, p_i)\) with varying sample size \(m = 5, 25, 125\) in each of the \(i = 1, \ldots, 156\) constituencies of Zambia. Direct estimates were obtained by the empirical ratio of data to sample size. Modelled estimates were obtained using a logistic regression with linear predictor given by an intercept and a spatial random effect. Estimates of HIV indicators for Zambia have previously been generated at the district-level, comprising 116 spatial units. Moving forward, there is interest in generating estimates at the higher-resolution constituency level, as program planning is devolved locally. This figure was adapted from a presentation given for the Zambia HIV Estimates Technical Working Group, available from https://github.com/athowes/zambia-unaids. The viridis colour palette used in this figure, as implemented by the viridis R package (Garnier et al. 2023), was designed to be perceptually uniform and accessible to colourblind viewers (Smith and Walt 2015)." width="95%" />
<p class="caption">
Figure 3.4: Simulation of a simple random sample <span class="math inline">\(y_i \sim \text{Bin}(m, p_i)\)</span> with varying sample size <span class="math inline">\(m = 5, 25, 125\)</span> in each of the <span class="math inline">\(i = 1, \ldots, 156\)</span> constituencies of Zambia. Direct estimates were obtained by the empirical ratio of data to sample size. Modelled estimates were obtained using a logistic regression with linear predictor given by an intercept and a spatial random effect. Estimates of HIV indicators for Zambia have previously been generated at the district-level, comprising 116 spatial units. Moving forward, there is interest in generating estimates at the higher-resolution constituency level, as program planning is devolved locally. This figure was adapted from a presentation given for the Zambia HIV Estimates Technical Working Group, available from <a href="https://github.com/athowes/zambia-unaids"><code>https://github.com/athowes/zambia-unaids</code></a>. The viridis colour palette used in this figure, as implemented by the <code>viridis</code> R package <span class="citation">(<a href="#ref-viridis">Garnier et al. 2023</a>)</span>, was designed to be perceptually uniform and accessible to colourblind viewers <span class="citation">(<a href="#ref-smith2015better">Smith and Walt 2015</a>)</span>.
</p>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:zmb-scatter"></span>
<img src="figures/bayesian/zmb-scatter.png" alt="The setting of this figure matches that of Figure 3.4. Estimates from surveys with higher sample size have higher sample Pearson correlation coefficient \(R\) with the underlying truth, illustrating the benefit of collecting more data. For a fixed sample size however, correlation can be improved by using modelled estimates to borrow information across spatial units, rather than using the higher variance direct estimates. Points along the dashed diagonal line correspond to agreement between the estimate obtained from the survey and the underlying truth used to generate the data. For each sample size, using a spatial model increases the correlation between the estimates and underlying truth. The effect is more pronounced for lower sample sizes." width="95%" />
<p class="caption">
Figure 3.5: The setting of this figure matches that of Figure <a href="bayes-st.html#fig:zmb-maps">3.4</a>. Estimates from surveys with higher sample size have higher sample Pearson correlation coefficient <span class="math inline">\(R\)</span> with the underlying truth, illustrating the benefit of collecting more data. For a fixed sample size however, correlation can be improved by using modelled estimates to borrow information across spatial units, rather than using the higher variance direct estimates. Points along the dashed diagonal line correspond to agreement between the estimate obtained from the survey and the underlying truth used to generate the data. For each sample size, using a spatial model increases the correlation between the estimates and underlying truth. The effect is more pronounced for lower sample sizes.
</p>
</div>
</div>
</div>
<div id="model-structure" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Model structure<a href="bayes-st.html#model-structure" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Section <a href="bayes-st.html#correlation-structure">3.2.1.2</a> described that in spatio-temporal statistics observations are related to each other, and it is inappropriate to treat data as IID.
This section discusses ways to encode relations between observations mathematically via statistical models.
First, simple structures are discussed such as the linear model, before moving on to more expressive structures.</p>
<div id="linear-model" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Linear model<a href="bayes-st.html#linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In a linear model, each observation <span class="math inline">\(i \in [n]\)</span> is modelled using a Gaussian distribution
<span class="math display">\[\begin{equation}
y_i \sim \mathcal{N}(\mu_i, \sigma).
\end{equation}\]</span>
The conditional mean <span class="math inline">\(\mu_i\)</span> is assumed to be linearly related to a collection of <span class="math inline">\(l\)</span> covariates
<span class="math display" id="eq:linear-model">\[\begin{align}
\mu_i &amp;= \eta_i \\
\eta_i &amp;= \beta_0 + \sum_{l = 1}^{p} \beta_l z_{li}. \tag{3.4}
\end{align}\]</span>
Priors may be placed on the regression coefficients <span class="math inline">\(\beta_l \sim p(\beta_l)\)</span> for <span class="math inline">\(l = 0, \ldots, p\)</span> as well as the observation standard deviation <span class="math inline">\(\sigma \sim p(\sigma)\)</span>.</p>
</div>
<div id="generalised-linear-model" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Generalised linear model<a href="bayes-st.html#generalised-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Generalised linear models (GLMs) extend the linear model by allowing the conditional mean <span class="math inline">\(\mu_i\)</span> to be connected to the linear predictor <span class="math inline">\(\eta_i\)</span> via a link function <span class="math inline">\(g\)</span>.
<span class="math display">\[\begin{align}
y_i &amp;\sim p(y_i \, | \, \eta_i), \\
\mu_i &amp;= \mathbb{E}(y_i \, | \, \eta_i) = g(\eta_i).
\end{align}\]</span>
The logistic function <span class="math inline">\(g(\eta) = \exp(\eta) / (1 + \exp(\eta))\)</span> is commonly used as a link function to ensure that the conditional mean is in the range <span class="math inline">\([0, 1]\)</span>.
Similarly, the exponential function <span class="math inline">\(g(\eta) = \exp(\eta)\)</span> can be used to ensure the conditional mean is positive.
GLMs admit a wider range of likelihoods <span class="math inline">\(p(y_i \, | \, \eta_i)\)</span> than linear models, typically restricted to the so-called exponential family of distributions.
The equation for the linear predictor is the same as in the linear model case (Equation <a href="bayes-st.html#eq:linear-model">(3.4)</a>).</p>
</div>
<div id="generalised-linear-mixed-effects-model" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Generalised linear mixed effects model<a href="bayes-st.html#generalised-linear-mixed-effects-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In a generalised linear mixed effects model (GLMM) the linear predictor of the GLM is extended as follows
<span class="math display">\[\begin{equation}
\eta_i = \beta_0 + \sum_{l = 1}^{p} \beta_l z_{li} + \sum_{k = 1}^{r} u_k(w_{ki}).
\end{equation}\]</span>
The terms <span class="math inline">\(u_k\)</span> are called random effects, of additional covariates <span class="math inline">\(w_{ki}\)</span>.
The terms <span class="math inline">\(\beta_l\)</span> are referred to as fixed effects.
These terms have notoriously many different and incompatible definitions which unfortunately can cause confusion <span class="citation">(<a href="#ref-gelman2005analysis">Gelman 2005</a>)</span>.</p>
<p>Random effects allow for more complex sharing of information between observations.
To demonstrate this fact, consider the model
<span class="math display">\[\begin{equation}
\eta_i = \beta_0.
\end{equation}\]</span>
In this model, known as the complete pooling model, all observations are assumed to be equivalent.
Alternatively, consider the so-called no pooling model
<span class="math display">\[\begin{equation}
\eta_i = \beta_0 + \beta_1 z_i,
\end{equation}\]</span>
with <span class="math inline">\(z_i \in \{0, 1\}\)</span> a binary covariate.
Now, there are two groups of observations, each of which with its own mean, <span class="math inline">\(\beta_0\)</span> for the first group and <span class="math inline">\(\beta_0 + \beta_1\)</span> for the second.
Finally, consider an intermediate between these two extremes, known as the partial pooling model.
In the partial pooling model, the extent to which information is shared between groups is learnt rather than fixed at the outset, as with the complete or no pooling models.
The parameter <span class="math inline">\(\beta_0\)</span> applies to all groups, and each group is differentiated by a specific value of the random effects <span class="math inline">\(u_i\)</span>.</p>
<p>Random effects can be structured to share information between some observations more than others.
In spatio-temporal statistics, structured spatial and temporal random effects are often used to encode smoothness in space or time.</p>
<p>Generalised additive models [GAMs; <span class="citation">Wood (<a href="#ref-wood2017generalized">2017</a>)</span>; <span class="citation">Hastie and Tibshirani (<a href="#ref-hastie1987generalized">1987</a>)</span>] can be cast to fit into the GLMM framework.
GAMs place more of a focus on using <span class="math inline">\(u_k\)</span> to do something.</p>
</div>
<div id="lgm" class="section level3 hasAnchor" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> Latent Gaussian model<a href="bayes-st.html#lgm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Latent Gaussian models [LGMs; <span class="citation">Håvard Rue, Martino, and Chopin (<a href="#ref-rue2009approximate">2009</a>)</span>] are a type of GLMMs in which Gaussian priors are used for specific parameters of the model.
These parameters are <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\{\beta_j\}\)</span>, <span class="math inline">\(\{u_k(\cdot)\}\)</span>, and can be collected into a vector <span class="math inline">\(\mathbf{x} \in \mathbb{R}^N\)</span> called the latent field.
The Gaussian prior distribution is
<span class="math display">\[\begin{equation}
\mathbf{x} \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}(\boldsymbol{\mathbf{\theta}}_2)^{-1}),
\end{equation}\]</span>
where <span class="math inline">\(\boldsymbol{\mathbf{\theta}}_2 \in \mathbb{R}^{s_2}\)</span> are hyperparameters, with <span class="math inline">\(s_2\)</span> assumed small.
The vector <span class="math inline">\(\boldsymbol{\mathbf{\theta}}_1 \in \mathbb{R}^{s_1}\)</span>, with <span class="math inline">\(s_1\)</span> assumed small, are additional parameters of the likelihood.
Let <span class="math inline">\(\boldsymbol{\mathbf{\theta}} = (\boldsymbol{\mathbf{\theta}}_1, \boldsymbol{\mathbf{\theta}}_2) \in \mathbb{R}^m\)</span> with <span class="math inline">\(m = s_1 + s_2\)</span> be all hyperparameters, with prior distribution <span class="math inline">\(p(\boldsymbol{\mathbf{\theta}})\)</span>.
The posterior distribution under an LGM in then
<span class="math display">\[\begin{equation}
p(\mathbf{x}, \boldsymbol{\mathbf{\theta}} \, | \, \mathbf{y}) \propto p(\mathbf{y} \, | \, \mathbf{x}, \boldsymbol{\mathbf{\theta}}) p(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}) p(\boldsymbol{\mathbf{\theta}}),
\end{equation}\]</span>
with the complete set of parameters <span class="math inline">\(\boldsymbol{\mathbf{\phi}} = (\mathbf{x}, \boldsymbol{\mathbf{\theta}})\)</span>.</p>
</div>
<div id="elgm" class="section level3 hasAnchor" number="3.3.5">
<h3><span class="header-section-number">3.3.5</span> Extended latent Gaussian model<a href="bayes-st.html#elgm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Extended latent Gaussian models [ELGMs; <span class="citation">Stringer, Brown, and Stafford (<a href="#ref-stringer2022fast">2022</a>)</span>] facilitate modelling of data with greater non-linearities than an LGM.
In an ELGM, the structured additive predictor is redefined as
<span class="math display">\[\begin{equation}
\boldsymbol{\mathbf{\eta}} = (\eta_1, \ldots \eta_{N_n}),
\end{equation}\]</span>
where <span class="math inline">\(N_n \in \mathbb{N}\)</span> is a function of <span class="math inline">\(n\)</span>.
Unlike in the LGM case, it is possible that <span class="math inline">\(N_n \neq n\)</span>.
Each mean response <span class="math inline">\(\mu_i\)</span> now depends on some subset <span class="math inline">\(\mathcal{J}_i \subseteq [N_n]\)</span> of indices of <span class="math inline">\(\boldsymbol{\mathbf{\eta}}\)</span>, with <span class="math inline">\(\cup_{i = 1}^n \mathcal{J}_i = [N_n]\)</span> and <span class="math inline">\(1 \leq |\mathcal{J}_i| \leq N_n\)</span>, where <span class="math inline">\([N_n] = \{1, \ldots, N_n\}\)</span>.
The inverse link function <span class="math inline">\(g(\cdot)\)</span> is redefined for each observation to be a possibly many-to-one mapping <span class="math inline">\(g_i: \mathbb{R}^{|\mathcal{J}_i|} \to \mathbb{R}\)</span>, such that <span class="math inline">\(\mu_i = g_i(\boldsymbol{\mathbf{\eta}}_{\mathcal{J}_i})\)</span>.
Put together, ELGMs are of the form
<span class="math display">\[\begin{align*}
y_i &amp;\sim p(y_i \, | \, \boldsymbol{\mathbf{\eta}}_{\mathcal{J}_i}, \boldsymbol{\mathbf{\theta}}_1), \quad i = 1, \ldots, n, \\
\mu_i &amp;= \mathbb{E}(y_i \, | \, \boldsymbol{\mathbf{\eta}}_{\mathcal{J}_i}) = g_i(\boldsymbol{\mathbf{\eta}}_{\mathcal{J}_i}), \\
\eta_j &amp;= \beta_0 + \sum_{l = 1}^{p} \beta_l z_{li} + \sum_{k = 1}^{r} u_k(w_{ki}), \quad j \in [N_n].
\end{align*}\]</span>
The latent field and hyperparameter prior distributions are equivalent to the LGM case.</p>
<p>The ELGM class is well suited to small-area estimation of HIV.
While it can be transformed to an LGM using the Poisson-multinomial transformation <span class="citation">(<a href="#ref-baker1994multinomial">Baker 1994</a>)</span> the multinomial logistic regression model used in Chapter <a href="multi-agyw.html#multi-agyw">5</a> is most naturally written as an ELGM, where each observation depends on the set of structured additive predictors corresponding to the set of multinomial observations.
In Chapter <a href="naomi-aghq.html#naomi-aghq">6</a>, the Naomi small-area estimation model used to produce estimates of HIV indicators is shown to have the features of an ELGM.</p>
</div>
</div>
<div id="model-comparison" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Model comparison<a href="bayes-st.html#model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Many models can be formulated and fit to data during the course of an analysis.
Model comparison methods look to determine which models are most suitable to use, based on actualisation of suitability.</p>
<div id="bayes-factors" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Bayes factors<a href="bayes-st.html#bayes-factors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The evidence <span class="math inline">\(p(\mathbf{y})\)</span> (Equation <a href="bayes-st.html#eq:evidence">(3.3)</a>) can be used as a measure of model fit
Let <span class="math inline">\(\mathcal{M}_0\)</span> and <span class="math inline">\(\mathcal{M}_1\)</span> be two competing models.
Then the Bayes factor comparing <span class="math inline">\(\mathcal{M}_0\)</span> to <span class="math inline">\(\mathcal{M_1}\)</span> is
<span class="math display">\[\begin{equation}
B_{01} = \frac{p(\mathbf{y} \, | \, \ \mathcal{M}_0)}{p(\mathbf{y} \, | \, \ \mathcal{M}_1)},
\end{equation}\]</span>
where <span class="math inline">\(p(\mathbf{y} \, | \, \ \mathcal{M})\)</span> denotes the evidence under model <span class="math inline">\(\mathcal{M}\)</span>.
If <span class="math inline">\(B_{01} &gt; 1\)</span> then support is provided for <span class="math inline">\(\mathcal{M_0}\)</span> and if <span class="math inline">\(B_{01} &lt; 1\)</span> the support is provided for <span class="math inline">\(\mathcal{M_1}\)</span>.</p>
</div>
<div id="information-criteria" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Information criteria<a href="bayes-st.html#information-criteria" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The log pointwise predictive density (LPPD) is
<span class="math display">\[\begin{equation}
\text{lppd} = \log \left( \prod_{i = 1}^n p(y_i) \right) = \sum_{i = 1}^n \log \int p(y_i \, | \, \boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}}) \text{d} \boldsymbol{\mathbf{\phi}}.
\end{equation}\]</span>
The expected log pointwise predictive density for new data (ELPPD) is
<span class="math display">\[\begin{equation}
\text{elppd} = \log \left( \prod_{i = 1}^n p(\tilde y_i) \right).
\end{equation}\]</span></p>
<p>Information criteria such as the Akaike information criterion [AIC; <span class="citation">Akaike (<a href="#ref-akaike1973information">1973</a>)</span>], deviance information criterion [DIC; <span class="citation">D. J. Spiegelhalter et al. (<a href="#ref-spiegelhalter2002bayesian">2002</a>)</span>], and widely applicable information criterion [WAIC; <span class="citation">Watanabe (<a href="#ref-watanabe2013widely">2013</a>)</span>], are constructed using 1) an approximation to the ELPD given by <span class="math inline">\(\text{elpd}_{\texttt{IC}}\)</span> 2) a complexity penalty <span class="math inline">\(p_{\texttt{IC}}\)</span>.
For the AIC and DIC, the ELPD approximation is
<span class="math display">\[\begin{equation}
\text{elpd}_\texttt{IC} = \log p(\mathbf{y} \, | \, \hat{\boldsymbol{\mathbf{\phi}}}),
\end{equation}\]</span>
where <span class="math inline">\(\hat{\boldsymbol{\mathbf{\phi}}}\)</span> is a maximum likelihood estimate (AIC) or a alternative Bayesian point estimate (DIC).
The WAIC improves upon this limitaiton <span class="citation">(<a href="#ref-spiegelhalter2014deviance">D. J. Spiegelhalter et al. 2014</a>)</span> of the AIC and DIC by using the predictive density of the data
<span class="math display">\[\begin{align}
  \text{elpd}_\texttt{WAIC} = \sum_{i = 1}^n \log p(y_i \, | \, \mathbf{y}).
\end{align}\]</span></p>
<p>For a more complete review of these information criteria from a Bayesian perspective, see <span class="citation">Gelman, Hwang, and Vehtari (<a href="#ref-gelman2014understanding">2014</a>)</span>.</p>
</div>
<div id="cross-validation" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Cross-validation<a href="bayes-st.html#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The information criteria in Section <a href="bayes-st.html#information-criteria">3.4.2</a> can be viewed as approximations to cross-validation.
There are three parts of a cross-validation: the way the data is divided, the scoring rule used, the computational method used <span class="citation">(<a href="#ref-vehtari2020cross">Vehtari 2020</a>)</span>.</p>
</div>
<div id="scoring-rules" class="section level3 hasAnchor" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Scoring rules<a href="bayes-st.html#scoring-rules" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Scoring rules [SR; <span class="citation">Gneiting and Raftery (<a href="#ref-gneiting2007strictly">2007</a>)</span>] measure the quality of probabilistic forecasts.
The log score, used above in the LPPD and ELPPD, is one example of a scoring rule.</p>
<p>A scoring rule is called proper when such and such.
A scoring rule is called strictly proper when such and such.</p>
<p>The log score (LS) is a strictly proper scoring rule (SPSR).
Another example of a SPSR is the continuous ranked probability score [CRPS; <span class="citation">Matheson and Winkler (<a href="#ref-matheson1976scoring">1976</a>)</span>], which generalises the Brier score <span class="citation">(<a href="#ref-brier1950verification">Brier 1950</a>)</span> beyond binary classification.</p>
</div>
</div>
<div id="survey" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Survey methods<a href="bayes-st.html#survey" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Large national household surveys (Section <a href="hiv-aids.html#hiv-data">2.2.1</a>) provide the high quality population-level information about HIV indicators in SSA.
Two examples are the Demographic and Health Surveys [DHS; <span class="citation">DHS (<a href="#ref-measure2012sampling">2012</a>)</span>] and Population-based HIV Impact Assessment (PHIA) surveys.
DHS are funded by the United States Agency for International Development (USAID) and run every three to five years in most countries, and PHIA are funded by PEPFAR and run every four to five years in high HIV burden countries.
Specific methods are required to analyse the data produced by these surveys.</p>
<div id="survey-notation-and-key-terms" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Survey notation and key terms<a href="bayes-st.html#survey-notation-and-key-terms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a population of individuals <span class="math inline">\(i = 1, \ldots, N\)</span> with outcomes of interest <span class="math inline">\(y_i\)</span>.
It is usually too expensive or otherwise impractical to run a census, so only a subset of these individuals are sampled.
Let <span class="math inline">\(S_i\)</span> be an indicator for whether or not individual <span class="math inline">\(i\)</span> is sampled.
Furthermore, only a subset of those sampled have their outcome recorded, due to nonresponse or otherwise.
Let <span class="math inline">\(R_i\)</span> be an indicator for whether or not <span class="math inline">\(y_i\)</span> is recorded.
If <span class="math inline">\(S_i = 0\)</span> then <span class="math inline">\(R_i = 0\)</span>, and if <span class="math inline">\(S_i = 1\)</span> then individual <span class="math inline">\(i\)</span> may not respond such that <span class="math inline">\(R_i = 0\)</span>.</p>
<p>If a census were run, and all responses were recorded, then any population quantity could be directly calculated.
For example, if <span class="math inline">\(G_i = G(y_i)\)</span> then the population mean of <span class="math inline">\(G\)</span> is
<span class="math display">\[\begin{equation}
\bar G = \frac{1}{N} \sum_{i = 1}^N G(y_i).
\end{equation}\]</span>
Instead, population quantities may be estimated based on the recorded subset of the population by
<span class="math display">\[\begin{equation}
\bar G_R = \frac{\sum_{i = 1}^N R_i G(y_i)}{\sum_{i = 1}^N R_i}, \label{eq:direct}
\end{equation}\]</span>
where <span class="math inline">\(m_R = \sum_{i = 1}^N R_i\)</span> is the recorded sample size.</p>
<p>A probability sample refers to the case when individuals are selected to be included in the survey at random.
In a non-probability sample, inclusion or exclusion from the survey is deterministic.
A simple random sample (SRS) is a probability sample where the sampling probability for each individual is equal
<span class="math display">\[\begin{equation}
P(S_i = 1) = \frac{1}{N}.
\end{equation}\]</span>
A survey design is called complex when the sampling probabilities for each individual vary, such that
<span class="math display">\[\begin{equation}
P(S_i = 1) = \pi_i,
\end{equation}\]</span>
with <span class="math inline">\(\sum_{i = 1}^N \pi_i = 1\)</span> and <span class="math inline">\(\pi_i &gt; 0\)</span>.
Complex survey designs can offer both greater practicality and statistical efficiency than a SRS.
Care is required in analysing data collected using complex survey designs.
Under a complex design, not accounting for unequal sampling probabilities will result in bias (that said, for a SRS, nonresponse can cause analogous bias).</p>
</div>
<div id="survey-design" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Survey design<a href="bayes-st.html#survey-design" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- Add figure here to demonstrate sampling of DHS data -->
<p>The DHS employs a two-stage sampling procedure.
In the first stage, ennumeration areas (EAs) from a recently conducted census are typically used as the primary sampling unit (PSU).
The EAs are then stratified by region, as well as by urban-rural status.
After appropriate sample sizes are determined, EAs sampled with probability proportional to size (PPS).
In the second stage, the secondary sampling units (SSUs) are households.
All households in the selected EAs are listed, before being sampled systematically.
Finally, each selected household is visited, and all adults are interviewed.</p>
<p>The probability an individual is sampled is equal to the probability their household is sampled.
The first-stage sampling probability of the <span class="math inline">\(j\)</span>th cluster in stratum <span class="math inline">\(h\)</span> given by
<span class="math display">\[\begin{equation}
\pi_{1hj} = n_h \times \frac{N_{hj}}{\sum_j N_{hj}},
\end{equation}\]</span>
where <span class="math inline">\(N_{hj}\)</span> is the number of households and <span class="math inline">\(n_h\)</span> be the number of clusters selected in stratum <span class="math inline">\(h\)</span>.
The second-stage sampling probability each household within the <span class="math inline">\(i\)</span>th cluster in stratum <span class="math inline">\(h\)</span> is
<span class="math display">\[\begin{equation}
\pi_{1hj} = \frac{n_{hj}}{N_{hj}},
\end{equation}\]</span>
where <span class="math inline">\(n_{hj}\)</span> is the number of households selected in cluster <span class="math inline">\(j\)</span> and stratum <span class="math inline">\(h\)</span>.
That is, each household in the cluster has equal selection probability.
The overall selection probability of each household in cluster <span class="math inline">\(j\)</span> of stratum <span class="math inline">\(h\)</span> is <span class="math inline">\(\pi_{hi} = \pi_{1hj} \times \pi_{2hj}\)</span>.</p>
</div>
<div id="survey-analysis" class="section level3 hasAnchor" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Survey analysis<a href="bayes-st.html#survey-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose a complex survey is run with sampling probabilities <span class="math inline">\(\pi_i\)</span>.
The standard method for taking into account that some individuals are more likely to be included in the survey than others is to overweight the responses of those unlikely to be included, and underweight the responses of those likely to be included.
This can be achieved using design weights <span class="math inline">\(\delta_i = 1 / \pi_i\)</span>, which can be thought of as the number of individuals in the population represented by the <span class="math inline">\(i\)</span>th sampled individual.
Let <span class="math inline">\(P(R_i = 1 \, | \, S_i = 1) = \upsilon_i\)</span> be the probability of response for sampled individual <span class="math inline">\(i\)</span>.
The problem of nonresponse can be treated in the same way using nonresponse weights <span class="math inline">\(\gamma_i = 1 / \upsilon_i\)</span>, which analogously can be thought of as the number of sampled individuals represented by the <span class="math inline">\(i\)</span>th recorded individual.
Multiplying the design and nonresponse weights gives survey weights <span class="math inline">\(\omega_i = \delta_i \times \gamma_i\)</span>.</p>
<p>A weighted estimate <span class="citation">(<a href="#ref-hajek1971discussion">Hájek 1971</a>)</span> of the population mean using the survey weights <span class="math inline">\(\omega_i\)</span> is given by
<span class="math display">\[\begin{equation}
\bar G_\omega = \frac{\sum_{i = 1}^N \omega_j R_i G(y_i)}{\sum_{i = 1}^N \omega_i R_i}. \label{eq:hajek}
\end{equation}\]</span>
Decomposing the additive error of this estimate provides useful intuition as to the potential benefits of survey weighting.
Following <span class="citation">Meng (<a href="#ref-meng2018statistical">2018</a>)</span> then under SRS
<span class="math display">\[\begin{align}
\bar G_\omega - \bar G &amp;= \frac{\mathbb{E}(\omega_i R_i G_i)}{\mathbb{E}(\omega_i R_i)} - \mathbb{E}(G_i) = \frac{\mathbb{C}(\omega_i R_i G_i)}{\mathbb{E}(\omega_i R_i)} \\
&amp;= \rho_{R_\omega, G} \times \sqrt{\frac{N - m_{R_\omega}}{m_{R_\omega}}} \times \sigma_G,
\end{align}\]</span>
where <span class="math inline">\(R_\omega = \omega R\)</span>.
The data defect correlation (DDC) <span class="math inline">\(\rho_{R_\omega, G}\)</span> measures the correlation between the weighted recording mechanism and given function of the outcome of interest.
To minimise the DDC then <span class="math inline">\(G \perp \!\!\! \perp R_\omega\)</span>.
The data scarcity <span class="math inline">\(\sigma_{R_\omega} = \sqrt{(N - m_{R_\omega})/m_{R_\omega}}\)</span> measures the effective proportion of the population who have been recorded.
The problem difficultly <span class="math inline">\(\sigma_G\)</span> measures the intrinsic difficulty of the estimation problem, and is independent of the sampling or analysis method.</p>
<p>For simplicity, let <span class="math inline">\(G(y_i) = y_i\)</span> and each <span class="math inline">\(y_i \in \{0, 1\}\)</span>.
We weight then model following <span class="citation">Chen, Wakefield, and Lumely (<a href="#ref-chen2014use">2014</a>)</span>.
While this approach acknowledges the survey design, it has some important limitations.
We ignore clustering structure.
All of this isn’t great and that someone should figure this out <span class="citation">(<a href="#ref-gelman2007struggles">Gelman 2007</a>)</span>.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-akaike1973information" class="csl-entry">
Akaike, Hirotugu. 1973. <span>“<span class="nocase">Information theory as an extension of the maximum likelihood principle–In: Second International Symposium on Information Theory (Eds) BN Petrov, F</span>.”</span> <em>Csaki. BNPBF Csaki Budapest: Academiai Kiado</em>.
</div>
<div id="ref-baker1994multinomial" class="csl-entry">
Baker, Stuart G. 1994. <span>“<span class="nocase">The multinomial-Poisson transformation</span>.”</span> <em>Journal of the Royal Statistical Society: Series D (The Statistician)</em> 43 (4): 495–504.
</div>
<div id="ref-berger2006case" class="csl-entry">
Berger, James. 2006. <span>“<span class="nocase">The Case for objective Bayesian analysis</span>.”</span> <em>Bayesian Analysis</em> 1 (3): 385–402.
</div>
<div id="ref-bernardo2001bayesian" class="csl-entry">
Bernardo, José M, and Adrian FM Smith. 2001. <em><span class="nocase">Bayesian theory</span></em>. John Wiley &amp; Sons.
</div>
<div id="ref-bivand2008applied" class="csl-entry">
Bivand, Roger S, Edzer J Pebesma, Virgilio Gómez-Rubio, and Edzer Jan Pebesma. 2008. <em><span class="nocase">Applied spatial data analysis with R</span></em>. Vol. 747248717. Springer.
</div>
<div id="ref-blei2017variational" class="csl-entry">
Blei, David M, Alp Kucukelbir, and Jon D McAuliffe. 2017. <span>“<span class="nocase">Variational inference: A review for statisticians</span>.”</span> <em>Journal of the American Statistical Association</em> 112 (518): 859–77.
</div>
<div id="ref-brier1950verification" class="csl-entry">
Brier, Glenn W. 1950. <span>“<span class="nocase">Verification of forecasts expressed in terms of probability</span>.”</span> <em>Monthly Weather Review</em> 78 (1): 1–3.
</div>
<div id="ref-carpenter2017stan" class="csl-entry">
Carpenter, Bob, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2017. <span>“<span class="nocase">Stan: A probabilistic programming language</span>.”</span> <em>Journal of Statistical Software</em> 76 (1).
</div>
<div id="ref-chen2014use" class="csl-entry">
Chen, Cici, Jon Wakefield, and Thomas Lumely. 2014. <span>“<span class="nocase">The use of sampling weights in Bayesian hierarchical models for small area estimation</span>.”</span> <em>Spatial and Spatio-Temporal Epidemiology</em> 11: 33–43.
</div>
<div id="ref-chopin2020introduction" class="csl-entry">
Chopin, Nicolas, Omiros Papaspiliopoulos, et al. 2020. <em><span class="nocase">An introduction to sequential Monte Carlo</span></em>. Vol. 4. Springer.
</div>
<div id="ref-cressie2015statistics" class="csl-entry">
Cressie, Noel, and Christopher K Wikle. 2015. <em><span class="nocase">Statistics for spatio-temporal data</span></em>. John Wiley &amp; Sons.
</div>
<div id="ref-dempster1977maximum" class="csl-entry">
Dempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. <span>“<span class="nocase">Maximum likelihood from incomplete data via the EM algorithm</span>.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 39 (1): 1–22.
</div>
<div id="ref-measure2012sampling" class="csl-entry">
DHS. 2012. <span>“<span class="nocase">Sampling and Household Listing Manual: Demographic and Health Surveys Methodology</span>.”</span>
</div>
<div id="ref-duane1987hybrid" class="csl-entry">
Duane, Simon, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. 1987. <span>“<span>Hybrid Monte Carlo</span>.”</span> <em>Physics Letters B</em> 195 (2): 216–22.
</div>
<div id="ref-fisher1936design" class="csl-entry">
Fisher, Ronald Aylmer. 1936. <span>“<span class="nocase">Design of experiments</span>.”</span> <em>British Medical Journal</em> 1 (3923): 554.
</div>
<div id="ref-viridis" class="csl-entry">
Garnier, Simon, Ross, Noam, Rudis, Robert, Camargo, et al. 2023. <em><span class="nocase">viridis(Lite) - Colorblind-Friendly Color Maps for R</span></em>. <a href="https://doi.org/10.5281/zenodo.4679423">https://doi.org/10.5281/zenodo.4679423</a>.
</div>
<div id="ref-gelfand2001change" class="csl-entry">
Gelfand, Alan E, Li Zhu, and Bradley P Carlin. 2001. <span>“<span class="nocase">On the change of support problem for spatio-temporal data</span>.”</span> <em>Biostatistics</em> 2 (1): 31–45.
</div>
<div id="ref-gelman2005analysis" class="csl-entry">
Gelman, Andrew. 2005. <span>“<span class="nocase">Analysis of variance—why it is more important than ever</span>.”</span>
</div>
<div id="ref-gelman2007struggles" class="csl-entry">
———. 2007. <span>“<span class="nocase">Struggles with survey weighting and regression modeling</span>.”</span>
</div>
<div id="ref-gelman2013bayesian" class="csl-entry">
Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. <em><span class="nocase">Bayesian data analysis</span></em>. CRC press.
</div>
<div id="ref-gelman2014understanding" class="csl-entry">
Gelman, Andrew, Jessica Hwang, and Aki Vehtari. 2014. <span>“<span class="nocase">Understanding predictive information criteria for Bayesian models</span>.”</span> <em>Statistics and Computing</em> 24 (6): 997–1016.
</div>
<div id="ref-gelman1992inference" class="csl-entry">
Gelman, Andrew, and Donald B Rubin. 1992. <span>“<span class="nocase">Inference from iterative simulation using multiple sequences</span>.”</span> <em>Statistical Science</em>, 457–72.
</div>
<div id="ref-gelman2017prior" class="csl-entry">
Gelman, Andrew, Daniel Simpson, and Michael Betancourt. 2017. <span>“<span class="nocase">The prior can often only be understood in the context of the likelihood</span>.”</span> <em>Entropy</em> 19 (10): 555.
</div>
<div id="ref-gelman2020bayesian" class="csl-entry">
Gelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. 2020. <span>“<span class="nocase">Bayesian workflow</span>.”</span> <em>arXiv Preprint arXiv:2011.01808</em>.
</div>
<div id="ref-geman1984stochastic" class="csl-entry">
Geman, Stuart, and Donald Geman. 1984. <span>“<span class="nocase">Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images</span>.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, no. 6: 721–41.
</div>
<div id="ref-giordano2018covariances" class="csl-entry">
Giordano, Ryan, Tamara Broderick, and Michael I. Jordan. 2018. <span>“<span class="nocase">Covariances, Robustness, and Variational Bayes</span>.”</span> <em>Journal of Machine Learning Research</em> 19 (51): 1–49. <a href="http://jmlr.org/papers/v19/17-670.html">http://jmlr.org/papers/v19/17-670.html</a>.
</div>
<div id="ref-gneiting2007strictly" class="csl-entry">
Gneiting, Tilmann, and Adrian E Raftery. 2007. <span>“<span class="nocase">Strictly proper scoring rules, prediction, and estimation</span>.”</span> <em>Journal of the American Statistical Association</em> 102 (477): 359–78.
</div>
<div id="ref-goldstein2006subjective" class="csl-entry">
Goldstein, Michael. 2006. <span>“<span class="nocase">Subjective Bayesian analysis: principles and practice</span>.”</span>
</div>
<div id="ref-hajek1971discussion" class="csl-entry">
Hájek, Jaroslav. 1971. <span>“<span class="nocase">Discussion of <span>‘An essay on the logical foundations of survey sampling, part I’</span></span>.”</span> <em>Foundations of Statistical Inference (Proc. Sympos., Univ. Waterloo, Ontario, 1970)</em>, 236.
</div>
<div id="ref-hastie1987generalized" class="csl-entry">
Hastie, Trevor, and Robert Tibshirani. 1987. <span>“<span class="nocase">Generalized additive models: some applications</span>.”</span> <em>Journal of the American Statistical Association</em> 82 (398): 371–86.
</div>
<div id="ref-hastings1970monte" class="csl-entry">
Hastings, W Keith. 1970. <span>“<span class="nocase">Monte Carlo sampling methods using Markov chains and their applications</span>.”</span>
</div>
<div id="ref-hoffman2014no" class="csl-entry">
Hoffman, Matthew D, Andrew Gelman, et al. 2014. <span>“<span class="nocase">The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.</span>”</span> <em>J. Mach. Learn. Res.</em> 15 (1): 1593–623.
</div>
<div id="ref-knorr2000bayesian" class="csl-entry">
Knorr-Held, Leonhard. 2000. <span>“<span class="nocase">Bayesian modelling of inseparable space-time variation in disease risk</span>.”</span> <em>Statistics in Medicine</em> 19 (17-18): 2555–67.
</div>
<div id="ref-margossian2023many" class="csl-entry">
Margossian, Charles C, and Andrew Gelman. 2023. <span>“For How Many Iterations Should We Run Markov Chain Monte Carlo?”</span> <em>arXiv Preprint arXiv:2311.02726</em>.
</div>
<div id="ref-martin2023computing" class="csl-entry">
Martin, Gael M, David T Frazier, and Christian P Robert. 2023. <span>“<span class="nocase">Computing Bayes: From then ‘til now</span>.”</span> <em>Statistical Science</em> 1 (1): 1–17.
</div>
<div id="ref-matheson1976scoring" class="csl-entry">
Matheson, James E, and Robert L Winkler. 1976. <span>“<span class="nocase">Scoring rules for continuous probability distributions</span>.”</span> <em>Management Science</em> 22 (10): 1087–96.
</div>
<div id="ref-mcelreath2020statistical" class="csl-entry">
McElreath, Richard. 2020. <em><span class="nocase">Statistical rethinking: A Bayesian course with examples in R and Stan</span></em>. CRC press.
</div>
<div id="ref-meng2018statistical" class="csl-entry">
Meng, Xiao-Li. 2018. <span>“<span class="nocase">Statistical paradises and paradoxes in big data (i) law of large populations, big data paradox, and the 2016 us presidential election</span>.”</span> <em>The Annals of Applied Statistics</em> 12 (2): 685–726.
</div>
<div id="ref-metropolis1953equation" class="csl-entry">
Metropolis, Nicholas, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. 1953. <span>“<span class="nocase">Equation of State Calculations by Fast Computing Machines</span>.”</span> <em>J. Chem. Phys</em> 21: 1087.
</div>
<div id="ref-minka2001expectation" class="csl-entry">
Minka, Thomas P. 2001. <span>“<span class="nocase">Expectation Propagation for approximate Bayesian inference</span>.”</span> In <em>Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence</em>, 362–69.
</div>
<div id="ref-neal2011mcmc" class="csl-entry">
Neal, Radford M et al. 2011. <span>“<span class="nocase">MCMC using Hamiltonian dynamics</span>.”</span> <em>Handbook of Markov Chain Monte Carlo</em> 2 (11): 2.
</div>
<div id="ref-pfeffermann2013new" class="csl-entry">
Pfeffermann, Danny et al. 2013. <span>“<span class="nocase">New Important Developments in Small Area Estimation</span>.”</span> <em>Statistical Science</em> 28 (1): 40–68.
</div>
<div id="ref-porcu202130" class="csl-entry">
Porcu, Emilio, Reinhard Furrer, and Douglas Nychka. 2021. <span>“<span class="nocase">30 Years of space–time covariance functions</span>.”</span> <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 13 (2): e1512.
</div>
<div id="ref-robert2005monte" class="csl-entry">
Robert, Christian P, and George Casella. 2005. <span>“<span class="nocase">Monte Carlo Statistical Methods (Springer Texts in Statistics)</span>.”</span> Springer.
</div>
<div id="ref-roberts2004general" class="csl-entry">
Roberts, Gareth O, and Jeffrey S Rosenthal. 2004. <span>“<span class="nocase">General state space Markov chains and MCMC algorithms</span>.”</span>
</div>
<div id="ref-roy2020convergence" class="csl-entry">
Roy, Vivekananda. 2020. <span>“<span class="nocase">Convergence diagnostics for markov chain monte carlo</span>.”</span> <em>Annual Review of Statistics and Its Application</em> 7: 387–412.
</div>
<div id="ref-rue2009approximate" class="csl-entry">
Rue, Håvard, Sara Martino, and Nicolas Chopin. 2009. <span>“<span class="nocase">Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations</span>.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 71 (2): 319–92.
</div>
<div id="ref-shumway2017time" class="csl-entry">
Shumway, Robert H, and David S Stoffer. 2017. <span>“<span class="nocase">Time Series Analysis and Its Applications With R Examples</span>.”</span>
</div>
<div id="ref-sisson2018handbook" class="csl-entry">
Sisson, Scott A, Yanan Fan, and Mark Beaumont. 2018. <em><span class="nocase">Handbook of approximate Bayesian computation</span></em>. CRC Press.
</div>
<div id="ref-smith2015better" class="csl-entry">
Smith, Nathaniel, and Stéfan van der Walt. 2015. <span>“<span class="nocase">A Better Default Colormap for Matplotlib</span>.”</span> In <em>Proceedings of the 14th Python in Science Conference (SciPy)</em>.
</div>
<div id="ref-spiegelhalter2002bayesian" class="csl-entry">
Spiegelhalter, David J, Nicola G Best, Bradley P Carlin, and Angelika Van Der Linde. 2002. <span>“<span class="nocase">Bayesian measures of model complexity and fit</span>.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 64 (4): 583–639.
</div>
<div id="ref-spiegelhalter2014deviance" class="csl-entry">
Spiegelhalter, David J, Nicola G Best, Bradley P Carlin, and Angelika Van der Linde. 2014. <span>“<span class="nocase">The deviance information criterion: 12 years on</span>.”</span> <em>Journal of the Royal Statistical Society: Series B: Statistical Methodology</em>, 485–93.
</div>
<div id="ref-stringer2022fast" class="csl-entry">
Stringer, Alex, Patrick Brown, and Jamie Stafford. 2022. <span>“<span class="nocase">Fast, scalable approximations to posterior distributions in extended latent Gaussian models</span>.”</span> <em>Journal of Computational and Graphical Statistics</em>, 1–15.
</div>
<div id="ref-tobler1970computer" class="csl-entry">
Tobler, Waldo R. 1970. <span>“<span class="nocase">A computer movie simulating urban growth in the Detroit region</span>.”</span> <em>Economic Geography</em> 46 (sup1): 234–40.
</div>
<div id="ref-tokdar2010importance" class="csl-entry">
Tokdar, Surya T, and Robert E Kass. 2010. <span>“<span class="nocase">Importance sampling: a review</span>.”</span> <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 2 (1): 54–60.
</div>
<div id="ref-vehtari2020cross" class="csl-entry">
Vehtari, Aki. 2020. <span>“Cross-Validation FAQ.”</span> <em>Model Selection Tutorials and Talks</em>.
</div>
<div id="ref-watanabe2013widely" class="csl-entry">
Watanabe, Sumio. 2013. <span>“<span class="nocase">A widely applicable Bayesian information criterion</span>.”</span> <em>Journal of Machine Learning Research</em> 14 (Mar): 867–97.
</div>
<div id="ref-wood2017generalized" class="csl-entry">
Wood, Simon N. 2017. <em><span class="nocase">Generalized additive models: an introduction with R</span></em>. CRC press.
</div>
<div id="ref-yao2018yes" class="csl-entry">
Yao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018. <span>“<span class="nocase">Yes, but did it work?: Evaluating variational inference</span>.”</span> In <em>International Conference on Machine Learning</em>, 5581–90. PMLR.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hiv-aids.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="beyond-borders.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
