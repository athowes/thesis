<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Fast approximate Bayesian inference | Bayesian spatio-temporal methods for small-area estimation of HIV indicators</title>
  <meta name="description" content="6 Fast approximate Bayesian inference | Bayesian spatio-temporal methods for small-area estimation of HIV indicators" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Fast approximate Bayesian inference | Bayesian spatio-temporal methods for small-area estimation of HIV indicators" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Fast approximate Bayesian inference | Bayesian spatio-temporal methods for small-area estimation of HIV indicators" />
  
  
  

<meta name="author" content="Adam Howes" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multi-agyw.html"/>
<link rel="next" href="conclusions.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="templates/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#chapter-overview"><i class="fa fa-check"></i><b>1.1</b> Chapter overview</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hiv-aids.html"><a href="hiv-aids.html"><i class="fa fa-check"></i><b>2</b> The HIV/AIDS epidemic</a>
<ul>
<li class="chapter" data-level="2.1" data-path="hiv-aids.html"><a href="hiv-aids.html#background"><i class="fa fa-check"></i><b>2.1</b> Background</a></li>
<li class="chapter" data-level="2.2" data-path="hiv-aids.html"><a href="hiv-aids.html#surveillance"><i class="fa fa-check"></i><b>2.2</b> HIV surveillance</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="hiv-aids.html"><a href="hiv-aids.html#data"><i class="fa fa-check"></i><b>2.2.1</b> Data</a></li>
<li class="chapter" data-level="2.2.2" data-path="hiv-aids.html"><a href="hiv-aids.html#challenges"><i class="fa fa-check"></i><b>2.2.2</b> Challenges</a></li>
<li class="chapter" data-level="2.2.3" data-path="hiv-aids.html"><a href="hiv-aids.html#statistical-approaches"><i class="fa fa-check"></i><b>2.2.3</b> Statistical approaches</a></li>
<li class="chapter" data-level="2.2.4" data-path="hiv-aids.html"><a href="hiv-aids.html#future-directions"><i class="fa fa-check"></i><b>2.2.4</b> Future directions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayes-st.html"><a href="bayes-st.html"><i class="fa fa-check"></i><b>3</b> Bayesian spatio-temporal statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayes-st.html"><a href="bayes-st.html#bayesian-statistics"><i class="fa fa-check"></i><b>3.1</b> Bayesian statistics</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="bayes-st.html"><a href="bayes-st.html#bayesian-modelling"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian modelling</a></li>
<li class="chapter" data-level="3.1.2" data-path="bayes-st.html"><a href="bayes-st.html#bayesian-computation"><i class="fa fa-check"></i><b>3.1.2</b> Bayesian computation</a></li>
<li class="chapter" data-level="3.1.3" data-path="bayes-st.html"><a href="bayes-st.html#interplay-between-modelling-and-computation"><i class="fa fa-check"></i><b>3.1.3</b> Interplay between modelling and computation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="bayes-st.html"><a href="bayes-st.html#spatio-temporal-statistics"><i class="fa fa-check"></i><b>3.2</b> Spatio-temporal statistics</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="bayes-st.html"><a href="bayes-st.html#properties-of-spatio-temporal-data"><i class="fa fa-check"></i><b>3.2.1</b> Properties of spatio-temporal data</a></li>
<li class="chapter" data-level="3.2.2" data-path="bayes-st.html"><a href="bayes-st.html#small-area-estimation"><i class="fa fa-check"></i><b>3.2.2</b> Small-area estimation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="bayes-st.html"><a href="bayes-st.html#hierarchical-lgm-elgm"><i class="fa fa-check"></i><b>3.3</b> Model structure</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="bayes-st.html"><a href="bayes-st.html#linear-model"><i class="fa fa-check"></i><b>3.3.1</b> Linear model</a></li>
<li class="chapter" data-level="3.3.2" data-path="bayes-st.html"><a href="bayes-st.html#generalised-linear-model"><i class="fa fa-check"></i><b>3.3.2</b> Generalised linear model</a></li>
<li class="chapter" data-level="3.3.3" data-path="bayes-st.html"><a href="bayes-st.html#hierarchical"><i class="fa fa-check"></i><b>3.3.3</b> Hierarchical models</a></li>
<li class="chapter" data-level="3.3.4" data-path="bayes-st.html"><a href="bayes-st.html#generalised-linear-mixed-effects-model"><i class="fa fa-check"></i><b>3.3.4</b> Generalised linear mixed effects model</a></li>
<li class="chapter" data-level="3.3.5" data-path="bayes-st.html"><a href="bayes-st.html#latent-gaussian-model"><i class="fa fa-check"></i><b>3.3.5</b> Latent Gaussian model</a></li>
<li class="chapter" data-level="3.3.6" data-path="bayes-st.html"><a href="bayes-st.html#extended-latent-gaussian-model"><i class="fa fa-check"></i><b>3.3.6</b> Extended latent Gaussian model</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="bayes-st.html"><a href="bayes-st.html#model-comparison"><i class="fa fa-check"></i><b>3.4</b> Model comparison</a></li>
<li class="chapter" data-level="3.5" data-path="bayes-st.html"><a href="bayes-st.html#survey"><i class="fa fa-check"></i><b>3.5</b> Survey methods</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="bayes-st.html"><a href="bayes-st.html#survey-notation-and-key-terms"><i class="fa fa-check"></i><b>3.5.1</b> Survey notation and key terms</a></li>
<li class="chapter" data-level="3.5.2" data-path="bayes-st.html"><a href="bayes-st.html#survey-design"><i class="fa fa-check"></i><b>3.5.2</b> Survey design</a></li>
<li class="chapter" data-level="3.5.3" data-path="bayes-st.html"><a href="bayes-st.html#survey-analysis"><i class="fa fa-check"></i><b>3.5.3</b> Survey analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="beyond-borders.html"><a href="beyond-borders.html"><i class="fa fa-check"></i><b>4</b> Models for spatial structure</a>
<ul>
<li class="chapter" data-level="4.1" data-path="beyond-borders.html"><a href="beyond-borders.html#background-1"><i class="fa fa-check"></i><b>4.1</b> Background</a></li>
<li class="chapter" data-level="4.2" data-path="beyond-borders.html"><a href="beyond-borders.html#models-based-on-adjacency"><i class="fa fa-check"></i><b>4.2</b> Models based on adjacency</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="beyond-borders.html"><a href="beyond-borders.html#the-besag-model"><i class="fa fa-check"></i><b>4.2.1</b> The Besag model</a></li>
<li class="chapter" data-level="4.2.2" data-path="beyond-borders.html"><a href="beyond-borders.html#best-practises-for-the-besag-model"><i class="fa fa-check"></i><b>4.2.2</b> Best practises for the Besag model</a></li>
<li class="chapter" data-level="4.2.3" data-path="beyond-borders.html"><a href="beyond-borders.html#the-reparameterised-besag-york-mollié-model"><i class="fa fa-check"></i><b>4.2.3</b> The reparameterised Besag-York-Mollié model</a></li>
<li class="chapter" data-level="4.2.4" data-path="beyond-borders.html"><a href="beyond-borders.html#concerns-about-the-besag-models-representation-of-space"><i class="fa fa-check"></i><b>4.2.4</b> Concerns about the Besag model’s representation of space</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="beyond-borders.html"><a href="beyond-borders.html#models-using-kernels"><i class="fa fa-check"></i><b>4.3</b> Models using kernels</a></li>
<li class="chapter" data-level="4.4" data-path="beyond-borders.html"><a href="beyond-borders.html#simulation-study"><i class="fa fa-check"></i><b>4.4</b> Simulation study</a></li>
<li class="chapter" data-level="4.5" data-path="beyond-borders.html"><a href="beyond-borders.html#hiv-prevalence-study"><i class="fa fa-check"></i><b>4.5</b> HIV prevalence study</a></li>
<li class="chapter" data-level="4.6" data-path="beyond-borders.html"><a href="beyond-borders.html#discussion"><i class="fa fa-check"></i><b>4.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multi-agyw.html"><a href="multi-agyw.html"><i class="fa fa-check"></i><b>5</b> A model for risk group proportions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="multi-agyw.html"><a href="multi-agyw.html#background-2"><i class="fa fa-check"></i><b>5.1</b> Background</a></li>
<li class="chapter" data-level="5.2" data-path="multi-agyw.html"><a href="multi-agyw.html#data-1"><i class="fa fa-check"></i><b>5.2</b> Data</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="multi-agyw.html"><a href="multi-agyw.html#behavioural-data-from-household-surveys"><i class="fa fa-check"></i><b>5.2.1</b> Behavioural data from household surveys</a></li>
<li class="chapter" data-level="5.2.2" data-path="multi-agyw.html"><a href="multi-agyw.html#other-data"><i class="fa fa-check"></i><b>5.2.2</b> Other data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="multi-agyw.html"><a href="multi-agyw.html#model-for-risk-group-proportions"><i class="fa fa-check"></i><b>5.3</b> Model for risk group proportions</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="multi-agyw.html"><a href="multi-agyw.html#st-multinomial"><i class="fa fa-check"></i><b>5.3.1</b> Spatio-temporal multinomial logistic regression</a></li>
<li class="chapter" data-level="5.3.2" data-path="multi-agyw.html"><a href="multi-agyw.html#s-logistic"><i class="fa fa-check"></i><b>5.3.2</b> Spatial logistic regression</a></li>
<li class="chapter" data-level="5.3.3" data-path="multi-agyw.html"><a href="multi-agyw.html#model-combination"><i class="fa fa-check"></i><b>5.3.3</b> Model combination</a></li>
<li class="chapter" data-level="5.3.4" data-path="multi-agyw.html"><a href="multi-agyw.html#female-sex-worker-population-size-adjustment"><i class="fa fa-check"></i><b>5.3.4</b> Female sex worker population size adjustment</a></li>
<li class="chapter" data-level="5.3.5" data-path="multi-agyw.html"><a href="multi-agyw.html#results"><i class="fa fa-check"></i><b>5.3.5</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="multi-agyw.html"><a href="multi-agyw.html#prevalence-and-incidence-by-risk-group"><i class="fa fa-check"></i><b>5.4</b> Prevalence and incidence by risk group</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="multi-agyw.html"><a href="multi-agyw.html#disaggregation-of-naomi-prevalence-estimates"><i class="fa fa-check"></i><b>5.4.1</b> Disaggregation of Naomi prevalence estimates</a></li>
<li class="chapter" data-level="5.4.2" data-path="multi-agyw.html"><a href="multi-agyw.html#disaggregation-of-naomi-incidence-estimates"><i class="fa fa-check"></i><b>5.4.2</b> Disaggregation of Naomi incidence estimates</a></li>
<li class="chapter" data-level="5.4.3" data-path="multi-agyw.html"><a href="multi-agyw.html#expected-new-infections-reached"><i class="fa fa-check"></i><b>5.4.3</b> Expected new infections reached</a></li>
<li class="chapter" data-level="5.4.4" data-path="multi-agyw.html"><a href="multi-agyw.html#results-1"><i class="fa fa-check"></i><b>5.4.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="multi-agyw.html"><a href="multi-agyw.html#discussion-1"><i class="fa fa-check"></i><b>5.5</b> Discussion</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="multi-agyw.html"><a href="multi-agyw.html#limitations"><i class="fa fa-check"></i><b>5.5.1</b> Limitations</a></li>
<li class="chapter" data-level="5.5.2" data-path="multi-agyw.html"><a href="multi-agyw.html#conclusion"><i class="fa fa-check"></i><b>5.5.2</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="naomi-aghq.html"><a href="naomi-aghq.html"><i class="fa fa-check"></i><b>6</b> Fast approximate Bayesian inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#inference-methods-and-software"><i class="fa fa-check"></i><b>6.1</b> Inference methods and software</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#the-laplace-approximation"><i class="fa fa-check"></i><b>6.1.1</b> The Laplace approximation</a></li>
<li class="chapter" data-level="6.1.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#quadrature"><i class="fa fa-check"></i><b>6.1.2</b> Quadrature</a></li>
<li class="chapter" data-level="6.1.3" data-path="naomi-aghq.html"><a href="naomi-aghq.html#integrated-nested-laplace-approximation"><i class="fa fa-check"></i><b>6.1.3</b> Integrated nested Laplace approximation</a></li>
<li class="chapter" data-level="6.1.4" data-path="naomi-aghq.html"><a href="naomi-aghq.html#software"><i class="fa fa-check"></i><b>6.1.4</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#a-universal-inla-implementation-based-on-ad"><i class="fa fa-check"></i><b>6.2</b> A universal INLA implementation based on AD</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#epilepsy-example"><i class="fa fa-check"></i><b>6.2.1</b> Epilepsy example</a></li>
<li class="chapter" data-level="6.2.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#another-example"><i class="fa fa-check"></i><b>6.2.2</b> Another example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="naomi-aghq.html"><a href="naomi-aghq.html#naomi"><i class="fa fa-check"></i><b>6.3</b> The Naomi model</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#model-structure"><i class="fa fa-check"></i><b>6.3.1</b> Model structure</a></li>
<li class="chapter" data-level="6.3.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#connection-to-elgms"><i class="fa fa-check"></i><b>6.3.2</b> Connection to ELGMs</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="naomi-aghq.html"><a href="naomi-aghq.html#aghq-in-moderate-dimensions"><i class="fa fa-check"></i><b>6.4</b> AGHQ in moderate dimensions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#aghq-with-variable-levels"><i class="fa fa-check"></i><b>6.4.1</b> AGHQ with variable levels</a></li>
<li class="chapter" data-level="6.4.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#principal-components-analysis"><i class="fa fa-check"></i><b>6.4.2</b> Principal components analysis</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="naomi-aghq.html"><a href="naomi-aghq.html#malawi-case-study"><i class="fa fa-check"></i><b>6.5</b> Malawi case-study</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#nuts-convergence"><i class="fa fa-check"></i><b>6.5.1</b> NUTS convergence</a></li>
<li class="chapter" data-level="6.5.2" data-path="naomi-aghq.html"><a href="naomi-aghq.html#use-of-pca-aghq"><i class="fa fa-check"></i><b>6.5.2</b> Use of PCA-AGHQ</a></li>
<li class="chapter" data-level="6.5.3" data-path="naomi-aghq.html"><a href="naomi-aghq.html#model-assessment"><i class="fa fa-check"></i><b>6.5.3</b> Model assessment</a></li>
<li class="chapter" data-level="6.5.4" data-path="naomi-aghq.html"><a href="naomi-aghq.html#inference-comparison"><i class="fa fa-check"></i><b>6.5.4</b> Inference comparison</a></li>
<li class="chapter" data-level="6.5.5" data-path="naomi-aghq.html"><a href="naomi-aghq.html#exceedance-probabilities"><i class="fa fa-check"></i><b>6.5.5</b> Exceedance probabilities</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="naomi-aghq.html"><a href="naomi-aghq.html#discussion-2"><i class="fa fa-check"></i><b>6.6</b> Discussion</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="naomi-aghq.html"><a href="naomi-aghq.html#suggestions-for-future-work"><i class="fa fa-check"></i><b>6.6.1</b> Suggestions for future work</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>7</b> Future work and conclusions</a>
<ul>
<li class="chapter" data-level="7.1" data-path="conclusions.html"><a href="conclusions.html#strengths"><i class="fa fa-check"></i><b>7.1</b> Strengths</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="conclusions.html"><a href="conclusions.html#chapter-refbeyond-borders"><i class="fa fa-check"></i><b>7.1.1</b> Chapter @ref(beyond-borders)</a></li>
<li class="chapter" data-level="7.1.2" data-path="conclusions.html"><a href="conclusions.html#chapter-refmulti-agyw"><i class="fa fa-check"></i><b>7.1.2</b> Chapter @ref(multi-agyw)</a></li>
<li class="chapter" data-level="7.1.3" data-path="conclusions.html"><a href="conclusions.html#chapter-refnaomi-aghq"><i class="fa fa-check"></i><b>7.1.3</b> Chapter @ref(naomi-aghq)</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="conclusions.html"><a href="conclusions.html#future-work"><i class="fa fa-check"></i><b>7.2</b> Future work</a></li>
<li class="chapter" data-level="7.3" data-path="conclusions.html"><a href="conclusions.html#conclusions-1"><i class="fa fa-check"></i><b>7.3</b> Conclusions</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="models-for-spatial-structure.html"><a href="models-for-spatial-structure.html"><i class="fa fa-check"></i><b>A</b> Models for spatial structure</a></li>
<li class="chapter" data-level="B" data-path="a-model-for-risk-group-proportions.html"><a href="a-model-for-risk-group-proportions.html"><i class="fa fa-check"></i><b>B</b> A model for risk group proportions</a>
<ul>
<li class="chapter" data-level="B.1" data-path="a-model-for-risk-group-proportions.html"><a href="a-model-for-risk-group-proportions.html#the-global-aids-strategy"><i class="fa fa-check"></i><b>B.1</b> The Global AIDS Strategy</a></li>
<li class="chapter" data-level="B.2" data-path="a-model-for-risk-group-proportions.html"><a href="a-model-for-risk-group-proportions.html#household-survey-data"><i class="fa fa-check"></i><b>B.2</b> Household survey data</a></li>
<li class="chapter" data-level="B.3" data-path="a-model-for-risk-group-proportions.html"><a href="a-model-for-risk-group-proportions.html#spatial-analysis-levels"><i class="fa fa-check"></i><b>B.3</b> Spatial analysis levels</a></li>
<li class="chapter" data-level="B.4" data-path="a-model-for-risk-group-proportions.html"><a href="a-model-for-risk-group-proportions.html#survey-questions"><i class="fa fa-check"></i><b>B.4</b> Survey questions and risk group allocation</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html"><i class="fa fa-check"></i><b>C</b> Fast approximate Bayesian inference</a>
<ul>
<li class="chapter" data-level="C.1" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#epilepsy-example-1"><i class="fa fa-check"></i><b>C.1</b> Epilepsy example</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#tmb-epil"><i class="fa fa-check"></i><b>C.1.1</b> TMB C++ template</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#simplified-naomi-model-description"><i class="fa fa-check"></i><b>C.2</b> Simplified Naomi model description</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#process-specification"><i class="fa fa-check"></i><b>C.2.1</b> Process specification</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#model-assessment-1"><i class="fa fa-check"></i><b>C.3</b> Model assessment</a></li>
<li class="chapter" data-level="C.4" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#aghq-and-pca-aghq-details"><i class="fa fa-check"></i><b>C.4</b> AGHQ and PCA-AGHQ details</a></li>
<li class="chapter" data-level="C.5" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#normalising-constant-estimation"><i class="fa fa-check"></i><b>C.5</b> Normalising constant estimation</a></li>
<li class="chapter" data-level="C.6" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#inference-comparison-1"><i class="fa fa-check"></i><b>C.6</b> Inference comparison</a></li>
<li class="chapter" data-level="C.7" data-path="fast-approximate-bayesian-inference.html"><a href="fast-approximate-bayesian-inference.html#mcmc-convergence-and-suitability"><i class="fa fa-check"></i><b>C.7</b> MCMC convergence and suitability</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><p>Bayesian spatio-temporal methods for small-area estimation of HIV indicators</p></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="naomi-aghq" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Fast approximate Bayesian inference<a href="naomi-aghq.html#naomi-aghq" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!-- For PDF output, include these two LaTeX commands after unnumbered chapter headings, otherwise the mini table of contents and the running header will show the previous chapter -->
<p>This chapter describes a novel Bayesian inference method.
Development of the method was motivated the Naomi small-area estimation model <span class="citation">(<a href="#ref-eaton2021naomi">Eaton et al. 2021</a>)</span>.
Over 35 countries <span class="citation">(<a href="#ref-unaids2023global">UNAIDS 2023b</a>)</span> have used the Naomi model software (<a href="https://naomi.unaids.org"><code>https://naomi.unaids.org</code></a>) to produce subnational estimates of HIV indicators by synthesising evidence from multiple data sources.
The complexity and size of the model makes obtaining fast and accurate Bayesian inferences challenging.
As such, inferences have previously been obtained using an empirical Bayes approximation to full Bayesian inference, where hyperparameters are obtained by optimisation of the marginal posterior <span class="citation">(<a href="#ref-casella1985introduction">Casella 1985</a>)</span>.
This approximation is undesirable as it can result in underestimation of uncertainty, ultimately leading to worse policy decisions stemming from overconfidence.
Instead, I propose integrating over the hyperparameters using an modification of adaptive Gauss-Hermite quadrature <span class="citation">(<a href="#ref-naylor1982applications">Naylor and Smith 1982</a>)</span> for moderate dimensions.</p>
<p>The methods developed in this chapter combine Laplace approximations with adaptive quadrature, and are descended from the integrated nested Laplace approximation (INLA) method pioneered by <span class="citation">Håvard Rue, Martino, and Chopin (<a href="#ref-rue2009approximate">2009</a>)</span>.
The INLA method has enabled fast and accurate Bayesian inferences for a vast array of models, across a large number of scientific fields <span class="citation">(<a href="#ref-rue2017bayesian">Håvard Rue et al. 2017</a>)</span>.
The success of INLA is in large part due to its accessible implementation in the <code>R-INLA</code> software.
Most applications and methodological developments of the INLA method have used <code>R-INLA</code>.
Exceptions include the simplified INLA approach of <span class="citation">Wood (<a href="#ref-wood2020simplified">2020</a>)</span>, implemented in the <code>mgcv</code> R package, and <span class="citation">Stringer, Brown, and Stafford (<a href="#ref-stringer2022fast">2022</a>)</span>.
In pursuit of a suitable inference approach for the Naomi model, I implemented an INLA-like method outside <code>R-INLA</code>, building upon <span class="citation">Stringer, Brown, and Stafford (<a href="#ref-stringer2022fast">2022</a>)</span>.
My implementation is compatible with a wider range of models than <code>R-INLA</code>, and uses automatic differentiation to obtain the derivatives required for the Laplace approximation.</p>
<p>Though I began working on this project close to the start of my PhD, I only began making meaningful progress after reading <span class="citation">Stringer, Brown, and Stafford (<a href="#ref-stringer2022fast">2022</a>)</span>.
I am grateful to later have had the opportunity to collaborate with Alex Stringer, and work closely together during my visit to the University of Waterloo in the fall term of 2022.
The results of our work are presented in <span class="citation">Howes et al. (<a href="#ref-howes2023fast">2023+</a>)</span>.
Code for the analysis in this chapter is available from <a href="https://github.com/athowes/elgm-inf"><code>https://github.com/athowes/naomi-aghq</code></a>.</p>
<div id="inference-methods-and-software" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Inference methods and software<a href="naomi-aghq.html#inference-methods-and-software" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In a Bayesian analysis, the primary goal is to perform inference.
That is, to obtain the posterior distribution
<span class="math display">\[\begin{equation}
p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) = \frac{p(\boldsymbol{\mathbf{\phi}}, \mathbf{y})}{p(\mathbf{y})},
\end{equation}\]</span>
or some way to compute relevant functions of it.
The posterior distribution encapsulates beliefs about the parameters <span class="math inline">\(\boldsymbol{\mathbf{\phi}} = (\phi_1, \ldots, \phi_d)\)</span> having observed data <span class="math inline">\(\mathbf{y} = (y_1, \ldots, y_n)\)</span>, which I assume to be vectors.</p>
<p>Inference is a sensible goal because the posterior distribution is sufficient for use in decision making.
Given a loss function <span class="math inline">\(l(a, \boldsymbol{\mathbf{\phi}})\)</span> the expected posterior loss of a decision <span class="math inline">\(a\)</span> depends on the data only via the posterior distribution
<span class="math display">\[\begin{equation}
\mathbb{E}(l(a, \boldsymbol{\mathbf{\phi}}) \, | \, \mathbf{y}) = \int_{\mathbb{R}^d} l(a, \boldsymbol{\mathbf{\phi}}) p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) \text{d}\boldsymbol{\mathbf{\phi}}.
\end{equation}\]</span>
For example, given the posterior distribution of the current demand for HIV treatment at a particular facility, historic data about treatment demand are not required for planning of service provision.</p>
<p>It is usually intractable to straightforwardly obtain the posterior distribution.
This is because the denominator contains a potentially high-dimensional integral, depending on the value of <span class="math inline">\(d \in \mathbb{Z}^+\)</span>, over the parameters
<span class="math display" id="eq:evidence">\[\begin{equation}
p(\mathbf{y}) = \int_{\mathbb{R}^d} p(\mathbf{y}, \boldsymbol{\mathbf{\phi}}) \text{d}\boldsymbol{\mathbf{\phi}}, \tag{6.1}
\end{equation}\]</span>
sometimes called the evidence or posterior normalising constant.
For this reason, approximations to the posterior distribution <span class="math inline">\(\tilde p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y})\)</span> are typically used in place of the exact posterior distribution.</p>
<p>Some approximate Bayesian inference methods, like Markov chain Monte Carlo (MCMC), avoid directly calculating the posterior normalising constant, instead working with the unnormalised posterior distribution
<span class="math display">\[\begin{equation}
p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) \propto p(\boldsymbol{\mathbf{\phi}}, \mathbf{y}),
\end{equation}\]</span>
where <span class="math inline">\(p(\mathbf{y})\)</span> is not a function of <span class="math inline">\(\boldsymbol{\mathbf{\phi}}\)</span> and so can be removed as a constant.
Other approximate Bayesian inference methods can more directly be thought of as ways to estimate the posterior normalising constant.
The methods in this chapter fall into this latter category, sometimes described as deterministic Bayesian inference methods.
This section provides background on these methods and the software used to implement them.</p>
<div id="the-laplace-approximation" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> The Laplace approximation<a href="naomi-aghq.html#the-laplace-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Laplace’s method <span class="citation">(<a href="#ref-laplace1774memoire">Laplace 1774</a>)</span> is a technique used to approximate integrals of the form
<span class="math display">\[\begin{equation}
\int \exp(C h(\mathbf{z})) \text{d}\mathbf{z},
\end{equation}\]</span>
where <span class="math inline">\(C &gt; 0\)</span> is a constant, <span class="math inline">\(h\)</span> is a function which is twice-differentiable, and <span class="math inline">\(\mathbf{z}\)</span> are generic variables.
The Laplace approximation <span class="citation">(<a href="#ref-tierney1986accurate">Tierney and Kadane 1986</a>)</span> is obtained by application of Laplace’s method to calculate the posterior normalising constant (Equation <a href="naomi-aghq.html#eq:evidence">(6.1)</a>).
Let <span class="math inline">\(h(\boldsymbol{\mathbf{\phi}}) = \log p(\boldsymbol{\mathbf{\phi}}, \mathbf{y})\)</span> such that
<span class="math display">\[\begin{equation}
p(\mathbf{y}) = \int_{\mathbb{R}^d} p(\mathbf{y}, \boldsymbol{\mathbf{\phi}}) \text{d}\boldsymbol{\mathbf{\phi}} = \int_{\mathbb{R}^d} \exp(h(\boldsymbol{\mathbf{\phi}})) \text{d}\boldsymbol{\mathbf{\phi}}.
\end{equation}\]</span>
Laplace’s method involves approximating the function <span class="math inline">\(h\)</span> by its second order Taylor expansion.
This expansion is then evaluated at a maxima of <span class="math inline">\(h\)</span> to eliminate the first order term.
Let
<span class="math display" id="eq:posterior-mode">\[\begin{equation}
\hat{\boldsymbol{\mathbf{\phi}}} = \arg\max_{\boldsymbol{\mathbf{\phi}}} h(\boldsymbol{\mathbf{\phi}}) \tag{6.2}
\end{equation}\]</span>
be the posterior mode, and
<span class="math display" id="eq:hessian">\[\begin{equation}
\hat {\mathbf{H}} = - \frac{\partial^2}{\partial \boldsymbol{\mathbf{\phi}} \partial \boldsymbol{\mathbf{\phi}}^\top} h(\boldsymbol{\mathbf{\phi}}) \rvert_{\boldsymbol{\mathbf{\phi}} = \hat{\boldsymbol{\mathbf{\phi}}}} \tag{6.3}
\end{equation}\]</span>
be the Hessian matrix evaluated at the posterior mode.
The Laplace approximation is then
<span class="math display" id="eq:la2" id="eq:la">\[\begin{align}
\tilde p_{\texttt{LA}}(\mathbf{y}) &amp;= \int_{\mathbb{R}^d} \exp \left( h(\hat{\boldsymbol{\mathbf{\phi}}}) - \frac{1}{2} (\boldsymbol{\mathbf{\phi}} - \hat{\boldsymbol{\mathbf{\phi}}})^\top \hat {\mathbf{H}} (\boldsymbol{\mathbf{\phi}} - \hat{\boldsymbol{\mathbf{\phi}}}) \right) \text{d}\boldsymbol{\mathbf{\phi}} \tag{6.4} \\
&amp;= p(\hat{\boldsymbol{\mathbf{\phi}}}, \mathbf{y}) \cdot \frac{(2 \pi)^{d/2}}{| \hat {\mathbf{H}} |^{1/2}}. \tag{6.5}
\end{align}\]</span>
The result above is calculated using the known normalising constant of the Gaussian distribution
<span class="math display">\[\begin{equation}
p_\texttt{G}(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) = \mathcal{N}(\boldsymbol{\mathbf{\phi}} \, | \, \hat{\boldsymbol{\mathbf{\phi}}}, \hat {\mathbf{H}}^{-1}) = \frac{| \hat {\mathbf{H}} |^{1/2}}{(2 \pi)^{d/2}} \exp \left( - \frac{1}{2} (\boldsymbol{\mathbf{\phi}} - \hat{\boldsymbol{\mathbf{\phi}}})^\top \hat {\mathbf{H}} (\boldsymbol{\mathbf{\phi}} - \hat{\boldsymbol{\mathbf{\phi}}}) \right).
\end{equation}\]</span>
The Laplace approximation may be thought of as approximating the posterior distribution by a Gaussian distribution <span class="math inline">\(p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) \approx p_\texttt{G}(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y})\)</span> such that
<span class="math display">\[\begin{equation}
\tilde p_{\texttt{LA}}(\mathbf{y}) = \frac{p(\boldsymbol{\mathbf{\phi}}, \mathbf{y})}{p_\texttt{G}(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y})} \Big\rvert_{\boldsymbol{\mathbf{\phi}} = \hat{\boldsymbol{\mathbf{\phi}}}}.
\end{equation}\]</span></p>
<p>Calculation of the Laplace approximation requires obtaining the second derivative of <span class="math inline">\(h\)</span> with respect to <span class="math inline">\(\boldsymbol{\mathbf{\phi}}\)</span> (Equation <a href="naomi-aghq.html#eq:hessian">(6.3)</a>).
Derivatives may also be used to improve the performance of the optimisation algorithm used to obtain the maxima of <span class="math inline">\(h\)</span> (Equation <a href="naomi-aghq.html#eq:posterior-mode">(6.2)</a>) by providing access to the gradient of <span class="math inline">\(h\)</span> with respect to <span class="math inline">\(\boldsymbol{\mathbf{\phi}}\)</span>.</p>

<div class="figure" style="text-align: centre"><span style="display:block;" id="fig:laplace"></span>
<img src="figures/naomi-aghq/laplace.png" alt="Demonstration of the Laplace approximation for the simple Bayesian inference example of Figure 3.1. The unnormalised posterior is \(p(\phi, \mathbf{y}) = \phi^8 \exp(-4 \phi)\), and can be recognised as the unnormalised gamma distribution \(\text{Gamma}(9, 4)\). The true log normalising constant is \(\log p(\mathbf{y}) = \log\Gamma(9) - 9 \log(4) = -1.872046\), whereas the Laplace approximate log normalising constant is \(\log \tilde p_{\texttt{LA}}(\mathbf{y}) = -1.882458\), resulting from the Gaussian approximation \(p_\texttt{G}(\phi \, | \, \mathbf{y}) = \mathcal{N}(\phi \, | \,\mu = 2, \tau = 2)\)." width="95%" />
<p class="caption">
Figure 6.1: Demonstration of the Laplace approximation for the simple Bayesian inference example of Figure <a href="bayes-st.html#fig:conjugate">3.1</a>. The unnormalised posterior is <span class="math inline">\(p(\phi, \mathbf{y}) = \phi^8 \exp(-4 \phi)\)</span>, and can be recognised as the unnormalised gamma distribution <span class="math inline">\(\text{Gamma}(9, 4)\)</span>. The true log normalising constant is <span class="math inline">\(\log p(\mathbf{y}) = \log\Gamma(9) - 9 \log(4) = -1.872046\)</span>, whereas the Laplace approximate log normalising constant is <span class="math inline">\(\log \tilde p_{\texttt{LA}}(\mathbf{y}) = -1.882458\)</span>, resulting from the Gaussian approximation <span class="math inline">\(p_\texttt{G}(\phi \, | \, \mathbf{y}) = \mathcal{N}(\phi \, | \,\mu = 2, \tau = 2)\)</span>.
</p>
</div>
<div id="marginal-la" class="section level4 hasAnchor" number="6.1.1.1">
<h4><span class="header-section-number">6.1.1.1</span> The marginal Laplace approximation<a href="naomi-aghq.html#marginal-la" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Approximating the full joint posterior distribution using a Gaussian distribution may be inaccurate.
An alternative approach is to approximate the marginal posterior distribution of some subset of the parameters.
The remaining non-Gaussian parameters may then be handled using another, more appropriate, method.</p>
<p>Let <span class="math inline">\(\boldsymbol{\mathbf{\phi}} = (\mathbf{x}, \boldsymbol{\mathbf{\theta}})\)</span> and consider a three-stage hierarchical model
<span class="math display">\[\begin{equation}
p(\mathbf{y}, \mathbf{x}, \boldsymbol{\mathbf{\theta}}) = p(\mathbf{y} \, | \, \mathbf{x}, \boldsymbol{\mathbf{\theta}}) p(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}) p(\boldsymbol{\mathbf{\theta}}),
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{x} = (x_1, \ldots, x_N)\)</span> is the latent field, and <span class="math inline">\(\boldsymbol{\mathbf{\theta}} = (\theta_1, \ldots, \theta_m)\)</span> are the hyperparameters.
Applying a Gaussian approximation to the latent field, we have <span class="math inline">\(h(\mathbf{x}, \boldsymbol{\mathbf{\theta}}) = \log p(\mathbf{y}, \mathbf{x}, \boldsymbol{\mathbf{\theta}})\)</span> with <span class="math inline">\(N\)</span>-dimensional posterior mode
<span class="math display" id="eq:marginal-posterior-mode">\[\begin{equation}
\hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}}) = \arg\max_{\mathbf{x}} h(\mathbf{x}, \boldsymbol{\mathbf{\theta}}) \tag{6.6}
\end{equation}\]</span>
and <span class="math inline">\(N \times N\)</span>-dimensional Hessian matrix evaluated at the posterior mode
<span class="math display" id="eq:marginal-hessian">\[\begin{equation}
\hat {\mathbf{H}}(\boldsymbol{\mathbf{\theta}}) = - \frac{\partial^2}{\partial \mathbf{x} \partial \mathbf{x}^\top} h(\mathbf{x}, \boldsymbol{\mathbf{\theta}}) \rvert_{\mathbf{x} = \hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}})}. \tag{6.7}
\end{equation}\]</span>
Dependence on the hyperparameters <span class="math inline">\(\boldsymbol{\mathbf{\theta}}\)</span> is made explicit in both Equation <a href="naomi-aghq.html#eq:marginal-posterior-mode">(6.6)</a> and <a href="naomi-aghq.html#eq:marginal-hessian">(6.7)</a>.
In other words, there is a a Gaussian approximation to the marginal posterior of the latent field <span class="math inline">\(\tilde p_\texttt{G}(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}, \mathbf{y}) = \mathcal{N}(\mathbf{x} \, | \, \hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}}), \hat{\mathbf{H}}(\boldsymbol{\mathbf{\theta}})^{-1})\)</span> at each value <span class="math inline">\(\boldsymbol{\mathbf{\theta}}\)</span> in the space <span class="math inline">\(\mathbb{R}^m\)</span>.
The resulting marginal Laplace approximation, for a particular value of the hyperparameters, is then
<span class="math display" id="eq:marginalla">\[\begin{align}
\tilde p_{\texttt{LA}}(\boldsymbol{\mathbf{\theta}}, \mathbf{y}) &amp;= \int_{\mathbb{R}^N} \exp \left( h(\hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}}), \boldsymbol{\mathbf{\theta}}) - \frac{1}{2} (\mathbf{x} - \hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}}))^\top \hat {\mathbf{H}}(\boldsymbol{\mathbf{\theta}}) (\mathbf{x} - \hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}})) \right) \text{d}\mathbf{x} \tag{6.8} \\
&amp;= \exp(h(\hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}}), \mathbf{y})) \cdot \frac{(2 \pi)^{d/2}}{| \hat {\mathbf{H}}(\boldsymbol{\mathbf{\theta}}) |^{1/2}} \\
&amp;= \frac{p(\mathbf{y}, \mathbf{x}, \boldsymbol{\mathbf{\theta}})}{\tilde p_\texttt{G}(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}, \mathbf{y})} \Big\rvert_{\mathbf{x} = \hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}})}.
\end{align}\]</span></p>
<p>The marginal Laplace approximation is most accurate when the marginal posterior <span class="math inline">\(p(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}, \mathbf{y})\)</span> is accurately approximated by a Gaussian
distribution.
For the class of latent Gaussian models <span class="citation">(<a href="#ref-rue2009approximate">Håvard Rue, Martino, and Chopin 2009</a>)</span> the prior distribution on the latent field is Gaussian <span class="math inline">\(\mathbf{x} \sim \mathcal{N}(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}})\)</span>.
Although the marginal posterior distribution
<span class="math display">\[\begin{align}
p(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}, \mathbf{y}) \propto \mathcal{N}(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}) p(\mathbf{y}  \, | \, \mathbf{x}, \boldsymbol{\mathbf{\theta}})
\end{align}\]</span>
is not Gaussian, its deviation from Gaussianity can be expected to be small if <span class="math inline">\(\log p(\mathbf{y} \, | \, \mathbf{x}, \boldsymbol{\mathbf{\theta}})\)</span> satisfies some condition which remains to be detailed.</p>
</div>
</div>
<div id="quadrature" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Quadrature<a href="naomi-aghq.html#quadrature" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Quadrature is a method used to approximate integrals using a weighted sum of function evaluations.
As with the Laplace approximation, it is deterministic in that the computational procedure is not intrinsically random.
Let <span class="math inline">\(\mathcal{Q}\)</span> be a set of quadrature nodes <span class="math inline">\(\mathbf{z} \in \mathcal{Q}\)</span> and <span class="math inline">\(\omega: \mathbb{R}^d \to \mathbb{R}\)</span> be a weighting function.
Then, quadrature can be used to estimate the posterior normalising constant (Equation <a href="naomi-aghq.html#eq:evidence">(6.1)</a>) by
<span class="math display">\[\begin{equation}
\tilde p_{\mathcal{Q}}(\mathbf{y}) = \sum_{\mathbf{z} \in \mathcal{Q}} p(\mathbf{y}, \mathbf{z}) \omega(\mathbf{z}).
\end{equation}\]</span></p>
<p>To illustrate quadrature for a simple example, consider integrating the univariate function <span class="math inline">\(f(z) = z \sin(z)\)</span> between <span class="math inline">\(z = 0\)</span> and <span class="math inline">\(z = \pi\)</span>.
This integral is known analytically to be <span class="math inline">\(\pi\)</span>.
A quadrature approximation of this integral is
<span class="math display" id="eq:zsinz">\[\begin{equation}
\pi = \int_{0}^\pi z \sin(z) \text{d} z \approx \sum_{z \in \mathcal{Q}} z \sin(z) \omega(z), \tag{6.9}
\end{equation}\]</span>
where <span class="math inline">\(\mathcal{Q} = \{z_1, \ldots z_k\}\)</span> are a set of <span class="math inline">\(k\)</span> quadrature nodes and <span class="math inline">\(\omega: \mathbb{R} \to \mathbb{R}\)</span>.</p>
<p>The trapezoid rule is an example of a quadrature rule, in which quadrature nodes are spaced throughout the domain according to <span class="math inline">\(\epsilon_i = z_i - z_{i - 1} &gt; 0\)</span> for <span class="math inline">\(1 &lt; i &lt; k\)</span>.
The weighting function is <span class="math inline">\(\omega(z_i) = \epsilon_i\)</span> for <span class="math inline">\(1 &lt; i &lt; k\)</span> and <span class="math inline">\(\omega(z_i) = \epsilon_i / 2\)</span> for <span class="math inline">\(i \in \{1, k\}\)</span>.
Figure <a href="naomi-aghq.html#fig:trapezoid">6.2</a> shows application of the trapezoid rule to Equation <a href="naomi-aghq.html#eq:zsinz">(6.9)</a>.
The more quadrature nodes are used, the more accurate the estimate of the integrand is.
Under some regularity conditions on <span class="math inline">\(f\)</span>, as <span class="math inline">\(\epsilon \to 0\)</span> the quadrature estimate obtained using the trapezoid rule converges to the true value of the integral.
Indeed, this approach was used by Riemann to provide the first rigorous definition of the integral.</p>

<div class="figure" style="text-align: centre"><span style="display:block;" id="fig:trapezoid"></span>
<img src="figures/naomi-aghq/trapezoid.png" alt="The trapezoid rule with \(k = 5, 25, 125\) equally-spaced (\(\epsilon_i = \epsilon &gt; 0\)) quadrature nodes can be used to integrate the function \(f(z) = z \sin(z)\) in the domain \([0, \pi]\). Here, the exact solution is \(\pi \approx 3.1416\). As \(k\) increases and more nodes are used in the computation, the quadrature estimate becomes closer to the exact solution." width="95%" />
<p class="caption">
Figure 6.2: The trapezoid rule with <span class="math inline">\(k = 5, 25, 125\)</span> equally-spaced (<span class="math inline">\(\epsilon_i = \epsilon &gt; 0\)</span>) quadrature nodes can be used to integrate the function <span class="math inline">\(f(z) = z \sin(z)\)</span> in the domain <span class="math inline">\([0, \pi]\)</span>. Here, the exact solution is <span class="math inline">\(\pi \approx 3.1416\)</span>. As <span class="math inline">\(k\)</span> increases and more nodes are used in the computation, the quadrature estimate becomes closer to the exact solution.
</p>
</div>
<p>Quadrature methods are most effective when integrating over small dimensions.
This is because the number of quadrature nodes required to be evaluated in the computation grows exponentially with the dimension.
For even moderate dimension, this quickly becomes intractable.
For example, using 5, 25, or 125 quadrature nodes per dimension, as in Figure <a href="naomi-aghq.html#fig:trapezoid">6.2</a>, in three-dimensions (rather than one) would require 125, 15625 or 1953125 quadrature nodes respectively.
Though quadrature is embarrassingly parallel, solutions requiring the evaluation of upwards of millions of quadrature nodes are unlikely to be tractable.</p>
<div id="gauss-hermite-quadrature" class="section level4 hasAnchor" number="6.1.2.1">
<h4><span class="header-section-number">6.1.2.1</span> Gauss-Hermite quadrature<a href="naomi-aghq.html#gauss-hermite-quadrature" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Gauss-Hermite quadrature [GHQ; <span class="citation">Davis and Rabinowitz (<a href="#ref-davis1975methods">1975</a>)</span>] is a quadrature rule designed to integrate functions of the form <span class="math inline">\(f(\mathbf{z}) = \varphi(\mathbf{z}) P_\alpha(\mathbf{z})\)</span> exactly such that
<span class="math display" id="eq:ghqexact">\[\begin{equation}
\int \varphi(\mathbf{z}) P_\alpha(\mathbf{z}) \text{d} \mathbf{z} = \sum_{\mathbf{z} \in \mathcal{Q}}  \varphi(\mathbf{z}) P_\alpha(\mathbf{z}) \omega(\mathbf{z}). \tag{6.10}
\end{equation}\]</span>
The term <span class="math inline">\(\varphi(\cdot)\)</span> is a standard multivariate normal density <span class="math inline">\(\mathcal{N}(\cdot \, | \, \mathbf{0}, \mathbf{I})\)</span>, where <span class="math inline">\(\mathbf{0}\)</span> is the zero-vector and <span class="math inline">\(\mathbf{I}\)</span> is the identify matrix of relevant dimension, and the term <span class="math inline">\(P_\alpha(\cdot)\)</span> is a polynomial with highest degree monomial <span class="math inline">\(\alpha \leq 2k - 1\)</span>, where <span class="math inline">\(k\)</span> is the number of quadrature nodes per dimension.
GHQ is attractive for Bayesian inference problems because posterior distributions are typically well approximated by functions of this form.
This statement requires support using the Bernstein–von Mises theorem in combination with other arguments.</p>
<p>I follow the notation for GHQ established by <span class="citation">Bilodeau, Stringer, and Tang (<a href="#ref-bilodeau2022stochastic">2022</a>)</span>.
First, to construct the univariate GHQ rule for <span class="math inline">\(z \in \mathbb{R}\)</span>, let <span class="math inline">\(H_k(z)\)</span> be the <span class="math inline">\(k\)</span>th (probabilist’s) Hermite polynomial
<span class="math display">\[\begin{align}
H_k(z) &amp;= (-1)^k \exp(z^2 / 2) \frac{\text{d}}{\text{d}z^k} \exp(-z^2 / 2)
&amp;= (-1)^k \exp(z^2 / 2) \frac{\text{d}}{\text{d}z^k} \exp(-z^2 / 2)
\end{align}\]</span>
The Hermite polynomials are defined to be orthogonal with respect to the standard Gaussian probability density function
<span class="math display">\[\begin{equation}
\int H_k(z) H_l(z) \varphi(z) \text{d} z = \delta_{kl},
\end{equation}\]</span>
where <span class="math inline">\(\delta_{kl} = 1\)</span> if <span class="math inline">\(k = l\)</span> and <span class="math inline">\(\delta_{kl} = 0\)</span> otherwise.
The relevance of the above equation is that…
The GHQ nodes <span class="math inline">\(z \in \mathcal{Q}(1, k)\)</span> are given by the <span class="math inline">\(k\)</span> zeroes of the <span class="math inline">\(k\)</span>th Hermite polynomial.
For <span class="math inline">\(k = 1, 2, 3\)</span> these zeros, up to three decimal places, are
<span class="math display">\[\begin{align}
H_1(z) = z = 0 \implies \mathcal{Q}(1, 1) &amp;= \{0\}, \\
H_2(z) = z^2 - 1 = 0 \implies \mathcal{Q}(1, 2) &amp;= \{-0.707, 0.707\}, \\
H_3(z) = z^3 - 3z = 0 \implies \mathcal{Q}(1, 3) &amp;= \{-1.225, 0, 1.225\}.
\end{align}\]</span>
The corresponding weighting function <span class="math inline">\(\omega: \mathcal{Q}(1, k) \to \mathbb{R}\)</span> chosen to satisfy Equation <a href="naomi-aghq.html#eq:ghqexact">(6.10)</a> is given by
<span class="math display">\[\begin{equation}
\omega(z) = \frac{k!}{\varphi(z) \cdot [H_{k + 1}(z)]^2}.
\end{equation}\]</span></p>
<p>Multivariate GHQ rules are usually constructed using the product rule with identical univariate GHQ rules in each dimension.
In <span class="math inline">\(d\)</span> dimensions, the multivariate GHQ nodes <span class="math inline">\(\mathbf{z} \in \mathcal{Q}(d, k)\)</span> are defined by
<span class="math display">\[\begin{equation}
\mathcal{Q}(d, k) = \mathcal{Q}(1, k)^d = \mathcal{Q}(1, k) \times \cdots \times \mathcal{Q}(1, k).
\end{equation}\]</span>
The corresponding weighting function <span class="math inline">\(\omega: \mathcal{Q}(d, k) \to \mathbb{R}\)</span> is given by <span class="math inline">\(\omega(\mathbf{z}) = \prod_{j = 1}^d \omega(z_j)\)</span>.</p>
</div>
<div id="adaptive-quadrature" class="section level4 hasAnchor" number="6.1.2.2">
<h4><span class="header-section-number">6.1.2.2</span> Adaptive quadrature<a href="naomi-aghq.html#adaptive-quadrature" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In adaptive quadrature, the quadrature nodes and weights depend on the specific integrand being considered.
Using an adaptive quadrature rules is particularly important for Bayesian inference problems because the posterior normalising constant <span class="math inline">\(p(\mathbf{y})\)</span> is a function of the data.
No fixed quadrature rule can be expected to effectively integrate all possible posterior distributions produced by observation of certain data <span class="math inline">\(\mathbf{y}\)</span>.
For example, effective use of the trapezoid rule requires good choices for the start and end point, and spacing between quadrature nodes.</p>
<p>In adaptive GHQ [AGHQ; <span class="citation">Naylor and Smith (<a href="#ref-naylor1982applications">1982</a>)</span>] the quadrature nodes are shifted by the mode of the integrand, and rotated based on a matrix decomposition of the inverse curvature at the mode.
Consider application of AGHQ to calculation of the posterior normalising constant.
The transformation of the GHQ nodes <span class="math inline">\(\mathcal{Q}(d, k)\)</span> is then
<span class="math display">\[\begin{equation}
\boldsymbol{\mathbf{\phi}}(\mathbf{z}) = \hat{\mathbf{P}} \mathbf{z} + \hat{\boldsymbol{\mathbf{\phi}}},
\end{equation}\]</span>
where <span class="math inline">\(\hat{\mathbf{P}}\)</span> is a matrix decomposition of <span class="math inline">\(\hat{\boldsymbol{\mathbf{H}}}^{-1} = \hat{\mathbf{P}} \hat{\mathbf{P}}^\top\)</span>.
The resulting adaptive quadrature estimate of the posterior normalising constant is
<span class="math display">\[\begin{equation}
\tilde p_{\texttt{AQ}}(\mathbf{y}) = | \hat{\mathbf{P}} | \sum_{\mathbf{z} \in \mathcal{Q}} p(\mathbf{y}, \boldsymbol{\mathbf{\phi}}(\mathbf{z})) \omega(\mathbf{z}) =  | \hat{\mathbf{P}} | \sum_{\mathbf{z} \in \mathcal{Q}} p(\mathbf{y}, \hat{\mathbf{P}} \mathbf{z} + \hat{\boldsymbol{\mathbf{\phi}}}) \omega(\mathbf{z}).
\end{equation}\]</span></p>
<p>The quantities <span class="math inline">\(\hat{\boldsymbol{\mathbf{\phi}}}\)</span> and <span class="math inline">\(\hat{\boldsymbol{\mathbf{H}}}\)</span> are exactly those given in Equations <a href="naomi-aghq.html#eq:posterior-mode">(6.2)</a> and <a href="naomi-aghq.html#eq:hessian">(6.3)</a> and used in the Laplace approximation.
Indeed, when <span class="math inline">\(k = 1\)</span> then AGHQ corresponds exactly to the Laplace approximation.
To see this, we have <span class="math inline">\(H_1(z)\)</span> with zero <span class="math inline">\(z = 0\)</span> such that the adapted node is given by the mode <span class="math inline">\(\boldsymbol{\mathbf{\phi}}(\mathbf{z} = \mathbf{0}) = \hat{\boldsymbol{\mathbf{\phi}}}\)</span>.
The weighting function is given by
<span class="math display">\[\begin{equation}
\omega(0)^d = \left( \frac{1!}{\varphi(0) \cdot H_{2}(0)^2} \right)^d = \left( \frac{1}{\varphi(0)} \right)^d = \left(2 \pi\right)^{d / 2}.
\end{equation}\]</span>
The AGHQ estimate of the normalising constant for <span class="math inline">\(k = 1\)</span> is given by
<span class="math display">\[\begin{equation}
\tilde p_{\texttt{AQ}}(\mathbf{y}) = p(\mathbf{y}, \hat{\boldsymbol{\mathbf{\phi}}}) \cdot | \hat{\mathbf{P}} | \cdot (2 \pi)^{d / 2} = p(\mathbf{y}, \hat{\boldsymbol{\mathbf{\phi}}}) \cdot \frac{(2 \pi)^{d / 2}}{| \hat{\mathbf{H}} | ^{1/2}},
\end{equation}\]</span>
and corresponds exactly to the Laplace approximation <span class="math inline">\(\tilde p_{\texttt{LA}}(\mathbf{y})\)</span> given in Equation <a href="naomi-aghq.html#eq:la2">(6.5)</a>.</p>

<div class="figure" style="text-align: centre"><span style="display:block;" id="fig:aghq-demo"></span>
<img src="figures/naomi-aghq/aghq-demo.png" alt="The Gauss-Hermite quadrature nodes \(\mathbf{z} \in \mathcal{Q}(2, 3)\) for a two-dimensional integral with three nodes per dimension (A). Adaption occurs based on the mode (B) and covariance of the integrand via either the Cholesky (C) or spectral (D) decomposition of the inverse curvature at the mode. Here, the integrand is \(f(z_1, z_2) = \text{sn}(0.5 z_1, \alpha = 2) \cdot \text{sn}(0.8 z_1 - 0.5 z_2, \alpha = -2)\), where \(\text{sn}(\cdot)\) is the standard skewnormal probability density function with shape parameter \(\alpha \in \mathbb{R}\). For each quadrature rule, the estimate of the integral is given by…" width="95%" />
<p class="caption">
Figure 6.3: The Gauss-Hermite quadrature nodes <span class="math inline">\(\mathbf{z} \in \mathcal{Q}(2, 3)\)</span> for a two-dimensional integral with three nodes per dimension (A). Adaption occurs based on the mode (B) and covariance of the integrand via either the Cholesky (C) or spectral (D) decomposition of the inverse curvature at the mode. Here, the integrand is <span class="math inline">\(f(z_1, z_2) = \text{sn}(0.5 z_1, \alpha = 2) \cdot \text{sn}(0.8 z_1 - 0.5 z_2, \alpha = -2)\)</span>, where <span class="math inline">\(\text{sn}(\cdot)\)</span> is the standard skewnormal probability density function with shape parameter <span class="math inline">\(\alpha \in \mathbb{R}\)</span>. For each quadrature rule, the estimate of the integral is given by…
</p>
</div>
<p>Two alternatives for the matrix decomposition <span class="math inline">\(\hat{\boldsymbol{\mathbf{H}}}^{-1} = \hat{\mathbf{P}} \hat{\mathbf{P}}^\top\)</span> are the Cholesky and spectral decomposition <span class="citation">(<a href="#ref-jackel2005note">Jäckel 2005</a>)</span>.
For the Cholesky decomposition <span class="math inline">\(\hat{\mathbf{P}} = \hat{\mathbf{L}}\)</span>, where <span class="math inline">\(\hat{\mathbf{L}}\)</span> is lower triangular.
For the spectral decomposition <span class="math inline">\(\hat{\mathbf{P}} = \hat{\mathbf{E}} \hat{\mathbf{\Lambda}}^{1/2}\)</span>, where <span class="math inline">\(\hat{\mathbf{E}} = (\hat{\mathbf{e}}_{1}, \ldots \hat{\mathbf{e}}_{ m})\)</span> contains the eigenvectors of <span class="math inline">\(\hat{\mathbf{H}}^{-1}\)</span> and <span class="math inline">\(\hat{\mathbf{\Lambda}}\)</span> is a diagonal matrix containing its eigenvalues <span class="math inline">\((\hat \lambda_{1}, \ldots, \hat \lambda_{m})\)</span>.
Figure <a href="naomi-aghq.html#fig:aghq-demo">6.3</a> demonstrates GHQ and AGHQ for a two-dimensional example, using both decomposition approaches.
Using the Cholesky decomposition results in adapted quadrature nodes which collapse along one of the dimensions, as a result of the matrix <span class="math inline">\(\hat{\mathbf{L}}\)</span> being lower triangular.
On the other hand, using the spectral decompositions results in adapted quadrature nodes which lie along the orthogonal eigenvectors of <span class="math inline">\(\hat{\mathbf{H}}^{-1}\)</span>.</p>
</div>
</div>
<div id="integrated-nested-laplace-approximation" class="section level3 hasAnchor" number="6.1.3">
<h3><span class="header-section-number">6.1.3</span> Integrated nested Laplace approximation<a href="naomi-aghq.html#integrated-nested-laplace-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The integrated nested Laplace approximation (INLA) method <span class="citation">(<a href="#ref-rue2009approximate">Håvard Rue, Martino, and Chopin 2009</a>)</span> combines marginal Laplace approximations with quadrature to enable approximation of posterior marginal distributions.
Consider the marginal Laplace approximation of Section <a href="naomi-aghq.html#marginal-la">6.1.1.1</a> for a three-stage hierarchical model given by
<span class="math display">\[\begin{equation}
\tilde p_{\texttt{LA}}(\boldsymbol{\mathbf{\theta}}, \mathbf{y}) = \frac{p(\mathbf{y}, \mathbf{x}, \boldsymbol{\mathbf{\theta}})}{\tilde p_\texttt{G}(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}, \mathbf{y})} \Big\rvert_{\mathbf{x} = \hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}})}.
\end{equation}\]</span>
To complete approximation of the posterior normalising constant, the marginal Laplace approximation can be integrated over the hyperparameters using a quadrature rule.
Following <span class="citation">Stringer, Brown, and Stafford (<a href="#ref-stringer2022fast">2022</a>)</span> I consider use of AGHQ as the particular quadrature rule.
Let <span class="math inline">\(\mathbf{z} \in \mathcal{Q}(m, k)\)</span> be the <span class="math inline">\(m\)</span>-dimensional GHQ nodes constructed using the product rule with <span class="math inline">\(k\)</span> nodes per dimension, and <span class="math inline">\(\omega: \mathbb{R}^m \to \mathbb{R}\)</span> the corresponding weighting function.
These nodes are adapted by <span class="math inline">\(\boldsymbol{\mathbf{\theta}}(\mathbf{z}) = \hat{\mathbf{P}}_\texttt{LA} \mathbf{z} + \hat{\boldsymbol{\mathbf{\theta}}}_\texttt{LA}\)</span> where
<span class="math display">\[\begin{align}
\hat{\boldsymbol{\mathbf{\theta}}}_\texttt{LA} &amp;= \arg\max_{\boldsymbol{\mathbf{\theta}}} \log \tilde p_{\texttt{LA}}(\boldsymbol{\mathbf{\theta}}, \mathbf{y}), \\
\hat{\boldsymbol{\mathbf{H}}}_\texttt{LA} &amp;= - \frac{\partial^2}{\partial \boldsymbol{\mathbf{\theta}} \partial \boldsymbol{\mathbf{\theta}}^\top} \log \tilde p_{\texttt{LA}}(\boldsymbol{\mathbf{\theta}}, \mathbf{y}) \rvert_{\boldsymbol{\mathbf{\theta}} = \hat{\boldsymbol{\mathbf{\theta}}}_\texttt{LA}}, \\
\hat{\boldsymbol{\mathbf{H}}}_\texttt{LA}^{-1} &amp;= \hat{\mathbf{P}}_\texttt{LA} \hat{\mathbf{P}}_\texttt{LA}^\top.
\end{align}\]</span>
The nested AGHQ estimate of the posterior normalising constant is then
<span class="math display" id="eq:aghqnormconst">\[\begin{equation}
\tilde p_{\texttt{AQ}}(\mathbf{y}) = | \hat{\mathbf{P}}_\texttt{LA} | \sum_{\mathbf{z} \in \mathcal{Q}(m, k)} \tilde p_\texttt{LA}(\boldsymbol{\mathbf{\theta}}(\mathbf{z}), \mathbf{y}) \omega(\mathbf{z}). \tag{6.11}
\end{equation}\]</span>
The normalised marginal Laplace approximation is
<span class="math display">\[\begin{equation}
\tilde p_{\texttt{LA}}(\boldsymbol{\mathbf{\theta}} \, | \, \mathbf{y}) = \frac{\tilde p_{\texttt{LA}}(\boldsymbol{\mathbf{\theta}}, \mathbf{y})}{\tilde p_{\texttt{AQ}}(\mathbf{y})}.
\end{equation}\]</span>
The posterior marginals <span class="math inline">\(\tilde p(\theta_j \, | \, \mathbf{y})\)</span> may be obtained by…</p>
<p>The following sections discuss methods for obtaining <span class="math inline">\(\tilde p(\mathbf{x} \, | \, \mathbf{y})\)</span>.</p>
<div id="gaussian-marginals" class="section level4 hasAnchor" number="6.1.3.1">
<h4><span class="header-section-number">6.1.3.1</span> Gaussian marginals<a href="naomi-aghq.html#gaussian-marginals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Inferences for the latent field can be obtained by approximation of <span class="math inline">\(p(\mathbf{x} \, | \, \mathbf{y})\)</span> using nested application of the quadrature rule <span class="citation">(<a href="#ref-rue2007approximate">Håvard Rue and Martino 2007</a>)</span>
<span class="math display" id="eq:mixgaussian">\[\begin{align}
p(\mathbf{x} \, | \, \mathbf{y}) &amp;= \int p(\mathbf{x}, \boldsymbol{\mathbf{\theta}} \, | \, \mathbf{y}) \text{d} \boldsymbol{\mathbf{\theta}} = \int p(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}, \mathbf{y}) p(\boldsymbol{\mathbf{\theta}} \, | \, \mathbf{y}) \text{d} \boldsymbol{\mathbf{\theta}} \\
&amp;\approx |\hat{\mathbf{P}}_\texttt{LA}| \sum_{\mathbf{z} \in \mathcal{Q}(m, k)} \tilde p_\texttt{G}(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}(\mathbf{z}), \mathbf{y}) \tilde p_\texttt{LA}(\boldsymbol{\mathbf{\theta}}(\mathbf{z}) \, | \, \mathbf{y}) \omega(\mathbf{z}). \tag{6.12}
\end{align}\]</span>
The quadrature rule <span class="math inline">\(\mathbf{z} \in \mathcal{Q}(m, k)\)</span> is used both internally to normalise the marginal Laplace approximation, and externally to perform integration with respect to the hyperparameters.
Equation <a href="naomi-aghq.html#eq:mixgaussian">(6.12)</a> is a mixture of Gaussian distributions <span class="math inline">\(p_\texttt{G}(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}(\mathbf{z}), \mathbf{y})\)</span> with multinomial probabilities
<span class="math display">\[\begin{equation}
\lambda(\mathbf{z}) = |\hat{\mathbf{P}}_\texttt{LA}| p_\texttt{LA}(\boldsymbol{\mathbf{\theta}}(\mathbf{z}) \, | \, \mathbf{y}) \omega(\mathbf{z}),
\end{equation}\]</span>
where <span class="math inline">\(\sum \lambda(\mathbf{z}) = 1\)</span> and <span class="math inline">\(\lambda(\mathbf{z}) &gt; 0\)</span>.
Samples may therefore be easily obtained for the complete vector <span class="math inline">\(\mathbf{x}\)</span> jointly by first drawing a node <span class="math inline">\(\mathbf{z} \in \mathcal{Q}(m, k)\)</span> with multinomial probabilities <span class="math inline">\(\lambda(\mathbf{z})\)</span> then drawing a sample from the corresponding Gaussian distribution.
Algorithms for fast and exact simulation from a Gaussian distribution have been developed by <span class="citation">Håvard Rue (<a href="#ref-rue2001fast">2001</a>)</span>.
The posterior marginals for any subset of the complete vector, including <span class="math inline">\(p(x_i \, | \, \mathbf{y})\)</span>, can be simply obtained by keeping the relevant entries of <span class="math inline">\(\mathbf{x}\)</span>.</p>
</div>
<div id="laplace-marginals" class="section level4 hasAnchor" number="6.1.3.2">
<h4><span class="header-section-number">6.1.3.2</span> Laplace marginals<a href="naomi-aghq.html#laplace-marginals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An alternative more accurate, but more computationally intensive, approach is to calculate a Laplace approximation to the marginal posterior
<span class="math display" id="eq:lamarginaljoint">\[\begin{equation}
\tilde p_\texttt{LA}(x_i, \boldsymbol{\mathbf{\theta}}, \mathbf{y}) = \frac{p(x_i, \mathbf{x}_{-i}, \boldsymbol{\mathbf{\theta}}, \mathbf{y})}{\tilde p_\texttt{G}(\mathbf{x}_{-i} \, | \, x_i, \boldsymbol{\mathbf{\theta}}, \mathbf{y})} \Big\rvert_{\mathbf{x}_{-i} = \hat{\mathbf{x}}_{-i}(x_i, \boldsymbol{\mathbf{\theta}})}. \tag{6.13}
\end{equation}\]</span>
The variable <span class="math inline">\(x_i\)</span> is excluded from the Gaussian approximation
<span class="math display">\[\begin{equation}
p_\texttt{G}(\mathbf{x}_{-i} \, | \, x_i, \boldsymbol{\mathbf{\theta}}, \mathbf{y}) = \mathcal{N}(\mathbf{x}_{-i} \, | \, \hat{\mathbf{x}}_{-i}(x_i, \boldsymbol{\mathbf{\theta}}), \hat{\mathbf{H}}_{-i, -i}(x_i, \boldsymbol{\mathbf{\theta}}))
\end{equation}\]</span>
with <span class="math inline">\((N - 1)\)</span>-dimensional posterior mode and <span class="math inline">\((N - 1) \times (N - 1)\)</span>-dimensional Hessian matrix evaluated at the posterior mode
<span class="math display">\[\begin{align}
\hat{\mathbf{x}}_{-i}(x_i, \boldsymbol{\mathbf{\theta}}) &amp;= \arg\max_{\mathbf{x}_{-i}} \log p(\mathbf{y}, x_i, \mathbf{x}_{-i}, \boldsymbol{\mathbf{\theta}}), \\
\hat{\mathbf{H}}_{-i, -i}(x_i, \boldsymbol{\mathbf{\theta}}) &amp;= - \frac{\partial^2}{\partial \mathbf{x}_{-i} \partial \mathbf{x}_{-i}^\top} \log p(\mathbf{y}, x_i, \mathbf{x}_{-i}, \boldsymbol{\mathbf{\theta}}) \rvert_{\mathbf{x}_{-i} = \hat{\mathbf{x}}_{-i}(x_i, \boldsymbol{\mathbf{\theta}})}.
\end{align}\]</span>
The approximate posterior marginal <span class="math inline">\(\tilde p(x_i \, | \, \mathbf{y})\)</span> can be obtained by normalisation of the marginal Laplace approximation, and integration with respect to the hyperparameters, similarly to Equation <a href="naomi-aghq.html#eq:mixgaussian">(6.12)</a>
<span class="math display" id="eq:lamarginal">\[\begin{align}
p(x_i \, | \, \mathbf{y}) &amp;= \int p(x_i, \boldsymbol{\mathbf{\theta}} \, | \, \mathbf{y}) \text{d} \boldsymbol{\mathbf{\theta}} = \int p(x_i \, | \, \boldsymbol{\mathbf{\theta}}, \mathbf{y}) p(\boldsymbol{\mathbf{\theta}} \, | \, \mathbf{y}) \text{d} \boldsymbol{\mathbf{\theta}} \\
&amp;\approx |\hat{\mathbf{P}}_\texttt{LA}| \sum_{\mathbf{z} \in \mathcal{Q}(m, k)} \tilde p_\texttt{LA}(x_i \, | \, \boldsymbol{\mathbf{\theta}}(\mathbf{z}), \mathbf{y}) \tilde p_\texttt{LA}(\boldsymbol{\mathbf{\theta}}(\mathbf{z}) \, | \, \mathbf{y}) \omega(\mathbf{z}). \tag{6.14}
\end{align}\]</span></p>
<p>The term <span class="math inline">\(\tilde p_\texttt{LA}(x_i, \boldsymbol{\mathbf{\theta}} \, | \, \mathbf{y})\)</span> is obtained by normalising Equation <a href="naomi-aghq.html#eq:lamarginaljoint">(6.13)</a> using an estimate of the evidence <span class="math inline">\(\tilde p(\mathbf{y})\)</span>.
It’s possible to reuse the previously obtained posterior normalising constant estimate from Equation <a href="naomi-aghq.html#eq:aghqnormconst">(6.11)</a>.
Alternatively…</p>
</div>
<div id="simplified-laplace-marginals" class="section level4 hasAnchor" number="6.1.3.3">
<h4><span class="header-section-number">6.1.3.3</span> Simplified Laplace marginals<a href="naomi-aghq.html#simplified-laplace-marginals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="citation">Håvard Rue, Martino, and Chopin (<a href="#ref-rue2009approximate">2009</a>)</span> use the properties of Gauss-Markov random fields (GMRFs) to efficiently approximate the Laplace marginals.</p>
</div>
<div id="simplified-inla" class="section level4 hasAnchor" number="6.1.3.4">
<h4><span class="header-section-number">6.1.3.4</span> Simplified INLA<a href="naomi-aghq.html#simplified-inla" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="citation">Wood (<a href="#ref-wood2020simplified">2020</a>)</span> approximates the Laplace marginals without relying so heavily on sparsity via GMRFs and still achieves good performance.
For splines, as with extended latent Gaussian models [ELGMs; <span class="citation">Stringer, Brown, and Stafford (<a href="#ref-stringer2022fast">2022</a>)</span>], the precision matrix is not as sparse.</p>
</div>
</div>
<div id="software" class="section level3 hasAnchor" number="6.1.4">
<h3><span class="header-section-number">6.1.4</span> Software<a href="naomi-aghq.html#software" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="r-inla" class="section level4 hasAnchor" number="6.1.4.1">
<h4><span class="header-section-number">6.1.4.1</span> <code>R-INLA</code><a href="naomi-aghq.html#r-inla" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <code>R-INLA</code> software <span class="citation">(<a href="#ref-martins2013bayesian">Martins et al. 2013</a>)</span> implements the INLA method, as well as the stochastic partial differential equation (SPDE) approach of <span class="citation">Lindgren, Rue, and Lindström (<a href="#ref-lindgren2011explicit">2011</a>)</span>.
<code>R-INLA</code> is the R interface to the core <code>inla</code> program, which is written in C <span class="citation">(<a href="#ref-martino2009implementing">Martino and Rue 2009</a>)</span> and uses algorithms from the <code>GMRFLib</code> C library <span class="citation">(<a href="#ref-rue2001gmrflib">Håvard Rue and Follestad 2001</a>)</span>.
A formula interface of the form <code>y ~ ...</code> to specify the connection between the latent field <span class="math inline">\(\mathbf{x}\)</span> and the structured additive predictor <span class="math inline">\(\boldsymbol{\mathbf{\eta}}\)</span>.
For example, a model with one fixed effect <code>a</code> and one IID random effect <code>b</code> is written as <code>y ~ a + f(b, model = "iid")</code>.
This interface is easy to engage with for new users, but be limiting for more advanced users.
For more information, including recent developments, see the <code>R-INLA</code> website <a href="https://r-inla.org"><code>https://r-inla.org</code></a>.
The <code>inlabru</code> R package <span class="citation">(<a href="#ref-bachl2019inlabru">Bachl et al. 2019</a>)</span> is one notable example.
<span class="citation">Gaedke-Merzhäuser et al. (<a href="#ref-gaedke2023parallelized">2023</a>)</span> review recent computational improvements to <code>R-INLA</code>, including parallelism via <code>OpenMP</code> <span class="citation">(<a href="#ref-diaz2018openmp">Diaz et al. 2018</a>)</span> and use of the PARDISO sparse linear equation solver <span class="citation">(<a href="#ref-bollhofer2020state">Bollhöfer et al. 2020</a>)</span>.</p>
<p>The approach used to compute the marginals <span class="math inline">\(\tilde p(x_i \, | \, \mathbf{y})\)</span> can chosen by setting <code>method</code> to <code>"gaussian"</code> (Section <a href="naomi-aghq.html#gaussian-marginals">6.1.3.1</a>), <code>"laplace"</code> (Section <a href="naomi-aghq.html#laplace-marginals">6.1.3.2</a>) or <code>"simplified.laplace"</code> (Section <a href="naomi-aghq.html#simplified-laplace-marginals">6.1.3.3</a>).
The quadrature grid used can be chosen by setting <code>int.strategy</code> to <code>"eb"</code> (empirical Bayes), <code>"grid"</code>, or <code>"ccd"</code> [Box-Wilson central composite design; <span class="citation">Box and Wilson (<a href="#ref-box1992experimental">1992</a>)</span>].
Figure demonstrates these three integration strategies.</p>
</div>
<div id="tmb" class="section level4 hasAnchor" number="6.1.4.2">
<h4><span class="header-section-number">6.1.4.2</span> <code>TMB</code><a href="naomi-aghq.html#tmb" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Template Model Builder [TMB, or when referring to the software <code>TMB</code>; <span class="citation">Kristensen et al. (<a href="#ref-kristensen2016tmb">2016</a>)</span>] is an R package which implements the Laplace approximation.
In <code>TMB</code> derivatives are obtained using automatic differentiation [AD; <span class="citation">Baydin et al. (<a href="#ref-baydin2017automatic">2017</a>)</span>], a rule-based method based on decomposition of calculations into elementary operations, which can be combined by repeated use of the chain rule.
Give an example of <code>TMB</code> use.</p>
<p><code>TMB</code> C++ templates are compatible with other software.
<code>tmbstan</code> <span class="citation">(<a href="#ref-monnahan2018no">Monnahan and Kristensen 2018</a>)</span> allows running HMC via Stan.
<code>aghq</code> <span class="citation">(<a href="#ref-stringer2021implementing">Stringer 2021b</a>)</span> allows use of AGHQ via <code>mvQuad</code> <span class="citation">(<a href="#ref-weiser2016mvquad">Weiser 2016</a>)</span>.</p>
</div>
<div id="mgcv" class="section level4 hasAnchor" number="6.1.4.3">
<h4><span class="header-section-number">6.1.4.3</span> <code>mgcv</code><a href="naomi-aghq.html#mgcv" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Talk a little about <code>mgcv::ginla</code> and what it is capable of.</p>
</div>
</div>
</div>
<div id="a-universal-inla-implementation-based-on-ad" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> A universal INLA implementation based on AD<a href="naomi-aghq.html#a-universal-inla-implementation-based-on-ad" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, I implement the INLA method, with Laplace marginals, using the <code>TMB</code> package.
This implementation is universal in that it is compatible with any model with a <code>TMB</code> C++ template.
This leads the way for application of INLA to models which are not compatible with <code>R-INLA</code>.
Indeed, <span class="citation">Martino and Riebler (<a href="#ref-martino2019integrated">2019</a>)</span> note that “implementing INLA from scratch is a complex task” and as a result “applications of INLA are limited to the (large class of) models implemented [in <code>R-INLA</code>]”.
The potential benefits of a more flexible INLA implementation based on AD were noted by <span class="citation">Skaug (<a href="#ref-skaug2009approximate">2009</a>)</span> (a coauthor of TMB) in discussion of <span class="citation">Håvard Rue, Martino, and Chopin (<a href="#ref-rue2009approximate">2009</a>)</span>, noting that such a system would be “fast, flexible, and easy-to-use”.
This suggestion was made close to 15 years ago, so it is surprising that its potential remains unrealised.</p>
<div id="epilepsy-example" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Epilepsy example<a href="naomi-aghq.html#epilepsy-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the epilepsy generalised linear mixed model example of <span class="citation">D. Spiegelhalter et al. (<a href="#ref-spiegelhalter1996bugs">1996</a>)</span>.
This model is based on that of <span class="citation">Breslow and Clayton (<a href="#ref-breslow1993approximate">1993</a>)</span>, a modification of <span class="citation">Thall and Vail (<a href="#ref-thall1990some">1990</a>)</span>, and the data are from an epilepsy drug double-blind clinical trial <span class="citation">(<a href="#ref-leppik1985double">Leppik et al. 1985</a>)</span>.
<span class="citation">Håvard Rue, Martino, and Chopin (<a href="#ref-rue2009approximate">2009</a>)</span> (Section 5.2) demonstrate the INLA method using this example, and find a significant difference in approximation error depending on use of Gaussian or Laplace marginals.
As such, it is a good choice of example to use as demonstration.</p>
<p>In the trial, patients <span class="math inline">\(i = 1, \ldots, 59\)</span> were each assigned either the new drug <span class="math inline">\(\texttt{Trt}_i = 1\)</span> or placebo <span class="math inline">\(\texttt{Trt}_i = 0\)</span>.
Each patient made four visits the clinic <span class="math inline">\(j = 1, \ldots, 4\)</span>, and the observations <span class="math inline">\(y_{ij}\)</span> are the number of seizures of the <span class="math inline">\(i\)</span>th person in the two weeks preceding their <span class="math inline">\(j\)</span>th visit.
The covariates used in the model were age <span class="math inline">\(\texttt{Age}_i\)</span>, baseline seizure counts <span class="math inline">\(\texttt{Base}_i\)</span> and an indicator for the final clinic visit <span class="math inline">\(\texttt{V}_4\)</span>, which were all centred.
The observations were modelled using a Poisson distribution <span class="math inline">\(y_{ij} \sim \text{Poisson}(e^{\eta_{ij}})\)</span> with linear predictor
<span class="math display">\[\begin{align*}
\eta_{ij}
&amp;= \beta_0 + \beta_\texttt{Base} \log(\texttt{Baseline}_j / 4) + \beta_\texttt{Trt} \texttt{Trt}_i +
   \beta_{\texttt{Trt} \times \texttt{Base}} \texttt{Trt}_i \times \log(\texttt{Baseline}_j / 4) \\
&amp;+ \beta_\texttt{Age} \log(\texttt{Age}_i) + \beta_{\texttt{V}_4} {\texttt{V}_4}_j +
   \epsilon_i + \nu_{ij}, \quad i \in [59], \quad j \in [4],
\end{align*}\]</span>
where the prior distribution on each of the regression parameters, including the intercept, was <span class="math inline">\(\mathcal{N}(0, 100^2)\)</span>.
The patient <span class="math inline">\(\epsilon_i \sim \mathcal{N}(0, 1/\tau_\epsilon)\)</span> and patient-visit <span class="math inline">\(\nu_{ij} \sim \mathcal{N}(0, 1/\tau_\nu)\)</span> random effects were IID with precision prior distributions <span class="math inline">\(\tau_\epsilon, \tau_\nu \sim \Gamma(0.001, 0.001)\)</span>.</p>
<p>The linear predictor for this model can be specified in <code>R-INLA</code> by:</p>
<pre><code>formula &lt;- y ~ 1 + CTrt + ClBase4 + CV4 + ClAge + CBT +
  f(rand, model = &quot;iid&quot;, hyper = tau_prior) +  #&#39; Nu random effect
  f(Ind,  model = &quot;iid&quot;, hyper = tau_prior)    #&#39; Epsilon random effect</code></pre>
<p>The <code>TMB</code> C++ template for the model is in Appendix <a href="fast-approximate-bayesian-inference.html#tmb-epil">C.1.1</a>.</p>
</div>
<div id="another-example" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Another example<a href="naomi-aghq.html#another-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Choose an example of a model that <code>R-INLA</code> can’t fit but INLA with <code>TMB</code> can.
Demonstrate Laplace marginals making a difference to accuracy as compared with Gaussian marginals.</p>
</div>
</div>
<div id="naomi" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> The Naomi model<a href="naomi-aghq.html#naomi" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Naomi small-area estimation model <span class="citation">(<a href="#ref-eaton2021naomi">Eaton et al. 2021</a>)</span> synthesises data from multiple sources to estimate HIV indicators at a district-level, by age and sex.</p>
<div id="model-structure" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Model structure<a href="naomi-aghq.html#model-structure" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>I consider a simplified version of Naomi defined only at the time of the most recent household survey with HIV testing.
This version omits nowcasting and temporal projection.
These time points involve limited inferences.</p>
<div id="household-survey-component-household" class="section level4 hasAnchor" number="6.3.1.1">
<h4><span class="header-section-number">6.3.1.1</span> Household survey component {household}<a href="naomi-aghq.html#household-survey-component-household" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consider a country in sub-Saharan Africa where a household survey with complex survey design has taken place.
Let <span class="math inline">\(x \in \mathcal{X}\)</span> index district, <span class="math inline">\(a \in \mathcal{A}\)</span> index five-year age group, and <span class="math inline">\(s \in \mathcal{S}\)</span> index sex.
For ease of notation, let <span class="math inline">\(i\)</span> index the finest district-age-sex division included in the model.
Let <span class="math inline">\(I \subseteq \mathcal{X} \times \mathcal{A} \times \mathcal{S}\)</span> be a set of indices <span class="math inline">\(i\)</span> for which an aggregate observation is reported, and <span class="math inline">\(\mathcal{I}\)</span> be the set of all <span class="math inline">\(I\)</span> such that <span class="math inline">\(I \in \mathcal{I}\)</span>.</p>
<p>Let <span class="math inline">\(N_i \in \mathbb{N}\)</span> be the known, fixed population size.
HIV prevalence <span class="math inline">\(\rho_i \in [0, 1]\)</span>, antiretroviral therapy (ART) coverage <span class="math inline">\(\alpha_i \in [0, 1]\)</span>, and annual HIV incidence rate <span class="math inline">\(\lambda_i &gt; 0\)</span> are modelled using linked regression equations.</p>
<p>Independent logistic regression models are specified for HIV prevalence and ART coverage in the general population such that <span class="math inline">\(\text{logit}(\rho_i) = \eta^\rho_i\)</span> and <span class="math inline">\(\text{logit}(\alpha_i) = \eta^\alpha_i\)</span>.
HIV incidence rate is modelled on the log scale as <span class="math inline">\(\log(\lambda_i) = \eta^\lambda_i\)</span>, and depends on adult HIV prevalence and adult ART coverage.
Let <span class="math inline">\(\kappa_i\)</span> be the proportion recently infected among HIV positive persons.
This proportion is linked to HIV incidence via
<span class="math display" id="eq:kappa">\[\begin{equation}
\kappa_i = 1- \exp \left( - \lambda_i \cdot \frac{1 - \rho_i}{\rho_i} \cdot (\Omega_T - \beta_T) - \beta_T \right), \tag{6.15}
\end{equation}\]</span>
where the mean duration of recent infection <span class="math inline">\(\Omega_T\)</span> and the proportion of long-term HIV infections misclassified as recent <span class="math inline">\(\beta_T\)</span> are strongly informed by prior distributions for the particular survey.</p>
<p>These processes are each informed by household survey data.
Weighted aggregate survey observations are calculated as
<span class="math display">\[\begin{equation*}
\hat \theta_I = \frac{\sum_j w_j \cdot\theta_j}{\sum_j w_j},
\end{equation*}\]</span>
with individual responses <span class="math inline">\(\theta_j \in \{0, 1\}\)</span> and design weights <span class="math inline">\(w_j\)</span> for each of <span class="math inline">\(\theta \in \{\rho, \alpha, \kappa\}\)</span>.
The design weights are provided by the survey and aim to reduce bias by decreasing possible correlation between response and recording mechanism <span class="citation">(<a href="#ref-meng2018statistical">Meng 2018</a>)</span>.
The index <span class="math inline">\(j\)</span> runs across all individuals in strata <span class="math inline">\(i \in I\)</span> within the relevant denominator i.e. for ART coverage, only those individuals who are HIV positive.
The weighted observed number of outcomes is <span class="math inline">\(y^{\theta}_{I} = m^{\theta}_{I} \cdot \hat \theta_{I}\)</span> where
<span class="math display">\[\begin{equation*}
m^{\theta}_I = \frac{\left(\sum_j w_j\right)^2}{\sum_j w_j^2},
\end{equation*}\]</span>
is the Kish effective sample size (ESS) <span class="citation">(<a href="#ref-kish1965survey">Kish 1965</a>)</span>.
As the Kish ESS is maximised by constant design weights, in exchange for reducing bias the ESS is reduced and hence variance increased.
The weighted observed number of outcomes are modelled using a binomial working likelihood <span class="citation">(<a href="#ref-chen2014use">Chen, Wakefield, and Lumely 2014</a>)</span> defined to operate on the reals
<span class="math display">\[\begin{equation*}
y^{\theta}_{I} \sim \text{xBin}(m^{\theta}_{I}, \theta_{I}),
\end{equation*}\]</span>
where <span class="math inline">\(\theta_{I}\)</span> are the following weighted aggregates
<span class="math display">\[\begin{equation*}
\rho_{I} = \frac{\sum_{i \in I} N_i \rho_i}{\sum_{i \in I} N_i}, \quad
\alpha_{I} = \frac{\sum_{i \in I} N_i \rho_i \alpha_i}{\sum_{i \in I} N_i \rho_i}, \quad
\kappa_{I} = \frac{\sum_{i \in I} N_i \rho_i \kappa_i}{\sum_{i \in I} N_i \rho_i}.
\end{equation*}\]</span></p>
</div>
</div>
<div id="connection-to-elgms" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Connection to ELGMs<a href="naomi-aghq.html#connection-to-elgms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Naomi is a spatio-temporal model with a large Gaussian latent field, governed by a smaller number of hyperparameters.
However, it is an ELGM rather than an LGM, for the reasons below.
Note that when dependence on a specific number of structured additive predictors is given, it is for that factor in isolation.</p>
<!-- 1. In the household survey component, HIV incidence depends on district-level adult HIV prevalence and ART coverage. This reflects basic HIV epidemiology: HIV incidence is proportional to unsuppressed viral load such that such that $\lambda \propto \rho (1 - \omega \cdot \alpha)$, with $\omega = 0.7$ a fixed constant. As a result, each $\log(\lambda_i)$ depends on 28 structured additive predictors (where 28 arises from the product of 2 sexes [male and female], 7 age groups, [$\{\text{15-19}, \ldots, \text{45-49}\}$], and 2 indicators [HIV prevalence and ART coverage]). -->
<!-- 2. In the household survey component, HIV incidence and HIV prevalence are linked to the proportion recently infected via Equation \ref{eq:kappa}. -->
<!-- 3. In the ANC testing component, HIV prevalence and ART coverage depend upon the respective indicators in the household survey component. Though $\text{logit}(\rho_i)$ and $\text{logit}(\alpha_i)$ are Gaussian, this nonetheless introduces dependence of each mean response on two structured additive predictors. -->
<!-- 4. Throughout the model components, processes are modelled at the finest district-age-sex division $i$, but likelihoods are defined for observations aggregated over sets of indices $i \in I$. As such, all observations are related to $|I|$ structured additive predictors. -->
<!-- 5. Individuals taking ART, or who have been recently infected, must be HIV positive.  -->
<!-- 6. The ART attendance component uses a multinomial model with softmax link function which takes as input $|\{x': x' \sim x\}| + 1$ structured additive predictors, one for each neighbouring district plus one for remaining in the home district. -->
<!-- 7. Multiple link functions are used throughout the model, such that there is no one inverse link function $g$. -->
</div>
</div>
<div id="aghq-in-moderate-dimensions" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> AGHQ in moderate dimensions<a href="naomi-aghq.html#aghq-in-moderate-dimensions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Naomi model has <span class="math inline">\(m = 24\)</span> hyperparameters.
AGHQ with the product rule grid requires evaluation of <span class="math inline">\(|\mathcal{Q}(m, k)| = k^m\)</span> quadrature points.
This is intractable for <span class="math inline">\(m = 24\)</span>.
This section focuses on the development of AGHQ rules for moderate dimensions, for use within the nested Laplace approximation algorithm.</p>
<div id="aghq-with-variable-levels" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> AGHQ with variable levels<a href="naomi-aghq.html#aghq-with-variable-levels" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(\mathbf{k} = (k_1, \ldots, k_m)\)</span> be a vector of levels for each dimension of <span class="math inline">\(\boldsymbol{\mathbf{\theta}}\)</span>.
We may then define <span class="math inline">\(\mathcal{Q}(m, \mathbf{k}) = \mathcal{Q}(1, k_1) \times \cdots \times \mathcal{Q}(1, k_m)\)</span> to be a GHQ grid with possible variable levels of size <span class="math inline">\(|\mathcal{Q}(m, \mathbf{k})| = \prod_{j = 1}^m k_j\)</span>.
Let <span class="math inline">\(\mathcal{Q}(m, s, k)\)</span> correspond to <span class="math inline">\(\mathcal{Q}(m, \mathbf{k})\)</span> with choice of levels <span class="math inline">\(k_j = k, j \leq s\)</span> and <span class="math inline">\(k_j = 1, j &gt; s\)</span> for some <span class="math inline">\(s \leq m\)</span>.
For example, for <span class="math inline">\(m = 2\)</span> and <span class="math inline">\(s = 1\)</span> then <span class="math inline">\(\mathbf{k} = (k, 1)\)</span>.
In combination with use of the spectral decomposition, this choice of levels is analogous to a principal components analysis (PCA) approach to AGHQ.
We refer to this approach as PCA-AGHQ, with corresponding estimate of the normalising constant given by
<span class="math display">\[\begin{equation}
\tilde p_\texttt{PCA}(\mathbf{y}) = |\hat{\mathbf{E}}_{\texttt{LA}} \hat{\mathbf{\Lambda}}_{\texttt{LA}}^{1/2}|\sum_{\mathbf{z} \in \mathcal{Q}(m, s, k)} \tilde p_\texttt{LA}(\hat{\mathbf{E}}_{\texttt{LA}, s} \hat{\mathbf{\Lambda}}_{\texttt{LA}, s}^{1/2} \mathbf{z} + \hat{\boldsymbol{\mathbf{\theta}}}_\texttt{LA}, \mathbf{y}) \omega(\mathbf{z}),
\end{equation}\]</span>
where <span class="math inline">\(\hat{\mathbf{E}}_{\texttt{LA}, s}\)</span> is an <span class="math inline">\(m \times s\)</span> matrix containing the first <span class="math inline">\(s\)</span> eigenvectors, <span class="math inline">\(\hat{\mathbf{\Lambda}}_{\texttt{LA}, s}\)</span> is the <span class="math inline">\(s \times s\)</span> diagonal matrix containing the first <span class="math inline">\(s\)</span> eigenvalues, and <span class="math inline">\(\omega(\mathbf{z}) = \prod_{j = 1}^s \omega_s(z_j) \times \prod_{j = s + 1}^d \omega_1(z_j)\)</span>.
Panel C of Figure <span class="math inline">\(\ref{fig:aghq}\)</span> illustrates PCA-AGHQ for a case when <span class="math inline">\(m = 2\)</span> and <span class="math inline">\(s = 1\)</span>.
As AGHQ with <span class="math inline">\(k = 1\)</span> corresponds to the Laplace approximation, PCA-AGHQ can be interpreted as performing AGHQ on the first <span class="math inline">\(s\)</span> principal components of the inverse curvature, and a Laplace approximation on the remaining <span class="math inline">\(m - s\)</span> principal components.
Inference for the latent field follows analogously to Equation <span class="math inline">\(\ref{eq:nest}\)</span>.</p>
</div>
<div id="principal-components-analysis" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Principal components analysis<a href="naomi-aghq.html#principal-components-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>

<div class="figure" style="text-align: centre"><span style="display:block;" id="fig:pca-demo"></span>
<img src="figures/naomi-aghq/pca-demo.png" alt="See Figure 6.3." width="95%" />
<p class="caption">
Figure 6.4: See Figure <a href="naomi-aghq.html#fig:aghq-demo">6.3</a>.
</p>
</div>
</div>
</div>
<div id="malawi-case-study" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Malawi case-study<a href="naomi-aghq.html#malawi-case-study" class="anchor-section" aria-label="Anchor link to header"></a></h2>

<div class="figure" style="text-align: centre"><span style="display:block;" id="fig:naomi-output"></span>
<img src="resources/naomi-aghq/20230811-095752-5b8181d8/depends/figB.png" alt="Figure caption." width="95%" />
<p class="caption">
Figure 6.5: Figure caption.
</p>
</div>
<p>The Naomi model, as described in Section <a href="naomi-aghq.html#naomi">6.3</a>, was fit to data from Malawi using three inferential approaches.</p>
<div id="nuts-convergence" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> NUTS convergence<a href="naomi-aghq.html#nuts-convergence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="use-of-pca-aghq" class="section level3 hasAnchor" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Use of PCA-AGHQ<a href="naomi-aghq.html#use-of-pca-aghq" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="model-assessment" class="section level3 hasAnchor" number="6.5.3">
<h3><span class="header-section-number">6.5.3</span> Model assessment<a href="naomi-aghq.html#model-assessment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="inference-comparison" class="section level3 hasAnchor" number="6.5.4">
<h3><span class="header-section-number">6.5.4</span> Inference comparison<a href="naomi-aghq.html#inference-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="exceedance-probabilities" class="section level3 hasAnchor" number="6.5.5">
<h3><span class="header-section-number">6.5.5</span> Exceedance probabilities<a href="naomi-aghq.html#exceedance-probabilities" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="discussion-2" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Discussion<a href="naomi-aghq.html#discussion-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We developed an approximate Bayesian inference algorithm, combining AGHQ with PCA, motivated by a challenging problem in small-area estimation of HIV indicators.
For the simplified Naomi model in Malawi (Section <span class="math inline">\(\ref{sec:results}\)</span>) we demonstrated the method to be more accurate at inferring posterior distributions of model parameters, across a broad range of metrics, than TMB, and substantially faster than NUTS.
However, improvements in accuracy for model parameters did not translate into model outputs.
Indeed, we found posterior exceedance probabilities (Section <span class="math inline">\(\ref{sec:exceedance}\)</span>) from both TMB and PCA-AGHQ to have systematically inaccuracies, with the potential to meaningfully mislead policy.
If possible, though not a desirable situation, it could be advisable to provide gold-standard NUTS results after the workshop has concluded.
However, running NUTS for Naomi took days, in countries with 100s of districts it may simply not be feasible.</p>
<p>PCA-AGHQ could be added to the Naomi web interface as an alternative to TMB.
Analysts may then quickly iterate over model options using a fast inference approach, before switching to a more accurate approach once they are happy with the results.
By selecting <span class="math inline">\(s\)</span> and <span class="math inline">\(k\)</span>, PCA-AGHQ can be adjusted to suit the computational budget available.
We selected <span class="math inline">\(s\)</span> based on the Scree plot, and for the most part fixed <span class="math inline">\(k = 3\)</span>.
Whether it is preferable, for a given computational budget, to increase <span class="math inline">\(s\)</span> or increase <span class="math inline">\(k\)</span> is an open question.
Further strategies, such as gradually lowering <span class="math inline">\(k\)</span> over the principal components, could also be considered.</p>
<p>We hope that our work further encourages use of deterministic inference algorithms for ELGMs in applied settings, as well as methodological exploration of their accuracy and limitations.
Among the ELGM-type structures of particular interest in spatial epidemiology are aggregated Gaussian process models <span class="citation">(<a href="#ref-nandi2020disaggregation">Nandi et al. 2020</a>)</span> and evidence synthesis models <span class="citation">(<a href="#ref-amoah2020geostatistical">Amoah, Diggle, and Giorgi 2020</a>)</span>.</p>
<div id="suggestions-for-future-work" class="section level3 hasAnchor" number="6.6.1">
<h3><span class="header-section-number">6.6.1</span> Suggestions for future work<a href="naomi-aghq.html#suggestions-for-future-work" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="improved-quadrature-grids-for-moderate-dimensions" class="section level4 hasAnchor" number="6.6.1.1">
<h4><span class="header-section-number">6.6.1.1</span> Improved quadrature grids for moderate dimensions<a href="naomi-aghq.html#improved-quadrature-grids-for-moderate-dimensions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We aimed to develop a quadrature grid which allocates more effort to more important dimensions.
While PCA is a sensible approach, there are avenues where it does not behave as one might hope, or otherwise overlooks potential benefits.
The first challenge we identified was using PCA when the dimensions have different scales.
Specifically, we found logit-scale hyperparameters to be systematically favoured over those on the log-scale.
Second, the amount of variation explained for the Hessian matrix is not of direct interest, rather the effect of the different dimensions on the relevant outputs.
Using measures of importance from sensitivity analysis, such as Shapley values <span class="citation">(<a href="#ref-shapley1953value">Shapley et al. 1953</a>)</span> may be preferable.
Third, it is more important to allocate quadrature nodes to those marginals which are non-Gaussian.
This is because the Laplace approximation is exact when the integrand is Gaussian, so a single quadrature node is sufficiently.
The difficulty is, of course, knowing in advance which marginals will be non-Gaussian.
This could be done if there were a cheap way to obtain posterior means, which could then be compared to posterior modes obtained using optimisation.
Another approach would be to measure the fit of marginal samples from a cheap approximation, like TMB.
The main challenge is that the measurements have to be for marginals, ruling out approaches like PSIS which operate on joint distributions <span class="citation">(<a href="#ref-yao2018yes">Yao et al. 2018</a>)</span>.</p>
</div>
<div id="computational-speed-ups" class="section level4 hasAnchor" number="6.6.1.2">
<h4><span class="header-section-number">6.6.1.2</span> Computational speed-ups<a href="naomi-aghq.html#computational-speed-ups" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Integration over a moderate number of hyperparameters posed a challenge, and led us to use a quadrature grids with a large number of nodes.
However, computation at each node is independent, such that the run-time of the algorithm could potentially be significantly improved by parallel computing.
Further computational speed-ups might be obtained using graphics processing units (GPUs) specialised for the relevant matrix operations.</p>
</div>
<div id="comparison-to-other-mcmc-algorithms" class="section level4 hasAnchor" number="6.6.1.3">
<h4><span class="header-section-number">6.6.1.3</span> Comparison to other MCMC algorithms<a href="naomi-aghq.html#comparison-to-other-mcmc-algorithms" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Blocked Gibbs sampling <span class="citation">(<a href="#ref-geman1984stochastic">Geman and Geman 1984</a>)</span> or slice sampling <span class="citation">(<a href="#ref-neal2003slice">Neal 2003</a>)</span>, may be better suited than NUTS to sampling from Naomi.
These algorithms are available, and customisable, including e.g. choice of block structure within the <code>NIMBLE</code> probabilistic programming language <span class="citation">(<a href="#ref-de2017programming">Valpine et al. 2017</a>)</span>.</p>
</div>
<div id="implementation-into-probabilistic-programming-languages" class="section level4 hasAnchor" number="6.6.1.4">
<h4><span class="header-section-number">6.6.1.4</span> Implementation into probabilistic programming languages<a href="naomi-aghq.html#implementation-into-probabilistic-programming-languages" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Though gaining in popularity, the user-base of <code>TMB</code> remains relatively small.
Furthermore, for users unfamiliar with C++, it can be challenging to use.
As such, it could be beneficial to implement AGHQ within other probabilistic programming languages.
Implementation in <code>NIMBLE</code> could be relatively straightforward, as it (for version &gt;1.0.0) includes functionality for automatic differentiation and Laplace approximation, built using <code>CppAD</code> like <code>TMB</code>.
Similarly, implementation in Stan could be possible by use of the <code>bridgestan</code> package <span class="citation">(<a href="#ref-bridgestan">Ward 2023</a>)</span> together with the adjoint-differentiated Laplace approximation of <span class="citation">Margossian et al. (<a href="#ref-margossian2020hamiltonian">2020</a>)</span>.
<!-- Note: can fit hyperparameters with Laplace on latent field using HMC via tmbstan::tmbstan(laplace = TRUE) -->
<!-- What Charles Margossian has implemented for Stan is done similarly by default in TMB --></p>
</div>
<div id="statistical-theory" class="section level4 hasAnchor" number="6.6.1.5">
<h4><span class="header-section-number">6.6.1.5</span> Statistical theory<a href="naomi-aghq.html#statistical-theory" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="citation">Stringer, Brown, and Stafford (<a href="#ref-stringer2022fast">2022</a>)</span> (Theorem 1) bound the total variation error of AGHQ, establishing convergence in probability of coverage probabilities under the approximate posterior distribution to those under the true posterior distribution.
Similar theory might be established for PCA-AGHQ, or more generally AGHQ with varying numbers of nodes per dimension.
The challenge of connecting this theory to use of the quadrature rule within nested computations remains an open question.</p>
<!-- Finn Lindgren is working on a method for non-linear predictors, called the [iterative INLA method](https://github.com/inlabru-org/inlabru/blob/55896f10d563c14e34cab577b29b733aac051f86/vignettes/method.Rmd). More [slides](https://informatique-mia.inrae.fr/reseau-resste/sites/default/files/2020-09/slides-Lindgren_Avignon2018.pdf) here -->

</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-amoah2020geostatistical" class="csl-entry">
Amoah, Benjamin, Peter J Diggle, and Emanuele Giorgi. 2020. <span>“A Geostatistical Framework for Combining Spatially Referenced Disease Prevalence Data from Multiple Diagnostics.”</span> <em>Biometrics</em> 76 (1): 158–70.
</div>
<div id="ref-bachl2019inlabru" class="csl-entry">
Bachl, Fabian E, Finn Lindgren, David L Borchers, and Janine B Illian. 2019. <span>“Inlabru: An r Package for Bayesian Spatial Modelling from Ecological Survey Data.”</span> <em>Methods in Ecology and Evolution</em> 10 (6): 760–66.
</div>
<div id="ref-baydin2017automatic" class="csl-entry">
Baydin, Atılım Günes, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. <span>“<span class="nocase">Automatic differentiation in machine learning: a survey</span>.”</span> <em>The Journal of Machine Learning Research</em> 18 (1): 5595–5637.
</div>
<div id="ref-bilodeau2022stochastic" class="csl-entry">
Bilodeau, Blair, Alex Stringer, and Yanbo Tang. 2022. <span>“Stochastic Convergence Rates and Applications of Adaptive Quadrature in Bayesian Inference.”</span> <em>Journal of the American Statistical Association</em>, 1–11.
</div>
<div id="ref-bollhofer2020state" class="csl-entry">
Bollhöfer, Matthias, Olaf Schenk, Radim Janalik, Steve Hamm, and Kiran Gullapalli. 2020. <span>“State-of-the-Art Sparse Direct Solvers.”</span> <em>Parallel Algorithms in Computational Science and Engineering</em>, 3–33.
</div>
<div id="ref-box1992experimental" class="csl-entry">
Box, George EP, and Kenneth B Wilson. 1992. <span>“On the Experimental Attainment of Optimum Conditions.”</span> In <em>Breakthroughs in Statistics: Methodology and Distribution</em>, 270–310. Springer.
</div>
<div id="ref-breslow1993approximate" class="csl-entry">
Breslow, Norman E, and David G Clayton. 1993. <span>“Approximate Inference in Generalized Linear Mixed Models.”</span> <em>Journal of the American Statistical Association</em> 88 (421): 9–25.
</div>
<div id="ref-casella1985introduction" class="csl-entry">
Casella, George. 1985. <span>“An Introduction to Empirical Bayes Data Analysis.”</span> <em>The American Statistician</em> 39 (2): 83–87.
</div>
<div id="ref-chen2014use" class="csl-entry">
Chen, Cici, Jon Wakefield, and Thomas Lumely. 2014. <span>“The Use of Sampling Weights in Bayesian Hierarchical Models for Small Area Estimation.”</span> <em>Spatial and Spatio-Temporal Epidemiology</em> 11: 33–43.
</div>
<div id="ref-davis1975methods" class="csl-entry">
Davis, Philip J, and Philip Rabinowitz. 1975. <em>Methods of Numerical Integration</em>. Academic Press.
</div>
<div id="ref-diaz2018openmp" class="csl-entry">
Diaz, Jose Monsalve, Swaroop Pophale, Oscar Hernandez, David E Bernholdt, and Sunita Chandrasekaran. 2018. <span>“Openmp 4.5 Validation and Verification Suite for Device Offload.”</span> In <em>Evolving OpenMP for Evolving Architectures: 14th International Workshop on OpenMP, IWOMP 2018, Barcelona, Spain, September 26–28, 2018, Proceedings 14</em>, 82–95. Springer.
</div>
<div id="ref-eaton2021naomi" class="csl-entry">
Eaton, Jeffrey W, Laura Dwyer-Lindgren, Steve Gutreuter, Megan O’Driscoll, Oliver Stevens, Sumali Bajaj, Rob Ashton, et al. 2021. <span>“Naomi: A New Modelling Tool for Estimating HIV Epidemic Indicators at the District Level in Sub-Saharan Africa.”</span>
</div>
<div id="ref-gaedke2023parallelized" class="csl-entry">
Gaedke-Merzhäuser, Lisa, Janet van Niekerk, Olaf Schenk, and Håvard Rue. 2023. <span>“Parallelized Integrated Nested Laplace Approximations for Fast Bayesian Inference.”</span> <em>Statistics and Computing</em> 33 (1): 25.
</div>
<div id="ref-geman1984stochastic" class="csl-entry">
Geman, Stuart, and Donald Geman. 1984. <span>“Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, no. 6: 721–41.
</div>
<div id="ref-howes2023fast" class="csl-entry">
Howes, Adam, Alex Stringer, Seth R. Flaxman, and Jeffrey W. Eaton. 2023+. <span>“<span class="nocase">Fast approximate Bayesian inference of HIV indicators using PCA adaptive Gauss-Hermite quadrature</span>,”</span> 2023+.
</div>
<div id="ref-jackel2005note" class="csl-entry">
Jäckel, Peter. 2005. <span>“A Note on Multivariate Gauss-Hermite Quadrature.”</span> <em>London: ABN-Amro. Re</em>.
</div>
<div id="ref-kish1965survey" class="csl-entry">
Kish, Leslie. 1965. <em>Survey Sampling</em>. 04; HN29, K5.
</div>
<div id="ref-kristensen2016tmb" class="csl-entry">
Kristensen, Kasper, Anders Nielsen, Casper W Berg, Hans Skaug, Bradley M Bell, et al. 2016. <span>“TMB: Automatic Differentiation and Laplace Approximation.”</span> <em>Journal of Statistical Software</em> 70 (i05).
</div>
<div id="ref-laplace1774memoire" class="csl-entry">
Laplace, P. S. 1774. <span>“Memoire Sur La Probabilite de Causes Par Les Evenements.”</span> <em>Memoire de l’Academie Royale Des Sciences</em>.
</div>
<div id="ref-leppik1985double" class="csl-entry">
Leppik, IE, FE Dreifuss, T Bowman-Cloyd, N Santilli, M Jacobs, C Crosby, J Cloyd, et al. 1985. <span>“A Double-Blind Crossover Evaluation of Progabide in Partial Seizures.”</span> <em>Neurology</em> 35 (4): 285.
</div>
<div id="ref-lindgren2011explicit" class="csl-entry">
Lindgren, Finn, Håvard Rue, and Johan Lindström. 2011. <span>“An Explicit Link Between Gaussian Fields and Gaussian Markov Random Fields: The Stochastic Partial Differential Equation Approach.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 73 (4): 423–98.
</div>
<div id="ref-margossian2020hamiltonian" class="csl-entry">
Margossian, Charles, Aki Vehtari, Daniel Simpson, and Raj Agrawal. 2020. <span>“Hamiltonian Monte Carlo Using an Adjoint-Differentiated Laplace Approximation: Bayesian Inference for Latent Gaussian Models and Beyond.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 9086–97.
</div>
<div id="ref-martino2019integrated" class="csl-entry">
Martino, Sara, and Andrea Riebler. 2019. <span>“Integrated Nested Laplace Approximations (INLA).”</span> <em>arXiv Preprint arXiv:1907.01248</em>.
</div>
<div id="ref-martino2009implementing" class="csl-entry">
Martino, Sara, and Håvard Rue. 2009. <span>“Implementing Approximate Bayesian Inference Using Integrated Nested Laplace Approximation: A Manual for the Inla Program.”</span> <em>Department of Mathematical Sciences, NTNU, Norway</em>.
</div>
<div id="ref-martins2013bayesian" class="csl-entry">
Martins, Thiago G, Daniel Simpson, Finn Lindgren, and Håvard Rue. 2013. <span>“<span class="nocase">Bayesian computing with INLA: new features</span>.”</span> <em>Computational Statistics &amp; Data Analysis</em> 67: 68–83.
</div>
<div id="ref-meng2018statistical" class="csl-entry">
Meng, Xiao-Li. 2018. <span>“Statistical Paradises and Paradoxes in Big Data (i) Law of Large Populations, Big Data Paradox, and the 2016 Us Presidential Election.”</span> <em>The Annals of Applied Statistics</em> 12 (2): 685–726.
</div>
<div id="ref-monnahan2018no" class="csl-entry">
Monnahan, Cole C, and Kasper Kristensen. 2018. <span>“No-u-Turn Sampling for Fast Bayesian Inference in ADMB and TMB: Introducing the Adnuts and Tmbstan r Packages.”</span> <em>PloS One</em> 13 (5): e0197954.
</div>
<div id="ref-nandi2020disaggregation" class="csl-entry">
Nandi, Anita K, Tim CD Lucas, Rohan Arambepola, Peter Gething, and Daniel J Weiss. 2020. <span>“Disaggregation: An r Package for Bayesian Spatial Disaggregation Modelling.”</span> <em>arXiv Preprint arXiv:2001.04847</em>.
</div>
<div id="ref-naylor1982applications" class="csl-entry">
Naylor, John C, and Adrian FM Smith. 1982. <span>“Applications of a Method for the Efficient Computation of Posterior Distributions.”</span> <em>Journal of the Royal Statistical Society Series C: Applied Statistics</em> 31 (3): 214–25.
</div>
<div id="ref-neal2003slice" class="csl-entry">
Neal, Radford M. 2003. <span>“Slice Sampling.”</span> <em>The Annals of Statistics</em> 31 (3): 705–67.
</div>
<div id="ref-rue2001fast" class="csl-entry">
Rue, Håvard. 2001. <span>“Fast Sampling of Gaussian Markov Random Fields.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 63 (2): 325–38.
</div>
<div id="ref-rue2001gmrflib" class="csl-entry">
Rue, Håvard, and Turid Follestad. 2001. <span>“GMRFLib: A c-Library for Fast and Exact Simulation of Gaussian Markov Random Fields.”</span> SIS-2002-236.
</div>
<div id="ref-rue2007approximate" class="csl-entry">
Rue, Håvard, and Sara Martino. 2007. <span>“<span class="nocase">Approximate Bayesian inference for hierarchical Gaussian Markov random field models</span>.”</span> <em>Journal of Statistical Planning and Inference</em> 137 (10): 3177–92.
</div>
<div id="ref-rue2009approximate" class="csl-entry">
Rue, Håvard, Sara Martino, and Nicolas Chopin. 2009. <span>“<span class="nocase">Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations</span>.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 71 (2): 319–92.
</div>
<div id="ref-rue2017bayesian" class="csl-entry">
Rue, Håvard, Andrea Riebler, Sigrunn H Sørbye, Janine B Illian, Daniel P Simpson, and Finn K Lindgren. 2017. <span>“Bayesian Computing with INLA: A Review.”</span> <em>Annual Review of Statistics and Its Application</em> 4: 395–421.
</div>
<div id="ref-shapley1953value" class="csl-entry">
Shapley, Lloyd S et al. 1953. <span>“A Value for n-Person Games.”</span>
</div>
<div id="ref-skaug2009approximate" class="csl-entry">
Skaug, Hans J. 2009. <span>“<span class="nocase">Discussion of "Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations"</span>.”</span> In <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 71:319–92. 2. Wiley Online Library.
</div>
<div id="ref-spiegelhalter1996bugs" class="csl-entry">
Spiegelhalter, David, Andrew Thomas, Nicky Best, and Wally Gilks. 1996. <span>“BUGS 0.5 Examples.”</span> <em>MRC Biostatistics Unit, Institute of Public Health, Cambridge, UK</em> 256.
</div>
<div id="ref-stringer2021implementing" class="csl-entry">
———. 2021b. <span>“Implementing Approximate Bayesian Inference Using Adaptive Quadrature: The Aghq Package.”</span> <em>arXiv Preprint arXiv:2101.04468</em>.
</div>
<div id="ref-stringer2022fast" class="csl-entry">
Stringer, Alex, Patrick Brown, and Jamie Stafford. 2022. <span>“Fast, Scalable Approximations to Posterior Distributions in Extended Latent Gaussian Models.”</span> <em>Journal of Computational and Graphical Statistics</em>, 1–15.
</div>
<div id="ref-thall1990some" class="csl-entry">
Thall, Peter F, and Stephen C Vail. 1990. <span>“Some Covariance Models for Longitudinal Count Data with Overdispersion.”</span> <em>Biometrics</em>, 657–71.
</div>
<div id="ref-tierney1986accurate" class="csl-entry">
Tierney, Luke, and Joseph B Kadane. 1986. <span>“<span class="nocase">Accurate approximations for posterior moments and marginal densities</span>.”</span> <em>Journal of the American Statistical Association</em> 81 (393): 82–86.
</div>
<div id="ref-unaids2023global" class="csl-entry">
———. 2023b. <span>“The Path That Ends AIDS: UNAIDS Global AIDS Update 2023.”</span> <a href="https://www.unaids.org/en/resources/documents/2023/global-aids-update-2023" class="uri">https://www.unaids.org/en/resources/documents/2023/global-aids-update-2023</a>.
</div>
<div id="ref-de2017programming" class="csl-entry">
Valpine, Perry de, Daniel Turek, Christopher J Paciorek, Clifford Anderson-Bergman, Duncan Temple Lang, and Rastislav Bodik. 2017. <span>“Programming with Models: Writing Statistical Algorithms for General Model Structures with NIMBLE.”</span> <em>Journal of Computational and Graphical Statistics</em> 26 (2): 403–13.
</div>
<div id="ref-bridgestan" class="csl-entry">
Ward, Brian. 2023. <em>Bridgestan: BridgeStan, Accessing Stan Model Functions in r</em>.
</div>
<div id="ref-weiser2016mvquad" class="csl-entry">
Weiser, Constantin. 2016. <em><span class="nocase">mvQuad</span>: Methods for Multivariate Quadrature.</em> <a href="http://CRAN.R-project.org/package=mvQuad">http://CRAN.R-project.org/package=mvQuad</a>.
</div>
<div id="ref-wood2020simplified" class="csl-entry">
———. 2020. <span>“<span class="nocase">Simplified integrated nested Laplace approximation</span>.”</span> <em>Biometrika</em> 107 (1): 223–30.
</div>
<div id="ref-yao2018yes" class="csl-entry">
Yao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018. <span>“Yes, but Did It Work?: Evaluating Variational Inference.”</span> In <em>International Conference on Machine Learning</em>, 5581–90. PMLR.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multi-agyw.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conclusions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/ulyngs/oxforddown/tree/master/06-naomi-aghq.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
