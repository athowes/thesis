---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: references.bib
---

```{r}
resource_version <- "20230811-095752-5b8181d8"
```

# Fast approximate Bayesian inference {#naomi-aghq}
\adjustmtc
\markboth{Fast approximate Bayesian inference}{}
<!-- For PDF output, include these two LaTeX commands after unnumbered chapter headings, otherwise the mini table of contents and the running header will show the previous chapter -->

In this chapter I describe a novel Bayesian inference method that I developed.
The method is motivated the Naomi small-area estimation model.
Over 35 countries have used the Naomi model software (`https://naomi.unaids.org`) to produce subnational estimates of HIV indicators [@unaids2023global].
Ensuring fast and accurate Bayesian inferences in this setting is challenging.
I began working on this project at the start of my PhD, and only started to make progress after reading @stringer2021fast.
I am grateful to have later collaborated with Alex Stringer.
The results of this work are presented in @howes2023fast.
Code for the analysis in this chapter is available from [`https://github.com/athowes/naomi-aghq`](https://github.com/athowes/elgm-inf).

## Inference methods

In a Bayesian analysis, the primary goal is to perform inference.
That is, to obtain the posterior distribution
\begin{equation}
p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) = \frac{p(\boldsymbol{\mathbf{\phi}}, \mathbf{y})}{p(\mathbf{y})},
\end{equation}
or some way to compute summaries of it.
Inference is a reasonable goal because the posterior distribution is sufficient for use in decision making.
Given a loss function, the posterior loss of a decision depends on the data only via the posterior distribution.
For example, given the posterior distribution of the current demand for HIV treatment at a particular facility, historic data about treatment demand are not required for planning of service provision.

It is usually intractable to directly obtain the posterior distribution.
This is because the denominator contains an intractable integral called the posterior normalising constant
\begin{equation}
p(\mathbf{y}) = \int_{\mathbb{R}^d} p(\mathbf{y}, \boldsymbol{\mathbf{\phi}}) \text{d}\boldsymbol{\mathbf{\phi}} (\#eq:evidence)
\end{equation}
For this reason, approximations to the posterior distribution $\tilde p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y})$ are typically used in place of the exact posterior distribution.
Some approximate Bayesian inference methods avoid directly calculating the posterior normalising constant, instead working with the unnormalised posterior distribution
\begin{equation}
p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) \propto p(\boldsymbol{\mathbf{\phi}}, \mathbf{y}).
\end{equation}
Other approximate Bayesian inference methods can more directly be thought of as ways to estimate the posterior normalising constant.
The methods in this chapter fall into this later category.

### The Laplace approximation

Laplace's method [@laplace1774memoire] is a technique used to approximate integrals of the form
\begin{equation}
\int \exp(C h(z)) \text{d}z,
\end{equation}
where $C > 0$ is a large constant and $h$ is a function which is twice-differentiable.
The Laplace approximation [@tierney1986accurate] is obtained by application of Laplace's method to calculate the posterior normalising constant.
Let $h(\boldsymbol{\mathbf{\phi}}) = \log p(\boldsymbol{\mathbf{\phi}}, \mathbf{y})$ such that
\begin{equation}
p(\mathbf{y}) = \int_{\mathbb{R}^d} p(\mathbf{y}, \boldsymbol{\mathbf{\phi}}) \text{d}\boldsymbol{\mathbf{\phi}} = \int_{\mathbb{R}^d} \exp(h(\boldsymbol{\mathbf{\phi}})) \text{d}\boldsymbol{\mathbf{\phi}}.
\end{equation}
Laplace's method involves approximating the function $h$ by its second order Taylor expansion.
This expansion is then evaluated at a maxima of $h$ to eliminate the first order term.
Let
\begin{equation}
\hat{\boldsymbol{\mathbf{\phi}}} = \arg\max_{\boldsymbol{\mathbf{\phi}}} h(\boldsymbol{\mathbf{\phi}})
\end{equation}
be the posterior mode, and
\begin{equation}
\hat {\mathbf{H}} = - \frac{\partial^2}{\partial \boldsymbol{\mathbf{\phi}} \partial \boldsymbol{\mathbf{\phi}}^\top} h(\boldsymbol{\mathbf{\phi}}) \rvert_{\boldsymbol{\mathbf{\phi}} = \hat{\boldsymbol{\mathbf{\phi}}}}
\end{equation}
be the Hessian matrix evaluated at the posterior mode.
The Laplace approximation is then
\begin{align}
\tilde p_{\texttt{LA}}(\mathbf{y}) &= \int_{\mathbb{R}^d} \exp \left( h(\hat{\boldsymbol{\mathbf{\phi}}}, \mathbf{y}) - \frac{1}{2} (\boldsymbol{\mathbf{\phi}} - \hat{\boldsymbol{\mathbf{\phi}}})^\top \hat {\mathbf{H}} (\boldsymbol{\mathbf{\phi}} - \hat{\boldsymbol{\mathbf{\phi}}}) \right) \text{d}\boldsymbol{\mathbf{\phi}} (\#eq:la) \\
&= p(\hat{\boldsymbol{\mathbf{\phi}}}, \mathbf{y}) \cdot \frac{(2 \pi)^{d/2}}{| \hat {\mathbf{H}} |^{1/2}}.
\end{align}
The above result is calculated using the known normalising constant of the Gaussian distribution
\begin{equation}
p_\texttt{G}(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) = \mathcal{N}(\boldsymbol{\mathbf{\phi}} \, | \, \hat{\boldsymbol{\mathbf{\phi}}}, \hat {\mathbf{H}}^{-1}) = \frac{| \hat {\mathbf{H}} |^{1/2}}{(2 \pi)^{d/2}} \exp \left( - \frac{1}{2} (\boldsymbol{\mathbf{\phi}} - \hat{\boldsymbol{\mathbf{\phi}}})^\top \hat {\mathbf{H}} (\boldsymbol{\mathbf{\phi}} - \hat{\boldsymbol{\mathbf{\phi}}}) \right).
\end{equation}
The Laplace approximation may be thought of as approximating the posterior distribution by a Gaussian distribution $p(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y}) \approx p_\texttt{G}(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y})$ such that
\begin{equation}
\tilde p_{\texttt{LA}}(\mathbf{y}) = \frac{p(\boldsymbol{\mathbf{\phi}}, \mathbf{y})}{p_\texttt{G}(\boldsymbol{\mathbf{\phi}} \, | \, \mathbf{y})} \Big\rvert_{\boldsymbol{\mathbf{\phi}} = \hat{\boldsymbol{\mathbf{\phi}}}}.
\end{equation}

(ref:laplace) Demonstration of the Laplace approximation for the simple Bayesian inference example of Figure \@ref(fig:conjugate). The unnormalised posterior $p(\phi, \mathbf{y}) = \phi^8 \exp(-4 \phi)$ can be recognised as the unnomalised gamma distribution $\text{Gamma}(9, 4)$. The true log normalising constant is $\log p(\mathbf{y}) = \log\Gamma(9) - 9 \log(4) = -1.872046$, whereas the Laplace approximate log normalising constant is $\log \tilde p_{\texttt{LA}}(\mathbf{y}) = -1.882458$, resulting from the Gaussian approximation $p_\texttt{G}(\phi \, | \, \mathbf{y}) = \mathcal{N}(\phi \, | \,\mu = 2, \tau = 2)$.

```{r laplace, fig.cap="(ref:laplace)"}
knitr::include_graphics("figures/naomi-aghq/laplace.png")
```

#### The marginal Laplace approximation

Approximating the full joint posterior distribution using a Gaussian distribution may be inaccurate.
An alternative is to approximate the marginal posterior distribution of some subset of the parameters which might have posterior distributions which are close to being Gaussian.
As before, let $\boldsymbol{\mathbf{\phi}} = (\mathbf{x}, \boldsymbol{\mathbf{\theta}})$ where $\mathbf{x}$ is the latent field, and $\boldsymbol{\mathbf{\theta}}$ are the hyperparameters.
Applying an equivalent Laplace approximation to the latent field, we have $h(\mathbf{x}, \boldsymbol{\mathbf{\theta}}) = \log p(\mathbf{x}, \boldsymbol{\mathbf{\theta}}, \mathbf{y})$ with posterior mode
\begin{equation}
\hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}}) = \arg\max_{\mathbf{x}} h(\mathbf{x}, \boldsymbol{\mathbf{\theta}})
\end{equation}
and Hessian matrix evaluated at the posterior mode
\begin{equation}
\hat {\mathbf{H}}(\boldsymbol{\mathbf{\theta}}) = - \frac{\partial^2}{\partial \mathbf{x} \partial \mathbf{x}^\top} h(\mathbf{x}, \boldsymbol{\mathbf{\theta}}) \rvert_{\mathbf{x} = \hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}})}.
\end{equation}
For both quantities dependence on the hyperparameters $\boldsymbol{\mathbf{\theta}}$ is made explicit.
The resulting marginal Laplace approximation is then
\begin{align}
\tilde p_{\texttt{LA}}(\boldsymbol{\mathbf{\theta}}, \mathbf{y}) &= \int_{\mathbb{R}^N} \exp \left( h(\hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}}), \boldsymbol{\mathbf{\theta}}, \mathbf{y}) - \frac{1}{2} (\mathbf{x} - \hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}}))^\top \hat {\mathbf{H}}(\boldsymbol{\mathbf{\theta}}) (\mathbf{x} - \hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}})) \right) \text{d}\mathbf{x} (\#eq:marginalla) \\
&= \exp(h(\hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}}), \mathbf{y})) \cdot \frac{(2 \pi)^{d/2}}{| \hat {\mathbf{H}}(\boldsymbol{\mathbf{\theta}}) |^{1/2}} \\
&= \frac{p(\mathbf{x}, \boldsymbol{\mathbf{\theta}}, \mathbf{y})}{\tilde p_\texttt{G}(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}, \mathbf{y})} \Big\rvert_{\mathbf{x} = \hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}})},
\end{align}
where $\tilde p_\texttt{G}(\mathbf{x} \, | \, \boldsymbol{\mathbf{\theta}}, \mathbf{y}) = \mathcal{N}(\mathbf{x} \, | \, \hat{\mathbf{x}}(\boldsymbol{\mathbf{\theta}}), \hat{\mathbf{H}}(\boldsymbol{\mathbf{\theta}})^{-1})$ is a Gaussian approximation to the marginal posterior of the latent field.

### Quadrature

Quadrature is an approach which can be used to approximate integrals like the posterior normalising constant.
As with the Laplace approximation, it is deterministic in that the computational procedure is not intrinsically random.

Let $\mathcal{Q}$ be a set of quadrature points $\mathbf{z} \in \mathcal{Q}$ and $\omega: \mathbb{R}^d \to \mathbb{R}$ be a weighting function.
Then a quadrature approximation to the posterior normalising constant is given by
\begin{equation}
\tilde p_{\mathcal{Q}}(\mathbf{y}) = \sum_{\mathbf{z} \in \mathcal{Q}} p(\mathbf{y}, \mathbf{z}) \omega(\mathbf{z}).
\end{equation}

The one-dimensional trapezoid rule, in which quadrature points are spaced evenly across the domain of interest, is a simple example of a quadrature rule.
The more quadrature points are used per dimension, the more accurate the estiamte of the integrand (Figure \@ref(fig:trapezoid)).

Quadrature methods are most effective when integrating over small dimensions.
The number of quadrature points required grows exponentially with the dimension.
For even moderate dimension, this quickly becomes intractable.

(ref:trapezoid) The trapezoid rule with $k = 5, 25, 125$ quadrature nodes can be used to integrate the function $f(z) = z \sin(z)$ in the domain $[0, \pi]$. Here, the exact solution is $\pi \approx 3.1416$. As more nodes are used in the computation, the quadrature estimate becomes closer to the exact solution.

```{r trapezoid, fig.cap="(ref:trapezoid)"}
knitr::include_graphics("figures/naomi-aghq/trapezoid.png")
```

#### Gauss-Hermite quadrature

Gauss-Hermite quadrature [GHQ; @davis1975methods] is a quadrature rule designed to integrate functions of the form $f(z) = \phi(z) P_\alpha(z)$ exactly.
Here $\phi(\cdot)$ is a standard normal density, and $P_\alpha(\cdot)$ is a polynomial of degree $\alpha$.

I follow the notation for GHQ established by @bilodeau2022stochastic.
For $z \in \mathbb{R}$, let $H_k(z)$ be the $k$th Hermite polynomial
\begin{equation}
H_k(z) = (-1)^k \exp(z^2 / 2) \frac{\text{d}}{\text{d}z^k} \exp(-z^2 / 2).
\end{equation}
The univariate GHQ rule has nodes $z \in \mathcal{Q}(1, k)$ given by the $k$ zeroes of the $k$th Hermite polynomial.
The corresponding weighting function $\omega: \mathcal{Q}(1, k) \to \mathbb{R}$ is given by
\begin{equation}
\omega(z) = \frac{\phi(z) \cdot k!}{[H_{k + 1}(z)]^2}.
\end{equation}

Multivariate GHQ rules are usually constructed using the product rule over identical univariate GHQ rules in each dimension.
In $d$ dimensions, the multivariate GHQ nodes $\mathbf{z} \in \mathcal{Q}(d, k)$ are defined by
\begin{equation}
\mathcal{Q}(d, k) = \mathcal{Q}(1, k)^d = \mathcal{Q}(1, k) \times \cdots \times \mathcal{Q}(1, k).
\end{equation}
The corresponding weighting function $\omega: \mathcal{Q}(d, k) \to \mathbb{R}$ is given by $\omega(\mathbf{z}) = \prod_{j = 1}^d \omega(z_j)$.

#### Adaptive quadrature

(ref:aghq-demo) The Gauss-Hermite quadrature nodes $\mathbf{z} \in \mathcal{Q}(2, 3)$ for a two dimensional integral with three nodes per dimension (A). Adaption occurs based on the mode (B) and covariance of the integrand via either the Cholesky (C) or spectral (D) decomposition of the inverse curvature at the mode. The integrand is $f(z_1, z_2) = \text{sn}(0.5 z_1, \alpha = 2) \cdot \text{sn}(0.8 z_1 - 0.5 z_2, \alpha = -2)$, where $\text{sn}(\cdot)$ is the standard skewnormal probability density function with shape parameter $\alpha \in \mathbb{R}$.

```{r aghq-demo, fig.cap="(ref:aghq-demo)"}
knitr::include_graphics("figures/naomi-aghq/aghq-demo.png")
```

In adaptive quadrature, the quadrature nodes and weights depend on the specific integrand.
Adaptive quadrature rules are particularly important for Bayesian inference problems because the posterior normalising function is a function of the data.
No fixed quadrature rule can be expected to effectively integrate every posterior distribution produced by observation of particular data.

In adaptive GHQ (AGHQ) the quadrature nodes are shifted by the mode of the integrand, and rotated based on a matrix decomposition of the inverse curvature at the mode
\begin{equation}
\mathbf{z} \mapsto \hat{\mathbf{P}}_\texttt{LA} \mathbf{z} + \hat{\boldsymbol{\mathbf{\theta}}}_\texttt{LA}.
\end{equation}
Two alternatives for the matrix decomposition [@jackel2005note] are:

1. the Cholesky decomposition $\hat{\mathbf{P}}_\texttt{LA} = \hat{\mathbf{L}}_\texttt{LA}$, where $\hat{\mathbf{L}}_\texttt{LA}$ is lower triangular,
2. the spectral decomposition $\hat{\mathbf{P}}_\texttt{LA} = \hat{\mathbf{E}}_\texttt{LA} \hat{\mathbf{\Lambda}}_\texttt{LA}^{1/2}$, where $\hat{\mathbf{E}}_\texttt{LA} = (\hat{\mathbf{e}}_{\texttt{LA}, 1}, \ldots \hat{\mathbf{e}}_{\texttt{LA}, m})$ contains the eigenvectors of $[\hat{\mathbf{H}}_\texttt{LA}(\hat{\boldsymbol{\mathbf{\theta}}}_\texttt{LA})]^{-1}$ and $\hat{\mathbf{\Lambda}}_\texttt{LA}$ is a diagonal matrix containing its eigenvalues $(\hat \lambda_{\texttt{LA}, 1}, \ldots, \hat \lambda_{\texttt{LA}, m})$.

### Integrated nested Laplace approximation

The integrated nested Laplace approximation (INLA) method [@rue2009approximate] combines marginal Laplace approximations with quadrature to enable approximation of posterior marginal distributions.

## Software

### `TMB`

Template Model Builder [TMB, or when referring to the software `TMB`; @kristensen2016tmb] is an R package which implements the Laplace approximation.
In `TMB` derivatives are obtained using automatic differentiation [@baydin2017automatic].

### `R-INLA`

The `R-INLA` software implements the INLA method.
`R-INLA` uses a formula interface (e.g. `y ~ 1 + x`) to facilitate use of INLA for common models.
This is a beneficial design choice for new users.
For more advanced users, the formula interface can impose constraints on model choice.

## A universal INLA implementation

In this section, I implement the INLA method from scratch, using the `TMB` package.
The result is universal in that it is compatible with any model with a `TMB` C++ template.
This opens the door for application of INLA to models like Naomi which are not compatible with `R-INLA`.
Indeed, @martino2019integrated note that "implementing INLA from scratch is a complex task", and as a result "applications of INLA are limited to the (large class of) models implemented [in `R-INLA`]".
The potential benefits of a more flexible INLA implementation based on automatic differentiation were noted by @skaug2009approximate in discussion of @rue2009approximate.

### Epilepsy example

I use the epilepsy generalised linear mixed model example from @spiegelhalter1996bugs to demonstrate the implementation.
This model is based on that of @breslow1993approximate, a modification of @thall1990some, and the data are from an epilespy drug double-blind clinical trial [@leppik1985double].
@rue2009approximate (Section 5.2) demonstrate the INLA method using this example, and find a significant difference in approximation error depending on use of either the Gaussian or Laplace approximation for some parameters.

In the trial, patients $i = 1, \ldots, 59$ were each assigned either the new drug $\texttt{Trt}_i = 1$ or placebo $\texttt{Trt}_i = 0$.
Each patient made four visits the clinic $j = 1, \ldots, 4$, and the observations $y_{ij}$ are the number of seizures of the $i$th person in the two weeks preceding their $j$th visit.
The covariates used in the model were age $\texttt{Age}_i$, baseline seizure counts $\texttt{Base}_i$ and an indicator for the final clinic visit $\texttt{V}_4$, which were all centered.
The observations were modelled using a Poisson distribution $y_{ij} \sim \text{Poisson}(e^{\eta_{ij}})$ with linear predictor 
\begin{align*}
\eta_{ij}
&= \beta_0 + \beta_\texttt{Base} \log(\texttt{Baseline}_j / 4) + \beta_\texttt{Trt} \texttt{Trt}_i +
   \beta_{\texttt{Trt} \times \texttt{Base}} \texttt{Trt}_i \times \log(\texttt{Baseline}_j / 4) \\ 
&+ \beta_\texttt{Age} \log(\texttt{Age}_i) + \beta_{\texttt{V}_4} {\texttt{V}_4}_j +
   \epsilon_i + \nu_{ij}, \quad i \in [59], \quad j \in [4],
\end{align*}
where the prior distribution on each of the regression parameters, including the intercept, was $\mathcal{N}(0, 100^2)$.
The random effects are IID $\epsilon_i \sim \mathcal{N}(0, 1/\tau_\epsilon)$ and $\nu_{ij} \sim \mathcal{N}(0, 1/\tau_\nu)$ with precision prior distributions $\tau_\epsilon, \tau_\nu \sim \Gamma(0.001, 0.001)$.

## The Naomi model

The Naomi small-area estimation model [@eaton2021naomi] synthesises data from multiple sources to estimate HIV indicators at a district-level, by age and sex.

### Model structure

I consider a simplified version of Naomi defined only at the time of the most recent household survey with HIV testing.
This version omits nowcasting and temporal projection.
These time points involve limited inferences.

#### Household survey component \label{sec:household}

Consider a country in sub-Saharan Africa where a household survey with complex survey design has taken place.
Let $x \in \mathcal{X}$ index district, $a \in \mathcal{A}$ index five-year age group, and $s \in \mathcal{S}$ index sex.
For ease of notation, let $i$ index the finest district-age-sex division included in the model.
Let $I \subseteq \mathcal{X} \times \mathcal{A} \times \mathcal{S}$ be a set of indices $i$ for which an aggregate observation is reported, and $\mathcal{I}$ be the set of all $I$ such that $I \in \mathcal{I}$.

Let $N_i \in \mathbb{N}$ be the known, fixed population size.
HIV prevalence $\rho_i \in [0, 1]$, antiretroviral therapy (ART) coverage $\alpha_i \in [0, 1]$, and annual HIV incidence rate $\lambda_i > 0$ are modelled using linked regression equations.

Independent logistic regression models are specified for HIV prevalence and ART coverage in the general population such that $\text{logit}(\rho_i) = \eta^\rho_i$ and $\text{logit}(\alpha_i) = \eta^\alpha_i$.
HIV incidence rate is modelled on the log scale as $\log(\lambda_i) = \eta^\lambda_i$, and depends on adult HIV prevalence and adult ART coverage.
Let $\kappa_i$ be the proportion recently infected among HIV positive persons.
This proportion is linked to HIV incidence via
\begin{equation}
\kappa_i = 1- \exp \left( - \lambda_i \cdot \frac{1 - \rho_i}{\rho_i} \cdot (\Omega_T - \beta_T) - \beta_T \right), (\#eq:kappa)
\end{equation}
where the mean duration of recent infection $\Omega_T$ and the proportion of long-term HIV infections misclassified as recent $\beta_T$ are strongly informed by priors for the particular survey.

These processes are each informed by household survey data.
Weighted aggregate survey observations are calculated as
\begin{equation*}
\hat \theta_I = \frac{\sum_j w_j \cdot\theta_j}{\sum_j w_j},
\end{equation*}
with individual responses $\theta_j \in \{0, 1\}$ and design weights $w_j$ for each of $\theta \in \{\rho, \alpha, \kappa\}$.
The design weights are provided by the survey and aim to reduce bias by decreasing possible correlation between response and recording mechanism [@meng2018statistical].
The index $j$ runs across all individuals in strata $i \in I$ within the relevant denominator i.e. for ART coverage, only those individuals who are HIV positive.
The weighted observed number of outcomes is $y^{\theta}_{I} = m^{\theta}_{I} \cdot \hat \theta_{I}$ where
\begin{equation*}
m^{\theta}_I = \frac{\left(\sum_j w_j\right)^2}{\sum_j w_j^2},
\end{equation*}
is the Kish effective sample size (ESS) [@kish1965survey].
As the Kish ESS is maximised by constant design weights, in exchange for reducing bias the ESS is reduced and hence variance increased.
The weighted observed number of outcomes are modelled using a binomial working likelihood [@chen2014use] defined to operate on the reals
\begin{equation*}
y^{\theta}_{I} \sim \text{xBin}(m^{\theta}_{I}, \theta_{I}),
\end{equation*}
where $\theta_{I}$ are the following weighted aggregates
\begin{equation*}
\rho_{I} = \frac{\sum_{i \in I} N_i \rho_i}{\sum_{i \in I} N_i}, \quad
\alpha_{I} = \frac{\sum_{i \in I} N_i \rho_i \alpha_i}{\sum_{i \in I} N_i \rho_i}, \quad
\kappa_{I} = \frac{\sum_{i \in I} N_i \rho_i \kappa_i}{\sum_{i \in I} N_i \rho_i}.
\end{equation*}

### Connection to ELGMs

## Extension of AGHQ to moderate dimensions

The Naomi model has $m = 24$ hyperparameters.

### AGHQ with variable levels

### Principal components analysis

(ref:pca-demo) See Figure \@ref(fig:aghq-demo).

```{r pca-demo, fig.cap="(ref:pca-demo)"}
knitr::include_graphics("figures/naomi-aghq/pca-demo.png")
```

## Malawi case-study

```{r figB, fig.cap="Naomi output."}
knitr::include_graphics(paste0("resources/naomi-aghq/", resource_version, "/depends/figB.png"))
```

### NUTS convergence

### Use of PCA-AGHQ

### Model assessment

### Inference comparison

### Exceedance probabilities

## Discussion

<!-- Finn Lindgren is working on a method for non-linear predictors, called the [iterative INLA method](https://github.com/inlabru-org/inlabru/blob/55896f10d563c14e34cab577b29b733aac051f86/vignettes/method.Rmd). More [slides](https://informatique-mia.inrae.fr/reseau-resste/sites/default/files/2020-09/slides-Lindgren_Avignon2018.pdf) here -->